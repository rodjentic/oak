here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/runner.py:
<runner.py>
# src/oak_runner/runner.py
"""
Arazzo Workflow and OpenAPI Operation Runner

This library executes Arazzo workflows step-by-step, following the paths defined in the
workflow specification. It builds an execution tree based on the possible paths and
executes OpenAPI operations sequentially, handling success/failure conditions and flow control.
"""

import json
import logging
from collections.abc import Callable
from typing import Any

import requests

from .auth.auth_processor import AuthProcessor
from .auth.credentials.provider import CredentialProvider, CredentialProviderFactory
from .evaluator import ExpressionEvaluator
from .executor import StepExecutor
from .executor.server_processor import ServerProcessor
from .http import HTTPExecutor
from .models import (
    ActionType,
    ArazzoDoc,
    ExecutionState,
    OpenAPIDoc,
    RuntimeParams,
    StepStatus,
    WorkflowExecutionResult,
    WorkflowExecutionStatus,
)
from .utils import (
    deprecated,
    dump_state,
    load_arazzo_doc,
    load_openapi_file,
    load_source_descriptions,
)

logger = logging.getLogger("arazzo-runner")


class OAKRunner:
    """
    Executes Arazzo workflows step-by-step, following the defined paths
    and handling success/failure conditions
    """

    def __init__(
        self,
        arazzo_doc: ArazzoDoc | None = None,
        source_descriptions: dict[str, OpenAPIDoc] = None,
        http_client=None,
        auth_provider: CredentialProvider | None = None
    ):
        """
        Initialize the runner with Arazzo document and source descriptions

        Args:
            arazzo_doc: Parsed Arazzo document
            source_descriptions: Dictionary of Open API Specs where the key is the source description name as defined in the Arazzo document
            http_client: Optional HTTP client for direct API calls (defaults to requests)
            auth_provider: Optional authentication provider
        """
        if not arazzo_doc and not source_descriptions:
            raise ValueError("Either arazzo_doc or source_descriptions must be provided.")

        self.arazzo_doc = arazzo_doc
        self.source_descriptions = source_descriptions

        # Process API authentication
        auth_processor = AuthProcessor()
        auth_config = auth_processor.process_api_auth(
            openapi_specs=source_descriptions,
            arazzo_specs=[arazzo_doc] if arazzo_doc else [],
        )

        http_client = http_client or requests.Session()
        self.auth_provider = auth_provider or CredentialProviderFactory.create_default(
            auth_requirements=auth_config.get("auth_requirements", []),
            env_mapping=auth_config.get("env_mappings", {}),
            http_client=http_client
        )

        # Initialize HTTP client
        http_executor = HTTPExecutor(http_client, self.auth_provider)

        # Initialize step executor
        self.step_executor = StepExecutor(http_executor, self.source_descriptions)

        # Execution state
        self.execution_states = {}

        # Event callbacks
        self.event_callbacks = {
            "step_start": [],
            "step_complete": [],
            "workflow_start": [],
            "workflow_complete": [],
        }

    @classmethod
    def from_arazzo_path(cls, arazzo_path: str, base_path: str = None, http_client=None, auth_provider=None):
        """
        Initialize the runner with an Arazzo document path

        Args:
            arazzo_path: Path to the Arazzo document
            base_path: Optional base path for source descriptions
            http_client: Optional HTTP client for direct API calls (defaults to requests)
        """
        if not arazzo_path:
            raise ValueError("Arazzo document path is required to initialize the runner.")

        arazzo_doc = load_arazzo_doc(arazzo_path)
        source_descriptions = load_source_descriptions(arazzo_doc, arazzo_path, base_path, http_client)
        return cls(arazzo_doc, source_descriptions, http_client, auth_provider)

    @classmethod
    def from_openapi_path(cls, openapi_path: str):
        """
        Initialize the runner with a single OpenAPI specification path.

        Args:
            openapi_path: Path to the local OpenAPI specification file.
        """
        if not openapi_path:
            raise ValueError("OpenAPI specification path is required.")

        try:
            # Use the simplified utility function (no http_client needed)
            openapi_doc = load_openapi_file(openapi_path)
            source_descriptions = {"default": openapi_doc}
        except Exception as e:
            logger.error(f"Failed to load OpenAPI spec from {openapi_path}: {e}")
            raise ValueError(f"Could not load OpenAPI spec from {openapi_path}") from e

        # Initialize the runner without an Arazzo document
        # __init__ will create default http_client and auth_provider if needed
        return cls(
            arazzo_doc=None,
            source_descriptions=source_descriptions,
            http_client=None,
            auth_provider=None
        )

    def register_callback(self, event_type: str, callback: Callable):
        """
        Register a callback for workflow execution events

        Args:
            event_type: Type of event ('step_start', 'step_complete', 'workflow_start', 'workflow_complete')
            callback: Function to call when the event occurs
        """
        if event_type in self.event_callbacks:
            self.event_callbacks[event_type].append(callback)
        else:
            logger.warning(f"Unknown event type: {event_type}")

    def _trigger_event(self, event_type: str, **kwargs):
        """Trigger registered callbacks for an event"""
        for callback in self.event_callbacks.get(event_type, []):
            try:
                callback(**kwargs)
            except Exception as e:
                logger.error(f"Error in {event_type} callback: {e}")

    def start_workflow(self, workflow_id: str, inputs: dict[str, Any] | None = None, runtime_params: RuntimeParams | None = None) -> str:
        """
        Start a new workflow execution

        Args:
            workflow_id: ID of the workflow to execute
            inputs: Input parameters for the workflow
            runtime_params: Optional runtime parameters for execution (e.g., server variables).

        Returns:
            execution_id: Unique ID for this workflow execution
        """
        # Generate a unique execution ID
        execution_id = f"{workflow_id}_{len(self.execution_states) + 1}"

        # Find the workflow definition
        workflow = None
        for wf in self.arazzo_doc.get("workflows", []):
            if wf.get("workflowId") == workflow_id:
                workflow = wf
                break

        if not workflow:
            raise ValueError(f"Workflow {workflow_id} not found in Arazzo document")

        # Execute dependency workflows if they exist
        depends_on = workflow.get("dependsOn", [])
        dependency_outputs = {}
        if depends_on:
            logger.info(f"Workflow {workflow_id} depends on {depends_on}")
            for dep_workflow_id in depends_on:
                logger.info(f"Executing dependency workflow: {dep_workflow_id}")
                # Execute the dependency workflow and wait for completion
                # Pass runtime_params to the dependent workflow execution
                dep_execution_id = self.start_workflow(dep_workflow_id, inputs, runtime_params)

                # Run the dependency workflow until completion
                while True:
                    # execute_next_step will now retrieve runtime_params from the state
                    result = self.execute_next_step(dep_execution_id)
                    if result.get("status") in [WorkflowExecutionStatus.WORKFLOW_COMPLETE, WorkflowExecutionStatus.ERROR]:
                        break

                # Get the dependency workflow outputs
                dep_state = self.execution_states.get(dep_execution_id)
                if not dep_state:
                    raise ValueError(
                        f"Dependency workflow execution state not found: {dep_execution_id}"
                    )

                # Store the dependency outputs for later use
                logger.info(
                    f"Dependency workflow {dep_workflow_id} outputs: {dep_state.workflow_outputs}"
                )
                dependency_outputs[dep_workflow_id] = dep_state.workflow_outputs.copy()
                # Double check dependency outputs are stored properly
                logger.info(
                    f"After storing dependency {dep_workflow_id}, dependency_outputs: {dependency_outputs}"
                )

                # Check if dependency succeeded
                if result.get("status") == WorkflowExecutionStatus.ERROR:
                    logger.error(f"Dependency workflow {dep_workflow_id} failed")
                    raise ValueError(f"Dependency workflow {dep_workflow_id} failed")

        # Initialize execution state
        state = ExecutionState(
            workflow_id=workflow_id,
            inputs=inputs or {},
            dependency_outputs=dependency_outputs, # Store dependency outputs
            runtime_params=runtime_params # Store runtime parameters in ExecutionState
        )

        # Initialize step statuses
        if workflow and "steps" in workflow:
            for step in workflow.get("steps", []):
                step_id = step.get("stepId")
                if step_id:
                    state.status[step_id] = StepStatus.PENDING

        # Store the execution state
        self.execution_states[execution_id] = state

        # Trigger workflow_start event
        self._trigger_event(
            "workflow_start", execution_id=execution_id, workflow_id=workflow_id, inputs=inputs
        )

        return execution_id

    def execute_workflow(
        self,
        workflow_id: str,
        inputs: dict[str, Any] = None,
        runtime_params: RuntimeParams | None = None
    ) -> WorkflowExecutionResult:
        """
        Start and execute a workflow until completion, returning the outputs.

        Args:
            workflow_id: ID of the workflow to execute
            inputs: Input parameters for the workflow
            runtime_params: Runtime parameters for execution (e.g., server variables)

        Returns:
            A WorkflowExecutionResult object containing the status, workflow_id, outputs, and any error
        """
        def on_workflow_start(execution_id, workflow_id, inputs):
            logger.debug(f"\n=== Starting workflow: {workflow_id} ===")
            logger.debug(f"Inputs: {json.dumps(inputs, indent=2)}")

        def on_step_start(execution_id, workflow_id, step_id):
            logger.debug(f"\n--- Starting step: {step_id} ---")

        def on_step_complete(execution_id, workflow_id, step_id, success, outputs=None, error=None):
            logger.debug(f"--- Completed step: {step_id} (Success: {success}) ---")
            if outputs:
                logger.debug(f"Outputs: {json.dumps(outputs, indent=2)}")
            if error:
                logger.debug(f"Error: {error}")

        def on_workflow_complete(execution_id, workflow_id, outputs):
            logger.debug(f"\n=== Completed workflow: {workflow_id} ===")
            logger.debug(f"Outputs: {json.dumps(outputs, indent=2)}")

        self.register_callback("workflow_start", on_workflow_start)
        self.register_callback("step_start", on_step_start)
        self.register_callback("step_complete", on_step_complete)
        self.register_callback("workflow_complete", on_workflow_complete)

        execution_id = self.start_workflow(workflow_id, inputs, runtime_params)

        while True:
            result = self.execute_next_step(execution_id)

            if result.get("status") in [WorkflowExecutionStatus.WORKFLOW_COMPLETE, WorkflowExecutionStatus.ERROR]:
                # Get the execution state to access step outputs
                state = self.execution_states[execution_id]

                # Convert the dictionary result to a WorkflowExecutionResult object
                execution_result = WorkflowExecutionResult(
                    status=result["status"],
                    workflow_id=result.get("workflow_id", workflow_id),
                    outputs=result.get("outputs", {}),
                    step_outputs=state.step_outputs if state.step_outputs else None,
                    inputs=inputs,
                    error=result.get("error")
                )
                return execution_result

    def execute_next_step(self, execution_id: str) -> dict:
        """
        Execute the next step in the workflow

        Args:
            execution_id: ID of the workflow execution

        Returns:
            WorkflowExecutionResult: Result of the step execution
        """
        if execution_id not in self.execution_states:
            raise ValueError(f"Execution {execution_id} not found")

        state = self.execution_states[execution_id]
        workflow_id = state.workflow_id

        # Find the workflow definition
        workflow = None
        for wf in self.arazzo_doc.get("workflows", []):
            if wf.get("workflowId") == workflow_id:
                workflow = wf
                break

        if not workflow:
            raise ValueError(f"Workflow {workflow_id} not found in Arazzo document")

        # Determine the next step to execute
        steps = workflow.get("steps", [])
        next_step = None
        next_step_idx = 0

        if state.current_step_id is None:
            # First step in the workflow
            if steps:
                next_step = steps[0]
        else:
            # Find the current step index
            current_idx = None
            for idx, step in enumerate(steps):
                if step.get("stepId") == state.current_step_id:
                    current_idx = idx
                    break

            if current_idx is not None and current_idx + 1 < len(steps):
                next_step = steps[current_idx + 1]
                next_step_idx = current_idx + 1

        if not next_step:
            # No more steps to execute, workflow is complete
            self._trigger_event(
                "workflow_complete",
                execution_id=execution_id,
                workflow_id=workflow_id,
                outputs=state.workflow_outputs,
            )
            return {
                "status": WorkflowExecutionStatus.WORKFLOW_COMPLETE,
                "workflow_id": workflow_id,
                "outputs": state.workflow_outputs,
            }

        # Execute the next step
        step_id = next_step.get("stepId")
        state.current_step_id = step_id
        state.status[step_id] = StepStatus.RUNNING

        # Dump state before executing the step for debugging
        logger.info(f"===== EXECUTING STEP: {step_id} =====")
        dump_state(state)

        # Trigger step_start event
        self._trigger_event(
            "step_start", execution_id=execution_id, workflow_id=workflow_id, step_id=step_id
        )

        # Execute the step
        try:
            if "workflowId" in next_step:
                # Handle nested workflow execution
                step_result = self._execute_nested_workflow(next_step, state)
            else:
                # Execute operation step
                step_result = self.step_executor.execute_step(next_step, state)

            success = step_result.get("success", False)

            # Update step status
            state.status[step_id] = StepStatus.SUCCESS if success else StepStatus.FAILURE

            # Store step outputs
            state.step_outputs[step_id] = step_result.get("outputs", {})

            # Check if we need to update workflow outputs
            if "outputs" in workflow:
                workflow_outputs = workflow.get("outputs", {})
                for output_name, output_expr in workflow_outputs.items():
                    # Evaluate the output expression
                    value = ExpressionEvaluator.evaluate_expression(
                        output_expr, state, self.source_descriptions
                    )
                    state.workflow_outputs[output_name] = value

            # Determine next action
            next_action = self.step_executor.determine_next_action(next_step, success, state)

            # Trigger step_complete event
            self._trigger_event(
                "step_complete",
                execution_id=execution_id,
                workflow_id=workflow_id,
                step_id=step_id,
                success=success,
                outputs=step_result.get("outputs", {}),
            )

            # Handle the action
            if next_action["type"] == ActionType.END:
                # Check if there's a failure flag from the step
                if not success:
                    # End the workflow with failure
                    self._trigger_event(
                        "workflow_error",
                        execution_id=execution_id,
                        workflow_id=workflow_id,
                        step_id=step_id,
                        error="Step failed success criteria",
                        outputs=state.workflow_outputs,
                    )
                    return {
                        "status": WorkflowExecutionStatus.ERROR,
                        "workflow_id": workflow_id,
                        "step_id": step_id,
                        "error": "Step failed success criteria",
                        "outputs": state.workflow_outputs,
                    }
                else:
                    # End the workflow successfully
                    self._trigger_event(
                        "workflow_complete",
                        execution_id=execution_id,
                        workflow_id=workflow_id,
                        outputs=state.workflow_outputs,
                    )
                    return {
                        "status": WorkflowExecutionStatus.WORKFLOW_COMPLETE,
                        "workflow_id": workflow_id,
                        "outputs": state.workflow_outputs,
                    }
            elif next_action["type"] == ActionType.GOTO:
                # Go to another step or workflow
                if "workflow_id" in next_action:
                    # Start a new workflow
                    new_execution_id = self.start_workflow(
                        next_action["workflow_id"], next_action.get("inputs", {})
                    )
                    return {
                        "status": WorkflowExecutionStatus.GOTO_WORKFLOW,
                        "workflow_id": next_action["workflow_id"],
                        "execution_id": new_execution_id,
                    }
                elif "step_id" in next_action:
                    # Go to a specific step in the current workflow
                    # Find the step index
                    for idx, step in enumerate(steps):
                        if step.get("stepId") == next_action["step_id"]:
                            next_step_idx = idx
                            break

                    # Update current step
                    state.current_step_id = steps[next_step_idx].get("stepId")
                    return {"status": WorkflowExecutionStatus.GOTO_STEP, "step_id": state.current_step_id}
            elif next_action["type"] == ActionType.RETRY:
                # Retry the current step
                # We don't change the step_id so it will retry on next execution
                state.status[step_id] = StepStatus.PENDING

                # If there's a delay, we should return that information
                retry_delay = next_action.get("retry_after", 0)
                return {"status": WorkflowExecutionStatus.RETRY, "step_id": step_id, "retry_after": retry_delay}

            # Default: continue to next step
            return {
                "status": WorkflowExecutionStatus.STEP_COMPLETE,
                "step_id": step_id,
                "success": success,
                "outputs": step_result.get("outputs", {}),
            }

        except Exception as e:
            logger.error(f"Error executing step {step_id}: {e}")
            state.status[step_id] = StepStatus.FAILURE

            # Trigger step_complete event with failure
            self._trigger_event(
                "step_complete",
                execution_id=execution_id,
                workflow_id=workflow_id,
                step_id=step_id,
                success=False,
                error=str(e),
            )

            return {"status": WorkflowExecutionStatus.STEP_ERROR, "step_id": step_id, "error": str(e)}

    def _execute_nested_workflow(self, step: dict, state: ExecutionState) -> dict:
        """Execute a nested workflow"""
        workflow_id = step.get("workflowId")

        # Prepare inputs for the nested workflow
        workflow_inputs = {}

        logger.info(f"Preparing inputs for nested workflow: {workflow_id}")

        for param in step.get("parameters", []):
            name = param.get("name")
            value = param.get("value")
            original_value = value

            # Process the value to resolve any expressions
            if isinstance(value, str):
                if value.startswith("$"):
                    # Direct expression
                    value = ExpressionEvaluator.evaluate_expression(
                        value, state, self.source_descriptions
                    )
                elif "{" in value and "}" in value:
                    # Template with expressions
                    import re

                    def replace_expr(match):
                        expr = match.group(1)
                        eval_value = ExpressionEvaluator.evaluate_expression(
                            expr, state, self.source_descriptions
                        )
                        return "" if eval_value is None else str(eval_value)

                    value = re.sub(r"\{([^}]+)\}", replace_expr, value)
            elif isinstance(value, dict):
                value = ExpressionEvaluator.process_object_expressions(
                    value, state, self.source_descriptions
                )
            elif isinstance(value, list):
                value = ExpressionEvaluator.process_array_expressions(
                    value, state, self.source_descriptions
                )

            logger.info(f"  Parameter: {name}, Original: {original_value}, Evaluated: {value}")
            workflow_inputs[name] = value

        # Start the nested workflow
        execution_id = self.start_workflow(workflow_id, workflow_inputs)

        # Execute the nested workflow until completion
        while True:
            result = self.execute_next_step(execution_id)
            if result.get("status") in [WorkflowExecutionStatus.WORKFLOW_COMPLETE, WorkflowExecutionStatus.ERROR]:
                break

        # Get the nested workflow outputs
        nested_state = self.execution_states.get(execution_id)
        if not nested_state:
            raise ValueError(f"Nested workflow execution state not found: {execution_id}")

        logger.info(f"Nested workflow outputs: {nested_state.workflow_outputs}")

        # Check if all steps succeeded
        all_success = True
        for step_id, step_status in nested_state.status.items():
            if step_status == StepStatus.FAILURE:
                all_success = False
                logger.warning(f"Nested workflow step failed: {step_id}")
                break

        return {"success": all_success, "outputs": nested_state.workflow_outputs}

    def execute_operation(
        self,
        inputs: dict[str, Any],
        operation_id: str | None = None,
        operation_path: str | None = None,
        runtime_params: RuntimeParams | None = None,
    ) -> dict:
        """
        Execute a single API operation directly, outside of a workflow context.

        This is the public entry point for direct operation execution.

        Args:
            inputs: Input parameters for the operation.
            operation_id: The operationId of the operation to execute.
            operation_path: The path and method (e.g., 'GET /users/{userId}') of the operation.
                          Provide either operation_id or operation_path, not both.
            runtime_params: Optional runtime parameters for execution (e.g., server variables).

        Returns:
            A dictionary containing the response status_code, headers, and body.
            Example: {'status_code': 200, 'headers': {...}, 'body': ...}

        Raises:
            ValueError: If neither or both operation_id and operation_path are provided,
                        or if the operation cannot be found, or parameters are invalid.
            requests.exceptions.HTTPError: If the API call results in an HTTP error status (4xx or 5xx).
            Exception: For other underlying execution errors.
        """
        # Initial validation duplicated here for clarity at the public API boundary
        if not operation_id and not operation_path:
            raise ValueError("Either operation_id or operation_path must be provided.")
        if operation_id and operation_path:
            raise ValueError("Provide either operation_id or operation_path, not both.")

        log_identifier = f"ID='{operation_id}'" if operation_id else f"Path='{operation_path}'"
        logger.debug(f"OAKRunner: Received request to execute operation directly: {log_identifier}")

        try:
            # Delegate to StepExecutor's implementation
            result = self.step_executor.execute_operation(
                inputs=inputs,
                operation_id=operation_id,
                operation_path=operation_path,
                runtime_params=runtime_params,
            )
            logger.info(f"OAKRunner: Direct operation execution finished for {log_identifier}")
            return result
        except (ValueError, requests.exceptions.HTTPError) as e:
            # Re-raise known error types directly
            logger.error(f"OAKRunner: Error executing operation {log_identifier}: {e}")
            raise e
        except Exception as e:
            # Catch unexpected errors
            logger.exception(f"OAKRunner: Unexpected error executing operation {log_identifier}: {e}")
            # Wrap or re-raise depending on desired error handling strategy
            raise RuntimeError(f"Unexpected error during operation execution: {e}") from e

    @deprecated("Use OAKRunner.generate_env_mappings instead. Will drop support in a future release.")
    def get_env_mappings(self) -> dict[str, Any]:
        """
        DEPRECATED: Use OAKRunner.generate_env_mappings instead.
        Returns the environment variable mappings for both authentication and server variables.
       
        Returns:
            Dictionary containing:
            - 'auth': Environment variable mappings for authentication
            - 'servers': Environment variable mappings for server URLs (only included if server variables exist)
        """
        # Get authentication environment mappings (old way)
        try:
            auth_mappings = self.auth_provider.env_mappings
        except AttributeError:
            auth_mappings = {}

        # Get authentication environment mappings via the EnvironmentVariableFetchStrategy
        try:
            auth_mappings = self.auth_provider.strategy._env_mapping
        except AttributeError:
            auth_mappings = {}

        result = {"auth": auth_mappings}

        # Get server variable environment mappings
        server_mappings = self.step_executor.server_processor.get_env_mappings()
        # Only include server mappings if they exist
        if server_mappings:
            result["servers"] = server_mappings

        return result

    @staticmethod
    def generate_env_mappings(
        arazzo_docs: list["ArazzoDoc"] | None = None,
        source_descriptions: dict[str, "OpenAPIDoc"] = None,
    ) -> dict:
        """
        Static method to return the environment variable mappings for both authentication and server variables.

        Args:
            arazzo_docs: List of parsed Arazzo documents (optional)
            source_descriptions: Dictionary of source names to OpenAPI spec dicts.

        Returns:
            Dictionary containing:
            - 'auth': Environment variable mappings for authentication
            - 'servers': Environment variable mappings for server URLs (only included if server variables exist)
        """
        auth_processor = AuthProcessor()
        auth_config = auth_processor.process_api_auth(
            openapi_specs=source_descriptions,
            arazzo_specs=arazzo_docs or [],
        )
        auth_env_mappings = auth_config.get("env_mappings", {})

        server_processor = ServerProcessor(source_descriptions or {})
        server_env_mappings = server_processor.get_env_mappings()
        result = {"auth": auth_env_mappings}
        if server_env_mappings:
            result["servers"] = server_env_mappings
        return result
</runner.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/models.py:
<models.py>
# src/oak_runner/models.py
"""
OAK Runner Data Models

This module defines the data models and enums used by the OAK Runner.
"""

from dataclasses import dataclass, field
from enum import Enum, StrEnum
from typing import Any, Optional

from pydantic import BaseModel, ConfigDict, Field

OpenAPIDoc = dict[str, Any]
ArazzoDoc = dict[str, Any]


class StepStatus(Enum):
    """Status of a workflow step execution"""

    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILURE = "failure"
    SKIPPED = "skipped"


class ActionType(Enum):
    """Type of action to take after a step execution"""

    CONTINUE = "continue"  # Continue to next step
    END = "end"  # End workflow
    GOTO = "goto"  # Go to another step or workflow
    RETRY = "retry"  # Retry the current step


class WorkflowExecutionStatus(StrEnum):
    """Represents the status strings returned by OAK Runner execution logic."""

    WORKFLOW_COMPLETE = "workflow_complete"
    ERROR = "error"
    GOTO_WORKFLOW = "goto_workflow"
    GOTO_STEP = "goto_step"
    RETRY = "retry"
    STEP_COMPLETE = "step_complete"
    STEP_ERROR = "step_error"

    def __repr__(self) -> str:
        return self.value

    def __str__(self) -> str:
        return self.value


@dataclass
class WorkflowExecutionResult:
    """Represents the result of a workflow execution
    
    This class models the structure of the result returned by the execute_workflow method.
    
    Attributes:
        status: The status of the workflow execution (e.g., WORKFLOW_COMPLETE, ERROR)
        workflow_id: The ID of the executed workflow
        outputs: The outputs produced by the workflow
        step_outputs: The outputs from each step in the workflow
        inputs: The original inputs provided to the workflow
        error: Optional error message if the workflow execution failed
    """
    status: WorkflowExecutionStatus
    workflow_id: str
    outputs: dict[str, Any] = field(default_factory=dict)
    step_outputs: dict[str, dict[str, Any]] | None = None
    inputs: dict[str, Any] | None = None
    error: str | None = None


@dataclass
class ExecutionState:
    """Represents the current execution state of a workflow"""

    workflow_id: str
    current_step_id: str | None = None
    inputs: dict[str, Any] = None
    step_outputs: dict[str, dict[str, Any]] = None
    workflow_outputs: dict[str, Any] = None
    dependency_outputs: dict[str, dict[str, Any]] = None
    status: dict[str, StepStatus] = None
    runtime_params: Optional['RuntimeParams'] = None

    def __post_init__(self):
        """Initialize default values"""
        if self.inputs is None:
            self.inputs = {}
        if self.step_outputs is None:
            self.step_outputs = {}
        if self.workflow_outputs is None:
            self.workflow_outputs = {}
        if self.dependency_outputs is None:
            self.dependency_outputs = {}
        if self.status is None:
            self.status = {}


class ServerVariable(BaseModel):
    """Represents a variable for server URL template substitution."""

    description: str | None = None
    default_value: str | None = Field(None, alias="default")
    enum_values: list[str] | None = Field(None, alias="enum")

    model_config = ConfigDict(populate_by_name=True, extra='allow')


class ServerConfiguration(BaseModel):
    """Represents an API server configuration with a templated URL and variables."""

    url_template: str = Field(alias="url")
    description: str | None = None
    variables: dict[str, ServerVariable] = Field(default_factory=dict)
    api_title_prefix: str | None = None # Derived from spec's info.title

    model_config = ConfigDict(populate_by_name=True, extra='allow')


class RuntimeParams(BaseModel):
    """
    Container for all runtime parameters that may influence workflow or operation execution.
    """
    servers: dict[str, str] | None = Field(
        default=None,
        description="Server variable overrides for server resolution."
    )
</models.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/auth/models.py:
<models.py>
# src/oak_runner/auth/models.py
"""
Authentication Models for OAK Runner.

This module defines Pydantic models for different authentication schemas used in
OpenAPI specifications and Arazzo workflows.
"""

from enum import Enum
from typing import Union

from pydantic import BaseModel, ConfigDict, Field, HttpUrl


# Environment variable key constants
class EnvVarKeys:
    """Constants for environment variable keys used in authentication."""
    API_KEY = 'apiKey'
    TOKEN = 'token'
    USERNAME = 'username'
    PASSWORD = 'password'
    AUTH_VALUE = 'auth_value'
    CLIENT_ID = 'client_id'
    CLIENT_SECRET = 'client_secret'
    REDIRECT_URI = 'redirect_uri'


# Authentication Types
class AuthLocation(str, Enum):
    """Locations where authentication credentials can be provided."""
    HEADER = "header"
    QUERY = "query"
    COOKIE = "cookie"
    PATH = "path"
    BODY = "body"


class AuthType(str, Enum):
    """Types of authentication schemas supported."""
    API_KEY = "apiKey"
    HTTP = "http"
    OAUTH2 = "oauth2"
    OPENID = "openIdConnect"
    CUSTOM = "custom"


class HttpSchemeType(str, Enum):
    """HTTP authentication scheme types."""
    BASIC = "basic"
    BEARER = "bearer"
    DIGEST = "digest"
    MUTUAL = "mutual"
    NEGOTIATE = "negotiate"
    OAUTH = "oauth"
    SCRAM_SHA_1 = "scram-sha-1"
    SCRAM_SHA_256 = "scram-sha-256"
    VAPID = "vapid"
    CUSTOM = "custom"


class OAuth2FlowType(str, Enum):
    """OAuth2 flow types."""
    AUTHORIZATION_CODE = "authorizationCode"
    IMPLICIT = "implicit"
    PASSWORD = "password"
    CLIENT_CREDENTIALS = "clientCredentials"


# Base Security Scheme
class SecurityScheme(BaseModel):
    """Base model for all authentication schemas."""
    type: AuthType
    name: str
    description: str | None = None


# API Key Authentication
class ApiKeyScheme(SecurityScheme):
    """API Key authentication schema."""
    type: AuthType = AuthType.API_KEY
    location: AuthLocation
    parameter_name: str = Field(..., description="Name of the parameter to use for the API key")


# HTTP Authentication
class HttpAuthScheme(SecurityScheme):
    """HTTP authentication schema (Basic, Bearer, etc.)."""
    type: AuthType = AuthType.HTTP
    scheme: HttpSchemeType
    location: AuthLocation = AuthLocation.HEADER
    bearer_format: str | None = None  # For Bearer auth, e.g., 'JWT'


# OAuth2 URLs
class OAuth2Urls(BaseModel):
    """URLs used in OAuth2 flows."""
    authorization: HttpUrl | None = None
    token: HttpUrl | None = None
    refresh: HttpUrl | None = None


# OAuth2 Flow
class OAuth2Flow(BaseModel):
    """Represents an OAuth2 flow configuration."""
    scopes: dict[str, str] = Field(default_factory=dict, description="Available scopes and their descriptions")

    model_config = ConfigDict(extra="forbid")


# OAuth2 Authorization Code Flow
class AuthorizationCodeFlow(OAuth2Flow):
    """Authorization Code flow for OAuth2."""
    authorization_url: HttpUrl
    token_url: HttpUrl
    refresh_url: HttpUrl | None = None


# OAuth2 Implicit Flow
class ImplicitFlow(OAuth2Flow):
    """Implicit flow for OAuth2."""
    authorization_url: HttpUrl


# OAuth2 Password Flow
class PasswordFlow(OAuth2Flow):
    """Password flow for OAuth2."""
    token_url: HttpUrl


# OAuth2 Client Credentials Flow
class ClientCredentialsFlow(OAuth2Flow):
    """Client Credentials flow for OAuth2."""
    token_url: HttpUrl


# OAuth2 Flows Container
class OAuth2Flows(BaseModel):
    """Container for all OAuth2 flows defined in a security scheme."""
    authorization_code: AuthorizationCodeFlow | None = None
    implicit: ImplicitFlow | None = None
    password: PasswordFlow | None = None
    client_credentials: ClientCredentialsFlow | None = None

    model_config = ConfigDict(extra="forbid")


# OAuth2 Authentication
class OAuth2Scheme(SecurityScheme):
    """OAuth2 authentication schema."""
    type: AuthType = AuthType.OAUTH2
    flows: OAuth2Flows = Field(default_factory=OAuth2Flows)


# OpenID Connect Authentication
class OpenIDScheme(SecurityScheme):
    """OpenID Connect authentication schema."""
    type: AuthType = AuthType.OPENID
    openid_connect_url: HttpUrl


# Custom Authentication
class CustomScheme(SecurityScheme):
    """Custom authentication schema for non-standard auth methods."""
    type: AuthType = AuthType.CUSTOM
    parameters: dict[str, str] = Field(default_factory=dict)


# Union type for all authentication schemas
SecuritySchemeUnion = Union[ApiKeyScheme, HttpAuthScheme, OAuth2Scheme, OpenIDScheme, CustomScheme]


class RequestAuthValue(BaseModel):
    """
    Represents an authentication value to be applied to an HTTP request.
    
    This model defines where and how an authentication value should be applied
    to an outgoing HTTP request.
    """
    location: AuthLocation
    name: str
    auth_value: str

    model_config = ConfigDict(extra="forbid")


# Base Auth class for all authentication values
class BaseAuth(BaseModel):
    """Base class for all authentication values."""
    type: AuthType

    model_config = ConfigDict(extra="forbid")


# Authentication Value Types
class BasicAuth(BaseAuth):
    """Credentials for HTTP Basic authentication."""
    type: AuthType = AuthType.HTTP
    username: str
    password: str

    model_config = ConfigDict(extra="forbid")


class BearerAuth(BaseAuth):
    """Token for HTTP Bearer authentication."""
    type: AuthType = AuthType.HTTP
    token: str

    model_config = ConfigDict(extra="forbid")


class ApiKeyAuth(BaseAuth):
    """API Key authentication value."""
    type: AuthType = AuthType.API_KEY
    api_key: str

    model_config = ConfigDict(extra="forbid")


class OAuth2Web(BaseAuth):
    """OAuth2 authentication values."""
    type: AuthType = AuthType.OAUTH2
    flow_type: OAuth2FlowType
    access_token: str
    token_type: str = "Bearer"
    refresh_token: str | None = None
    expires_in: int | None = None
    scope: str | None = None

    model_config = ConfigDict(extra="forbid")


class OAuth2ClientCredentials(BaseAuth):
    """OAuth2 client credentials."""
    type: AuthType = AuthType.OAUTH2
    flow_type: OAuth2FlowType = OAuth2FlowType.CLIENT_CREDENTIALS
    client_id: str
    client_secret: str
    access_token: str

    model_config = ConfigDict(extra="forbid")


class OAuth2AccessTokenOnly(BaseAuth):
    """OAuth2 access token only."""
    type: AuthType = AuthType.OAUTH2
    access_token: str
    token_type: str = "Bearer"
    refresh_token: str | None = None
    expires_in: int | None = None
    scope: str | None = None

    model_config = ConfigDict(extra="forbid")


class OpenIDAuth(BaseAuth):
    """OpenID Connect authentication value."""
    type: AuthType = AuthType.OPENID
    access_token: str
    token_type: str = "Bearer"
    refresh_token: str | None = None
    expires_in: int | None = None
    scope: str | None = None

    model_config = ConfigDict(extra="forbid")


# Union type for all authentication values
AuthValue = Union[BasicAuth, BearerAuth, ApiKeyAuth, OAuth2Web, OAuth2ClientCredentials, OpenIDAuth, OAuth2AccessTokenOnly]


# Security Requirements
class SecurityRequirement(BaseModel):
    """
    Represents a single security requirement in OpenAPI.
    
    In OpenAPI, security requirements are objects where:
    - Each key is a security scheme name
    - Each value is an array of scopes required for that scheme
    
    Example:
    {
      "api_key": [],
      "oauth2": ["read:pets", "write:pets"]
    }
    """
    scheme_name: str
    scopes: list[str] = Field(default_factory=list)

    model_config = ConfigDict(extra="forbid")


class SecurityOption(BaseModel):
    """
    Represents a single security option in OpenAPI.
    
    In OpenAPI security, each object in the security array represents an alternative
    option (OR relationship). Within each option, multiple schemes can be specified,
    which must all be satisfied (AND relationship).
    
    Example of a security array with multiple options:
    [
      {"api_key": []},                         # Option 1: Use API key only
      {"oauth2": ["read:pets", "write:pets"]}  # Option 2: Use OAuth2 with these scopes
    ]
    """
    requirements: list[SecurityRequirement] = Field(default_factory=list)

    model_config = ConfigDict(extra="forbid")


# Conversion functions
def auth_requirement_to_schema(req: 'AuthRequirement') -> SecuritySchemeUnion:
    """
    Convert an AuthRequirement object to the appropriate Pydantic schema.
    
    Args:
        req: The AuthRequirement object to convert
        
    Returns:
        The corresponding Pydantic schema object
    """
    # Common attributes
    common = {
        "name": req.name,
        "description": req.description,
        "required": req.required
    }

    if req.auth_type == AuthType.API_KEY:
        location = AuthLocation.HEADER
        if req.location:
            try:
                location = AuthLocation(req.location.value)
            except ValueError:
                pass

        return ApiKeyScheme(
            **common,
            location=location,
            parameter_name=req.name
        )

    elif req.auth_type == AuthType.HTTP:
        scheme = HttpSchemeType.CUSTOM
        if req.schemes and len(req.schemes) > 0:
            try:
                scheme = HttpSchemeType(req.schemes[0].lower())
            except ValueError:
                pass

        return HttpAuthScheme(
            **common,
            scheme=scheme
        )

    elif req.auth_type == AuthType.OAUTH2:
        flow_type = OAuth2FlowType.AUTHORIZATION_CODE
        if req.flow_type:
            try:
                flow_type = OAuth2FlowType(req.flow_type)
            except ValueError:
                pass

        auth_urls = OAuth2Urls()
        if req.auth_urls:
            auth_urls = OAuth2Urls(
                authorization=req.auth_urls.get("authorization"),
                token=req.auth_urls.get("token"),
                refresh=req.auth_urls.get("refresh")
            )

        flows = OAuth2Flows()
        if flow_type == OAuth2FlowType.AUTHORIZATION_CODE:
            flows.authorization_code = AuthorizationCodeFlow(
                authorization_url=auth_urls.authorization,
                token_url=auth_urls.token,
                refresh_url=auth_urls.refresh
            )
        elif flow_type == OAuth2FlowType.IMPLICIT:
            flows.implicit = ImplicitFlow(
                authorization_url=auth_urls.authorization
            )
        elif flow_type == OAuth2FlowType.PASSWORD:
            flows.password = PasswordFlow(
                token_url=auth_urls.token
            )
        elif flow_type == OAuth2FlowType.CLIENT_CREDENTIALS:
            flows.client_credentials = ClientCredentialsFlow(
                token_url=auth_urls.token
            )

        return OAuth2Scheme(
            **common,
            flows=flows
        )

    elif req.auth_type == AuthType.OPENID:
        openid_url = None
        if req.auth_urls and "openid_configuration" in req.auth_urls:
            openid_url = req.auth_urls["openid_configuration"]

        return OpenIDScheme(
            **common,
            openid_connect_url=openid_url or "https://example.com/.well-known/openid-configuration"
        )

    else:  # CUSTOM or unknown
        return CustomScheme(**common)
</models.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/auth/__init__.py:
<__init__.py>
# src/oak_runner/auth/__init__.py
"""
Authentication utilities for OAK Runner.

This module provides functionality to extract and manage authentication
requirements from OpenAPI specifications and Arazzo workflows.
"""

from .auth_parser import (
    AuthLocation,
    AuthRequirement,
    AuthType,
    extract_auth_from_arazzo,
    extract_auth_from_openapi,
    format_auth_requirements_markdown,
    summarize_auth_requirements,
)

__all__ = [
    "AuthLocation",
    "AuthRequirement",
    "AuthType",
    "extract_auth_from_arazzo",
    "extract_auth_from_openapi",
    "format_auth_requirements_markdown",
    "summarize_auth_requirements",
]
</__init__.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/auth/auth_processor.py:
<auth_processor.py>
# src/oak_runner/auth/auth_processor.py
"""Authentication processor for the Jentic MCP Plugin.

This module handles processing authentication requirements from OpenAPI specifications
and Arazzo workflows to generate appropriate configuration and environment variables.
"""

import logging
from typing import Any

from oak_runner.auth.models import SecurityOption, SecurityRequirement
from oak_runner.executor.operation_finder import OperationFinder
from oak_runner.models import ArazzoDoc, OpenAPIDoc
from oak_runner.utils import create_env_var_name, extract_api_title_prefix

from .auth_parser import AuthRequirement, extract_auth_from_openapi
from .models import AuthType, EnvVarKeys, SecurityOption

logger = logging.getLogger(__name__)


class AuthProcessor:
    """Processes authentication requirements for APIs."""

    def _normalize_openapi_spec(self, openapi_spec: dict[str, Any]) -> dict[str, Any]:
        """
        Normalize OpenAPI specifications to ensure consistent structure.
        
        Args:
            openapi_spec: OpenAPI specification
            
        Returns:
            Normalized OpenAPI specification
        """
        processed_spec = openapi_spec.copy()  # Make a copy to avoid modifying the original

        if isinstance(processed_spec, dict) and ("swagger" in processed_spec or "openapi" in processed_spec):
            # This is an actual OpenAPI spec
            if "components" not in processed_spec and "securityDefinitions" in processed_spec:
                # OpenAPI v2 format uses securityDefinitions
                # Convert to OpenAPI v3 format expected by our parser
                logger.debug("Converting OpenAPI v2 security definitions to v3 format")
                processed_spec["components"] = {"securitySchemes": processed_spec["securityDefinitions"]}

            elif "components" not in processed_spec and "securitySchemes" in processed_spec:
                # Some specs have securitySchemes at root level
                logger.debug("Moving root level securitySchemes to components")
                processed_spec["components"] = {"securitySchemes": processed_spec["securitySchemes"]}

        return processed_spec

    def process_api_auth(
        self,
        openapi_specs: dict[str, dict[str, Any]],
        arazzo_specs: list[dict[str, Any]] | None = None,
    ) -> dict[str, Any]:
        """
        Process API authentication requirements from OpenAPI specs and Arazzo workflows.
        
        Args:
            openapi_specs: Dictionary mapping source_description_ids to OpenAPI specifications
            arazzo_specs: Optional list of Arazzo workflow specifications
            
        Returns:
            Dictionary with auth requirements, environment mappings, and auth workflows
        """
        if arazzo_specs is None:
            arazzo_specs = []

        if not openapi_specs:
            logger.warning("No OpenAPI specs provided for auth processing")
            return {
                "auth_requirements": [],
                "env_mappings": {},
                "auth_workflows": []
            }

        logger.debug(f"Processing auth for {len(openapi_specs)} OpenAPI specs")

        # Fix OpenAPI spec structure if needed for each spec
        processed_specs = {}
        for source_id, spec in openapi_specs.items():
            processed_specs[source_id] = self._normalize_openapi_spec(spec)

        # Extract auth requirements from OpenAPI specs
        auth_requirements = []

        for source_id, spec in processed_specs.items():
            try:
                # Extract auth schemes
                spec_requirements = extract_auth_from_openapi(spec)

                # Set the source_description_id for each requirement
                for req in spec_requirements:
                    req.source_description_id = source_id

                if spec_requirements:
                    auth_requirements.extend(spec_requirements)

            except Exception as e:
                logger.warning(f"Error extracting auth requirements from spec with ID {source_id}: {str(e)}")

        # Generate environment variable mappings
        env_mappings = self.generate_env_mappings(auth_requirements)

        # Identify auth workflows
        auth_workflows = self.identify_auth_workflows(auth_requirements, arazzo_specs)

        # Create the final auth configuration
        auth_config = []
        for req in auth_requirements:
            auth_item = req.to_dict()
            auth_config.append(auth_item)

        result = {
            "auth_requirements": auth_config,
            "env_mappings": env_mappings,
            "auth_workflows": auth_workflows,
        }

        return result

    def generate_env_mappings(
        self,
        auth_requirements: list[AuthRequirement],
    ) -> dict[str, dict[str, str]]:
        """
        Generate environment variable mappings for auth requirements.

        Args:
            auth_requirements: List of auth requirements
            
        Returns:
            Dictionary with mappings of security scheme names to credential keys and environment variable names.
            When multiple source descriptions are present, uses a nested structure with source_name as the outer key.
        """
        # Track unique source descriptions to determine if we need a nested structure
        unique_source_descriptions = set()
        for auth_requirement in auth_requirements:
            source_description_id = auth_requirement.source_description_id or "default"
            unique_source_descriptions.add(source_description_id)

        # Determine if we have multiple sources
        has_multiple_sources = len(unique_source_descriptions) > 1

        # Create environment mappings structure based on whether we have multiple sources
        if has_multiple_sources:
            # Initialize nested structure with source descriptions as outer keys
            environment_mappings = {source_id: {} for source_id in unique_source_descriptions}
        else:
            # Use flat structure for single source
            environment_mappings = {}

        # Process each authentication requirement
        for auth_requirement in auth_requirements:
            source_description_id = auth_requirement.source_description_id or "default"
            security_scheme_name = auth_requirement.security_scheme_name

            # Determine API title prefix if available
            api_title_prefix = extract_api_title_prefix(auth_requirement.api_title)

            # Create the environment variable prefix using the scheme name and API title
            env_var_prefix = create_env_var_name(
                var_name=security_scheme_name,
                prefix=api_title_prefix
            )

            # For OAuth2, add the flow type as a suffix to distinguish different flows
            scheme_name_suffix = ""
            if auth_requirement.auth_type == AuthType.OAUTH2:
                if auth_requirement.flow_type in ["authorizationCode", "implicit"]:
                    scheme_name_suffix = ".web"
                elif auth_requirement.flow_type:
                    scheme_name_suffix = f".{auth_requirement.flow_type}"

            # Use the scheme name with suffix for the mappings
            full_scheme_name = f"{security_scheme_name}{scheme_name_suffix}"

            # Get the appropriate mapping dictionary based on structure
            if has_multiple_sources:
                # Use the source-specific mapping
                if full_scheme_name not in environment_mappings[source_description_id]:
                    environment_mappings[source_description_id][full_scheme_name] = {}
                scheme_env_vars = environment_mappings[source_description_id][full_scheme_name]
            else:
                # Use the flat mapping
                if full_scheme_name not in environment_mappings:
                    environment_mappings[full_scheme_name] = {}
                scheme_env_vars = environment_mappings[full_scheme_name]

            # Add appropriate environment variable mappings based on authentication type
            if auth_requirement.auth_type == AuthType.API_KEY:
                scheme_env_vars[EnvVarKeys.API_KEY] = f"{env_var_prefix}"

            elif auth_requirement.auth_type == AuthType.HTTP:
                http_auth_type = "basic" if "basic" in auth_requirement.schemes else (
                    "bearer" if "bearer" in auth_requirement.schemes or "Bearer" in auth_requirement.schemes else "generic"
                )

                if http_auth_type == "basic":
                    scheme_env_vars[EnvVarKeys.USERNAME] = f"{env_var_prefix}_USERNAME"
                    scheme_env_vars[EnvVarKeys.PASSWORD] = f"{env_var_prefix}_PASSWORD"
                elif http_auth_type == "bearer":
                    scheme_env_vars[EnvVarKeys.TOKEN] = f"{env_var_prefix}_TOKEN"
                else:
                    # Generic HTTP auth
                    scheme_env_vars[EnvVarKeys.AUTH_VALUE] = f"{env_var_prefix}_AUTH_VALUE"

            elif auth_requirement.auth_type == AuthType.OAUTH2:
                # Common OAuth2 params
                scheme_env_vars[EnvVarKeys.CLIENT_ID] = f"{env_var_prefix}_CLIENT_ID"
                scheme_env_vars[EnvVarKeys.CLIENT_SECRET] = f"{env_var_prefix}_CLIENT_SECRET"

                # Flow-specific params
                if auth_requirement.flow_type == "password":
                    scheme_env_vars[EnvVarKeys.USERNAME] = f"{env_var_prefix}_USERNAME"
                    scheme_env_vars[EnvVarKeys.PASSWORD] = f"{env_var_prefix}_PASSWORD"

                if auth_requirement.flow_type in ["authorizationCode", "implicit"]:
                    scheme_env_vars[EnvVarKeys.REDIRECT_URI] = f"{env_var_prefix}_REDIRECT_URI"

                scheme_env_vars[EnvVarKeys.TOKEN] = f"{env_var_prefix}_ACCESS_TOKEN"

            elif auth_requirement.auth_type == AuthType.OPENID:
                scheme_env_vars[EnvVarKeys.CLIENT_ID] = f"{env_var_prefix}_CLIENT_ID"
                scheme_env_vars[EnvVarKeys.CLIENT_SECRET] = f"{env_var_prefix}_CLIENT_SECRET"
                scheme_env_vars[EnvVarKeys.TOKEN] = f"{env_var_prefix}_ID_TOKEN"

            elif auth_requirement.auth_type == AuthType.CUSTOM:
                # For custom auth, use the name as a key
                normalized_name = self._convert_to_env_var(auth_requirement.name)
                scheme_env_vars[auth_requirement.name] = f"{env_var_prefix}_{normalized_name}"

        return environment_mappings

    def identify_auth_workflows(
        self,
        auth_requirements: list[AuthRequirement],
        arazzo_specs: list[dict[str, Any]] | None = None
    ) -> list[dict[str, Any]]:
        """
        Identify authentication workflows from Arazzo specs.
        
        Args:
            auth_requirements: List of auth requirements
            arazzo_specs: List of Arazzo workflow specifications
            
        Returns:
            List of auth workflow configurations
        """
        auth_workflows = []
        if not arazzo_specs:
            return auth_workflows

        # Look for auth-related workflows
        auth_keywords = [
            "auth", "login", "token", "authenticate", "oauth",
            "signin", "sign_in", "sign-in", "getToken", "get_token"
        ]

        # Find workflows that might be authentication workflows
        for arazzo_spec in arazzo_specs:
            workflows = arazzo_spec.get("workflows", [])
            for workflow in workflows:
                workflow_id = workflow.get("id", "")
                summary = workflow.get("summary", "").lower()
                description = workflow.get("description", "").lower()

                # Check if this is likely an auth workflow
                is_auth_workflow = False
                for keyword in auth_keywords:
                    if (keyword in workflow_id.lower() or
                        keyword in summary or
                        keyword in description):
                        is_auth_workflow = True
                        break

                if is_auth_workflow:
                    # Check the outputs for tokens
                    outputs = workflow.get("outputs", {})
                    token_output = None

                    for output_name, output_details in outputs.items():
                        if any(kw in output_name.lower() for kw in ["token", "access", "auth", "bearer"]):
                            token_output = output_name
                            break

                    auth_workflows.append({
                        "workflow_id": workflow_id,
                        "summary": workflow.get("summary", ""),
                        "token_output": token_output,
                        "outputs": list(outputs.keys())
                    })

        return auth_workflows

    @staticmethod
    def get_security_requirements_for_workflow(
        workflow_id: str,
        arazzo_spec: ArazzoDoc,
        source_descriptions: dict[str, OpenAPIDoc]
    ) -> dict[str, list[SecurityOption]]:
        """
        For a given workflow_id in an Arazzo spec (already parsed as dict),
        extract all unique SecurityOption objects for all operations in the workflow,
        grouped and deduplicated by source description.
        Args:
            workflow_id: The workflowId to extract security for
            arazzo_spec: The parsed Arazzo spec dict
            source_descriptions: Dict of OpenAPI source descriptions
        Returns:
            Dict mapping source_name to list of unique SecurityOption objects (deduplicated per source)
        """
        workflows = arazzo_spec.get("workflows", [])
        workflow = None
        for wf in workflows:
            if wf.get("workflowId") == workflow_id:
                workflow = wf
                break
        if not workflow:
            raise ValueError(f"Workflow with id '{workflow_id}' not found in Arazzo spec")

        op_finder = OperationFinder(source_descriptions)
        operations = op_finder.get_operations_for_workflow(workflow)

        # Group and merge options by source, merging options where scheme name is the same (merge scopes)
        by_source = {}
        import copy
        for op_info in operations:
            source = op_info.get("source")
            options = op_finder.extract_security_requirements(op_info)
            if not options:
                continue
            if source not in by_source:
                by_source[source] = []
            by_source[source].extend(copy.deepcopy(opt) for opt in options)
        # Merge SecurityOptions by scheme name (union all scopes per scheme)
        for source, options in by_source.items():
            scheme_to_scopes = {}
            for option in options:
                for req in option.requirements:
                    if req.scheme_name not in scheme_to_scopes:
                        scheme_to_scopes[req.scheme_name] = set()
                    scheme_to_scopes[req.scheme_name].update(req.scopes)
            merged_requirements = [
                SecurityRequirement(scheme_name=scheme, scopes=sorted(scopes))
                for scheme, scopes in scheme_to_scopes.items()
            ]
            by_source[source] = [SecurityOption(requirements=merged_requirements)] if merged_requirements else []
        return by_source

    @staticmethod
    def get_security_requirements_for_openapi_operation(
        openapi_spec: OpenAPIDoc,
        http_method: str,
        path: str
    ) -> list[SecurityOption]:
        """
        Extract SecurityOption objects for a single operation in an OpenAPI spec.
        Args:
            openapi_spec: The OpenAPI spec
            http_method: HTTP verb (e.g., 'get', 'post')
            path: The path string (e.g., '/users')
        Returns:
            List of SecurityOption objects for the operation
        """
        op_finder = OperationFinder({"default": openapi_spec})
        op_info = op_finder.find_by_http_path_and_method(path, http_method)
        if not op_info:
            raise ValueError(f"Operation {http_method.upper()} {path} not found in OpenAPI spec")
        return op_finder.extract_security_requirements(op_info)

    # Helper methods for environment variable names have been moved to utils.py
</auth_processor.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/auth/auth_parser.py:
<auth_parser.py>
# src/oak_runner/auth/auth_parser.py
"""
Authentication Parser for OAK Runner.

This module extracts authentication requirements from OpenAPI specifications
and Arazzo workflows.
"""

import logging
from typing import Any

from .models import (
    AuthLocation,
    AuthType,
    SecurityScheme,
    auth_requirement_to_schema,
)

# Configure logging
logger = logging.getLogger("arazzo-runner.auth")


class AuthRequirement:
    """Represents an authentication requirement extracted from API specifications."""

    def __init__(
        self,
        auth_type: AuthType,
        name: str,
        location: AuthLocation | None = None,
        description: str | None = None,
        required: bool = True,
        schemes: list[str] | None = None,
        scopes: list[str] | None = None,
        flow_type: str | None = None,
        auth_urls: dict[str, str] | None = None,
        security_scheme_name: str | None = None,
        api_title: str | None = None,
        source_description_id: str | None = None,
    ):
        """
        Initialize an authentication requirement.

        Args:
            auth_type: Type of authentication (API_KEY, HTTP, OAUTH2, etc.)
            name: Name of the authentication parameter or scheme
            location: Where the auth parameter should be provided
            description: Human-readable description of the auth requirement
            required: Whether this auth is required or optional
            schemes: For HTTP auth, the specific schemes (bearer, basic, etc.)
            scopes: For OAuth2, the required scopes
            flow_type: For OAuth2, the flow type (implicit, authorizationCode, etc.)
            auth_urls: For OAuth2, the authorization and token URLs
            security_scheme_name: Original name of the security scheme in the OpenAPI spec
            api_title: Title of the API source description
            source_description_id: Identifier for the source of this auth requirement (e.g., API name or file path)
        """
        self.auth_type = auth_type
        self.name = name
        self.location = location
        self.description = description
        self.required = required
        self.schemes = schemes or []
        self.scopes = scopes or []
        self.flow_type = flow_type
        self.auth_urls = auth_urls or {}
        self.security_scheme_name = security_scheme_name or name
        self.api_title = api_title
        self.source_description_id = source_description_id

    def __str__(self) -> str:
        """Return a string representation of the auth requirement."""
        base = f"{self.auth_type.value}:{self.name}"
        if self.location:
            base += f" (in {self.location.value})"
        if self.schemes:
            base += f" schemes={self.schemes}"
        if self.scopes:
            base += f" scopes={self.scopes}"
        return base

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary representation."""
        result = {
            "type": self.auth_type.value,
            "name": self.name,
            "security_scheme_name": self.security_scheme_name,
            "required": self.required,
        }

        if self.location:
            result["location"] = self.location.value

        if self.description:
            result["description"] = self.description

        if self.schemes:
            result["schemes"] = self.schemes

        if self.scopes:
            result["scopes"] = self.scopes

        if self.flow_type:
            result["flow_type"] = self.flow_type

        if self.auth_urls:
            result["auth_urls"] = self.auth_urls

        if self.api_title:
            result["api_title"] = self.api_title

        if self.source_description_id:
            result["source_description_id"] = self.source_description_id

        return result

    def to_pydantic_schema(self) -> SecurityScheme:
        """Convert to a Pydantic schema object."""
        return auth_requirement_to_schema(self)


def extract_auth_from_openapi(openapi_spec: dict[str, Any]) -> list[AuthRequirement]:
    """
    Extract authentication requirements from an OpenAPI specification.

    Args:
        openapi_spec: Parsed OpenAPI specification

    Returns:
        List of authentication requirements
    """
    auth_requirements = []

    # Extract API title from the OpenAPI spec
    api_title = openapi_spec.get("info", {}).get("title", "")

    # Check for security schemes in components
    security_schemes = openapi_spec.get("components", {}).get("securitySchemes", {})

    # Process each security scheme
    for scheme_name, scheme_data in security_schemes.items():
        auth_type_str = scheme_data.get("type", "")

        try:
            auth_type = AuthType(auth_type_str)
        except ValueError:
            logger.warning(f"Unknown security scheme type: {auth_type_str}, using CUSTOM")
            auth_type = AuthType.CUSTOM

        description = scheme_data.get("description", "")

        if auth_type == AuthType.API_KEY:
            # API Key auth
            location_str = scheme_data.get("in", "")
            try:
                location = AuthLocation(location_str)
            except ValueError:
                logger.warning(f"Unknown API key location: {location_str}, defaulting to HEADER")
                location = AuthLocation.HEADER

            param_name = scheme_data.get("name", scheme_name)

            auth_requirements.append(
                AuthRequirement(
                    auth_type=auth_type,
                    name=param_name,
                    location=location,
                    description=description,
                    security_scheme_name=scheme_name,
                    api_title=api_title,
                )
            )

        elif auth_type == AuthType.HTTP:
            # HTTP authentication (Basic, Bearer, etc.)
            scheme = scheme_data.get("scheme", "").lower()

            auth_requirements.append(
                AuthRequirement(
                    auth_type=auth_type,
                    name=scheme_name,
                    description=description,
                    schemes=[scheme],
                    location=AuthLocation.HEADER,  # HTTP auth is always in header
                    security_scheme_name=scheme_name,
                    api_title=api_title,
                )
            )

        elif auth_type == AuthType.OAUTH2:
            # OAuth2 authentication
            flows = scheme_data.get("flows", {})

            for flow_type, flow_data in flows.items():
                scopes = list(flow_data.get("scopes", {}).keys())

                auth_urls = {}
                if "authorizationUrl" in flow_data:
                    auth_urls["authorization"] = flow_data["authorizationUrl"]
                if "tokenUrl" in flow_data:
                    auth_urls["token"] = flow_data["tokenUrl"]
                if "refreshUrl" in flow_data:
                    auth_urls["refresh"] = flow_data["refreshUrl"]

                auth_requirements.append(
                    AuthRequirement(
                        auth_type=auth_type,
                        name=scheme_name,
                        description=description,
                        scopes=scopes,
                        flow_type=flow_type,
                        auth_urls=auth_urls,
                        security_scheme_name=scheme_name,
                        api_title=api_title,
                    )
                )

        elif auth_type == AuthType.OPENID:
            # OpenID Connect authentication
            # --- OAK RUNNER CHANGE: Skip unsupported OpenID Connect ---
            logger.debug(f"Skipping unsupported OpenID Connect scheme: {scheme_name}")
            continue  # Skip OpenID Connect entirely
            # --- END OAK RUNNER CHANGE ---
            # Original code for adding OpenID if supported would go here

        else:
            # Custom or unknown authentication type
            auth_requirements.append(
                AuthRequirement(
                    auth_type=AuthType.CUSTOM,
                    name=scheme_name,
                    description=description,
                    security_scheme_name=scheme_name,
                    api_title=api_title,
                )
            )

    # Check for global security requirements
    global_security = openapi_spec.get("security", [])
    if global_security:
        logger.debug(f"Found global security requirements: {global_security}")

    return auth_requirements


def extract_auth_from_arazzo(arazzo_spec: dict[str, Any]) -> list[AuthRequirement]:
    """
    Extract authentication requirements from an Arazzo workflow specification.

    Args:
        arazzo_spec: Parsed Arazzo specification

    Returns:
        List of authentication requirements
    """
    auth_requirements = []
    auth_params: set[tuple[str, str, str]] = set()  # (name, location, description)

    # Check steps for auth parameters
    steps = arazzo_spec.get("steps", [])
    for step in steps:
        # Check for API operation parameters
        operation = step.get("operation", {})
        if operation:
            # Extract parameters from operation
            params = operation.get("parameters", [])
            _extract_auth_params_from_list(params, auth_params)

    # Check for steps in workflows
    workflows = arazzo_spec.get("workflows", [])
    for workflow in workflows:
        workflow_steps = workflow.get("steps", [])
        for step in workflow_steps:
            # Extract parameters from step
            params = step.get("parameters", [])
            _extract_auth_params_from_list(params, auth_params)

    # Convert auth params to requirements
    for param_name, param_location, description in auth_params:
        auth_type = _determine_auth_type(param_name, param_location)
        location = None
        try:
            location = AuthLocation(param_location)
        except ValueError:
            logger.warning(f"Unknown parameter location: {param_location}")

        auth_requirements.append(
            AuthRequirement(
                auth_type=auth_type,
                name=param_name,
                location=location,
                description=description,
                api_title='Arazzo',
                source_description_id='Arazzo'
            )
        )

    return auth_requirements


def _extract_auth_params_from_list(
    params_list: list[dict[str, Any]], auth_params: set[tuple[str, str, str]]
):
    """
    Extract authentication parameters from a list of parameter objects.

    Args:
        params_list: List of parameter objects
        auth_params: Set to collect authentication parameters
    """
    for param in params_list:
        param_name = param.get("name", "")
        param_location = param.get("in", "")
        description = param.get("description", "")

        # Skip non-auth parameters
        if not param_name or not param_location:
            continue

        # Check if this looks like an auth parameter
        lower_name = param_name.lower()
        if (
            "api" in lower_name
            and "key" in lower_name
            or "token" in lower_name
            or "auth" in lower_name
            or "access" in lower_name
            or "secret" in lower_name
            or "credential" in lower_name
        ):
            auth_params.add((param_name, param_location, description))


def _determine_auth_type(param_name: str, param_location: str) -> AuthType:
    """
    Determine the authentication type based on parameter name and location.

    Args:
        param_name: Name of the parameter
        param_location: Location of the parameter

    Returns:
        Determined authentication type
    """
    lower_name = param_name.lower()

    if "api" in lower_name and "key" in lower_name:
        return AuthType.API_KEY

    if "bearer" in lower_name or "token" in lower_name:
        return AuthType.HTTP

    if "basic" in lower_name or ("user" in lower_name and "pass" in lower_name):
        return AuthType.HTTP

    if "oauth" in lower_name:
        return AuthType.OAUTH2

    if "openid" in lower_name:
        return AuthType.OPENID

    # Default to API Key for header/query auth parameters
    if param_location in ["header", "query"]:
        return AuthType.API_KEY

    return AuthType.CUSTOM


def auth_requirements_to_dict(auth_requirements: list[AuthRequirement]) -> list[dict[str, Any]]:
    """
    Convert auth requirements to a list of dictionaries.

    Args:
        auth_requirements: List of authentication requirements

    Returns:
        List of dictionaries
    """
    result = []
    for req in auth_requirements:
        result.append(req.to_dict())
    return result


def auth_requirements_to_pydantic(auth_requirements: list[AuthRequirement]) -> list[SecurityScheme]:
    """
    Convert authentication requirements to Pydantic schema objects.

    Args:
        auth_requirements: List of authentication requirements

    Returns:
        List of Pydantic schema objects
    """
    return [req.to_pydantic_schema() for req in auth_requirements]


def format_auth_requirements_markdown(auth_requirements: list[AuthRequirement]) -> str:
    """
    Format authentication requirements as Markdown.

    Args:
        auth_requirements: List of authentication requirements

    Returns:
        Markdown formatted string
    """
    if not auth_requirements:
        return "No authentication requirements found."

    lines = ["# Authentication Requirements", ""]

    # Group by auth type
    by_type = {}
    for req in auth_requirements:
        auth_type = req.auth_type.value
        if auth_type not in by_type:
            by_type[auth_type] = []
        by_type[auth_type].append(req)

    # Format each auth type
    for auth_type, reqs in by_type.items():
        lines.append(f"## {auth_type.title()} Authentication")
        lines.append("")

        for req in reqs:
            lines.append(f"### {req.name}")
            if req.description:
                lines.append(f"*{req.description}*")
            lines.append("")

            details = []
            if req.location:
                details.append(f"- **Location**: {req.location.value}")
            if req.required:
                details.append("- **Required**: Yes")
            else:
                details.append("- **Required**: No")

            if req.auth_type == AuthType.HTTP and req.schemes:
                details.append(f"- **Scheme**: {', '.join(req.schemes)}")

            if req.auth_type == AuthType.OAUTH2:
                if req.flow_type:
                    details.append(f"- **Flow**: {req.flow_type}")
                if req.scopes:
                    details.append(f"- **Scopes**: {', '.join(req.scopes)}")
                if req.auth_urls:
                    for url_type, url in req.auth_urls.items():
                        details.append(f"- **{url_type.title()} URL**: {url}")

            lines.extend(details)
            lines.append("")

    return "\n".join(lines)


def summarize_auth_requirements(auth_requirements: list[AuthRequirement]) -> str:
    """
    Generate a concise summary of authentication requirements.

    Args:
        auth_requirements: List of authentication requirements

    Returns:
        Summary string
    """
    if not auth_requirements:
        return "No authentication required."

    # Count by type
    type_counts = {}
    for req in auth_requirements:
        auth_type = req.auth_type.value
        if auth_type not in type_counts:
            type_counts[auth_type] = 0
        type_counts[auth_type] += 1

    summary_parts = []
    for auth_type, count in type_counts.items():
        if count == 1:
            # Get the specific requirement
            for req in auth_requirements:
                if req.auth_type.value == auth_type:
                    if auth_type == "apiKey":
                        location = req.location.value if req.location else "header"
                        summary_parts.append(f"API Key ({req.name} in {location})")
                    elif auth_type == "http":
                        scheme = req.schemes[0] if req.schemes else "bearer"
                        summary_parts.append(f"{scheme.title()} Authentication")
                    elif auth_type == "oauth2":
                        flow = req.flow_type if req.flow_type else "authorization code"
                        summary_parts.append(f"OAuth2 ({flow})")
                    elif auth_type == "openIdConnect":
                        summary_parts.append("OpenID Connect")
                    else:
                        summary_parts.append(f"{auth_type.title()} Authentication")
                    break
        else:
            # Multiple requirements of the same type
            if auth_type == "apiKey":
                summary_parts.append(f"{count} API Keys")
            elif auth_type == "http":
                summary_parts.append(f"{count} HTTP Authentication methods")
            elif auth_type == "oauth2":
                summary_parts.append(f"{count} OAuth2 flows")
            elif auth_type == "openIdConnect":
                summary_parts.append(f"{count} OpenID Connect providers")
            else:
                summary_parts.append(f"{count} {auth_type.title()} Authentication methods")

    if len(summary_parts) == 1:
        return f"Authentication required: {summary_parts[0]}"
    else:
        return f"Authentication required: {', '.join(summary_parts[:-1])} and {summary_parts[-1]}"
</auth_parser.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/auth/credentials/fetch.py:
<fetch.py>
# src/oak_runner/auth/credentials/fetch.py
import logging
import os
from abc import ABC, abstractmethod
from dataclasses import dataclass

import requests

from oak_runner.auth.auth_parser import AuthLocation, AuthRequirement, AuthType, HttpSchemeType
from oak_runner.auth.credentials.models import Credential
from oak_runner.auth.models import (
    ApiKeyAuth,
    ApiKeyScheme,
    AuthorizationCodeFlow,
    AuthValue,
    BasicAuth,
    BearerAuth,
    ClientCredentialsFlow,
    CustomScheme,
    EnvVarKeys,
    HttpAuthScheme,
    ImplicitFlow,
    OAuth2AccessTokenOnly,
    OAuth2Flows,
    OAuth2FlowType,
    OAuth2Scheme,
    OAuth2Urls,
    OpenIDScheme,
    PasswordFlow,
    SecurityOption,
    SecurityScheme,
)

logger = logging.getLogger(__name__)


@dataclass
class FetchOptions:
    """Optional parameters that tweak how credentials are looked-up or resolved
    by a `FetchStrategy`.

    Parameters
    ----------
    source_name: str | None, default ``None``
        Identifier of the *source description* (for example the filename of an
        external OpenAPI document or an API server name) whose security scheme
        definitions should be consulted.  When ``None`` each strategy falls
        back to its own default (commonly ``"default"`` or the first available
        source).
    """
    source_name: str | None = None


class FetchStrategy(ABC):
    """Defines the synchronous interface that all credential-fetch mechanisms
    must implement.

    A concrete `FetchStrategy` is responsible for turning *security options*—the
    set of `SecurityOption` objects required for a request—into concrete
    `Credential` instances.

    Lifecycle
    ~~~~~~~~~
    1. ``fetch`` / ``fetch_one`` are invoked at runtime to retrieve the actual
    credentials needed for outgoing requests. These may involve network calls,
    secret-store look-ups, environment variable reads, etc.

    Methods
    -------
    fetch(requests, options)
        Retrieve credentials for a batch of ``SecurityOption`` instances.
    fetch_one(request, options)
        Retrieve credentials for a single ``SecurityOption`` instance.
    """

    @abstractmethod
    def fetch(self, requests: list[SecurityOption], options: FetchOptions | None = None) -> list[Credential]:
        """Fetch credential(s) based on requests."""
        raise NotImplementedError

    @abstractmethod
    def fetch_one(self, request: SecurityOption, options: FetchOptions | None = None) -> list[Credential]:
        """Fetch credential(s) based on request."""
        raise NotImplementedError


class EnvironmentVariableFetchStrategy(FetchStrategy):
    """Fetch credentials from environment variables."""

    def __init__(
        self,
        env_mapping: dict[str, str] | None = None,
        http_client: requests.Session | None = None,
        auth_requirements: list[AuthRequirement] = None
    ):
        self._env_mapping: dict[str, str] = env_mapping or {}
        self._http_client: requests.Session | None = http_client
        self._auth_requirements: list[AuthRequirement] = auth_requirements or []
        self._security_schemes: dict[str, dict[str, SecurityScheme]] = \
            create_security_schemes_from_auth_requirements(self._auth_requirements)

    def fetch_one(self, request: SecurityOption, options: FetchOptions | None = None) -> list[Credential]:
        """
        Fetch credential from environment variable.
        """
        logger.debug(f"Fetching credential for {request=}")
        credentials = []
        source_name = options.source_name if options else "default"

        for requirement in request.requirements:
            scheme_name = requirement.scheme_name
            logger.debug(f'Resolving auth scheme: {scheme_name} from source: {source_name}')

            # Try to find the scheme using source description if available
            scheme = self._get_security_scheme(scheme_name, source_name)
            if not scheme:
                continue

            logger.debug(f'Found matching auth scheme: {scheme_name}')
            credentials.append(
                Credential(
                    id=f"env-{scheme_name}",
                    security_scheme=scheme,
                    auth_value=self._resolve_auth_value(scheme_name, source_name, requirement.scopes)
                )
            )

        return credentials

    def fetch(self, requests: list[SecurityOption], options: FetchOptions | None = None) -> list[Credential]:
        """Fetch credential from environment variable."""
        # Fetch credentials for each request one at a time, as its going to the env
        # we dont need to batch this (but we could)
        credentials = []
        for req in requests:
            credentials.extend(self.fetch_one(req, options))
        return credentials

    ###########################################################################
    ###################### Private API ########################################
    ###########################################################################
    def _resolve_auth_value(
        self,
        scheme_name: str,
        source_name: str | None = None,
        scopes: list[str] | None = None
    ) -> AuthValue | None:
        """
        Resolve authentication value for a security scheme.
        
        Args:
            scheme: The security scheme
            source_name: Source name of the security scheme
            scopes: Optional list of scopes required for this authentication
            
        Returns:
            AuthValue if resolved, None otherwise
        """
        scheme = self._get_security_scheme(scheme_name, source_name)

        if not scheme:
            return None

        logger.debug(f"Resolving auth value for {scheme=}, {source_name=}, {scopes=}")
        if scheme.type == AuthType.API_KEY:
            logger.debug(f"Resolving API key for {scheme_name=}")
            api_key = self._loadFromEnvironment(scheme_name, EnvVarKeys.API_KEY, source_name)
            if not api_key:
                return None

            return ApiKeyAuth(
                type=AuthType.API_KEY,
                api_key=api_key
            )
        elif scheme.type == AuthType.HTTP:
            scheme: HttpAuthScheme = scheme
            # Handle HTTP auth types based on scheme
            if scheme.scheme == HttpSchemeType.BEARER:
                token = self._loadFromEnvironment(scheme_name, EnvVarKeys.TOKEN, source_name)
                if not token:
                    return None

                return BearerAuth(
                    type=AuthType.HTTP,
                    token=token
                )
            elif scheme.scheme == HttpSchemeType.BASIC:
                username = self._loadFromEnvironment(scheme_name, EnvVarKeys.USERNAME, source_name)
                password = self._loadFromEnvironment(scheme_name, EnvVarKeys.PASSWORD, source_name)
                if not username or not password:
                    return None

                return BasicAuth(
                    type=AuthType.HTTP,
                    username=username,
                    password=password
                )
            else:
                # Generic HTTP auth
                auth_value = self._loadFromEnvironment(scheme_name, EnvVarKeys.AUTH_VALUE, source_name)
                if not auth_value:
                    return None

                # Use BearerAuth as a fallback for generic HTTP auth
                return BearerAuth(
                    type=AuthType.HTTP,
                    token=auth_value
                )

        elif scheme.type == AuthType.OAUTH2:
            return self._resolve_oauth2_auth_value(scheme=scheme, scheme_name=scheme_name, source_name=source_name, scopes=scopes)

        elif scheme.type == AuthType.OPENID:
            # For OpenID, check for ID token
            id_token = self._loadFromEnvironment(scheme_name, EnvVarKeys.TOKEN, source_name)
            if not id_token:
                return None

            # Use OAuth2AccessTokenOnly for OpenID as well since they're similar
            return OAuth2AccessTokenOnly(
                type=AuthType.OPENID,
                access_token=id_token
            )

        elif scheme.type == AuthType.CUSTOM:
            # For custom auth, we need to check the scheme name
            # This is a simplification - in a real implementation, we would need to know what key to use
            # For now, we'll try to use the scheme name as the key
            auth_value = self._loadFromEnvironment(scheme_name, scheme.name, source_name)
            if not auth_value:
                return None

            # Use ApiKeyAuth as a fallback for custom auth
            return ApiKeyAuth(
                type=AuthType.CUSTOM,
                api_key=auth_value
            )

        return None

    def _resolve_oauth2_auth_value(
        self,
        scheme: OAuth2Scheme,
        scheme_name: str,
        source_name: str | None = None,
        scopes: list[str] | None = None
    ) -> AuthValue | None:
        """
        Resolve authentication value for OAuth2 security scheme.
        
        Args:
            scheme: The OAuth2 security scheme
            scheme_name: Name of the security scheme
            source_name: Optional source name of the security scheme
            scopes: Optional list of scopes required for this authentication
            
        Returns:
            AuthValue if resolved, None otherwise
        """
        logger.debug(f"Resolving OAuth2 auth value for {scheme=}, {scheme_name=}, {source_name=}, {scopes=}")
        # Determine the flow type and create a modified scheme name
        flow_type = None
        if scheme.flows.client_credentials:
            flow_type = "clientCredentials"
        elif scheme.flows.authorization_code or scheme.flows.implicit:
            flow_type = "web"
        elif scheme.flows.password:
            flow_type = "password"
        else:
            flow_type = "default"

        # Create modified scheme name based on the flow type
        modified_scheme_name = f"{scheme_name}.{flow_type}"

        # Initialize access token
        access_token = None

        # For client credentials flow, try to obtain a token dynamically first
        if flow_type == "clientCredentials" and scheme.flows.client_credentials:
            # Get client ID and secret from environment variables
            client_id = self._loadFromEnvironment(modified_scheme_name, EnvVarKeys.CLIENT_ID, source_name)
            client_secret = self._loadFromEnvironment(modified_scheme_name, EnvVarKeys.CLIENT_SECRET, source_name)

            # Get token URL from the security scheme
            token_url = scheme.flows.client_credentials.token_url

            if client_id and client_secret and token_url:
                access_token = self._request_oauth_access_token(
                    token_url=token_url,
                    client_id=client_id,
                    client_secret=client_secret,
                    scopes=scopes,
                    scheme_name=scheme_name
                )

        # If we couldn't get a token dynamically, try to load a pre-configured one as fallback
        if not access_token:
            access_token = self._loadFromEnvironment(modified_scheme_name, EnvVarKeys.TOKEN, source_name)

        if not access_token:
            return None

        return OAuth2AccessTokenOnly(
            type=AuthType.OAUTH2,
            access_token=access_token
        )

    def _request_oauth_access_token(
        self,
        token_url: str,
        client_id: str,
        client_secret: str,
        scopes: list[str] | None = None,
        scheme_name: str = ""
    ) -> str | None:
        """
        Request an OAuth2 access token using client credentials flow.
        
        Args:
            token_url: The token endpoint URL
            client_id: The OAuth2 client ID
            client_secret: The OAuth2 client secret
            scopes: Optional list of scopes required for this authentication
            scheme_name: Name of the security scheme for logging purposes
            
        Returns:
            Access token if successful, None otherwise
        """
        try:
            # Prepare the request for client credentials grant
            headers = {
                "Content-Type": "application/x-www-form-urlencoded",
                "Accept": "application/json"
            }

            data = {
                "grant_type": "client_credentials",
                "client_id": client_id,
                "client_secret": client_secret
            }

            # Add scopes if available
            if scopes:
                scopes_str = " ".join(scopes)
                data["scope"] = scopes_str

            # Make the token request
            response = self._http_client.post(token_url, headers=headers, data=data)

            if response.status_code == 200:
                token_data = response.json()
                access_token = token_data.get("access_token")

                # Log success but not the actual token
                logger.info(f"Successfully obtained OAuth2 access token for {scheme_name}")
                return access_token
            else:
                logger.warning(f"Failed to obtain OAuth2 access token: {response.status_code} {response.text}")
        except Exception as e:
            logger.error(f"Error obtaining OAuth2 access token: {str(e)}")

        return None

    def _loadFromEnvironment(
        self,
        scheme_name: str,
        mapping_key: str,
        source_name: str | None = None
    ) -> str | None:
        """
        Load a value from environment variables.
        
        Args:
            scheme_name: Name of the security scheme
            mapping_key: Key to look up in the environment mappings
            source_name: Optional source name to construct composite key
            
        Returns:
            Value from environment if found, None otherwise
        """
        logger.debug(f"Loading from environment: {scheme_name=}, {mapping_key=}, {source_name=}")
        # First try with the direct scheme name
        env_var_name: str | None = self._env_mapping.get(scheme_name, {}).get(mapping_key)
        if env_var_name:
            logger.debug(f"Found env var name: {env_var_name=}")
            return os.getenv(env_var_name, default=None)

        # Try to find using source name as the outer key
        if source_name and source_name in self._env_mapping:
            logger.debug(f"Found source name: {source_name=}")
            source_mappings = self._env_mapping[source_name]
            if isinstance(source_mappings, dict) and scheme_name in source_mappings:
                logger.debug(f"Found scheme name: {scheme_name=}")
                scheme_mappings = source_mappings[scheme_name]
                if isinstance(scheme_mappings, dict) and mapping_key in scheme_mappings:
                    logger.debug(f"Found mapping key: {mapping_key=}")
                    env_var_name = scheme_mappings[mapping_key]
                    if env_var_name:
                        logger.debug(f"Found env var name: {env_var_name=}")
                        return os.getenv(env_var_name, default=None)

        return None

    def _get_security_scheme(self, scheme_name: str, source_name: str | None = None) -> SecurityScheme | None:
        """
        Get a security scheme by name and source name.
        
        Args:
            scheme_name: Name of the security scheme
            source_name: Source name of the security scheme
            
        Returns:
            SecurityScheme if found, None otherwise
        """
        # If source name is provided, try to find the scheme in that source
        if source_name and source_name in self._security_schemes:
            if scheme_name in self._security_schemes[source_name]:
                return self._security_schemes[source_name][scheme_name]

        # If not found with source name or no source name provided,
        # try to find the scheme in any source
        for _source, schemes in self._security_schemes.items():
            if scheme_name in schemes:
                return schemes[scheme_name]

        return None


def create_security_schemes_from_auth_requirements(auth_requirements: list[AuthRequirement]) -> dict[str, dict[str, SecurityScheme]]:
    """
    Convert AuthRequirement dictionaries to SecurityScheme objects.
    
    Returns:
        Dictionary mapping source descriptions to dictionaries of scheme names to SecurityScheme objects
    """
    security_schemes = {}
    for req in auth_requirements:
        scheme_name = req.get("security_scheme_name")
        if not scheme_name:
            continue

        auth_type = req.get("type")
        if not auth_type:
            continue

        # Get the source description, defaulting to a generic value if not available
        source_description = req.get("source_description_id", "default")

        # Initialize the source description dictionary if it doesn't exist
        if source_description not in security_schemes:
            security_schemes[source_description] = {}

        # Check if we already have a scheme with this name for this source
        existing_scheme = security_schemes[source_description].get(scheme_name)

        # Create the appropriate SecurityScheme based on auth_type
        if auth_type == AuthType.API_KEY:
            # Create API Key scheme
            scheme = ApiKeyScheme(
                type=AuthType.API_KEY,
                name=req.get("name", ""),
                description=req.get("description"),
                location=req.get("location", AuthLocation.HEADER),
                parameter_name=req.get("name", "")
            )

        elif auth_type == AuthType.HTTP:
            # Create HTTP scheme
            scheme = HttpAuthScheme(
                type=AuthType.HTTP,
                name=req.get("name", ""),
                description=req.get("description"),
                scheme=req.get("schemes", ["bearer"])[0] if req.get("schemes") else "bearer"
            )

        elif auth_type == AuthType.OAUTH2:
            # Create OAuth2 URLs
            auth_urls = req.get("auth_urls", {})
            oauth2_urls = OAuth2Urls(
                authorization=auth_urls.get("authorization"),
                token=auth_urls.get("token"),
                refresh=auth_urls.get("refresh")
            )

            # If we already have an OAuth2 scheme, we'll merge the flows
            if existing_scheme and existing_scheme.type == AuthType.OAUTH2:
                # Use the existing scheme and just update its flows
                scheme = existing_scheme

                # Create OAuth2 flows based on flow_type
                flow_type = req.get("flow_type")
                scopes_dict = {scope: f"Scope: {scope}" for scope in req.get("scopes", [])}

                # Update the appropriate flow based on flow_type
                if flow_type == OAuth2FlowType.IMPLICIT:
                    # Create implicit flow
                    scheme.flows.implicit = ImplicitFlow(
                        scopes=scopes_dict,
                        authorization_url=oauth2_urls.authorization or ""
                    )

                elif flow_type == OAuth2FlowType.CLIENT_CREDENTIALS:
                    # Create client credentials flow
                    scheme.flows.client_credentials = ClientCredentialsFlow(
                        scopes=scopes_dict,
                        token_url=oauth2_urls.token or ""
                    )

                elif flow_type == OAuth2FlowType.AUTHORIZATION_CODE:
                    # Create authorization code flow
                    scheme.flows.authorization_code = AuthorizationCodeFlow(
                        scopes=scopes_dict,
                        authorization_url=oauth2_urls.authorization or "",
                        token_url=oauth2_urls.token or "",
                        refresh_url=oauth2_urls.refresh
                    )

                elif flow_type == OAuth2FlowType.PASSWORD:
                    # Create password flow
                    scheme.flows.password = PasswordFlow(
                        scopes=scopes_dict,
                        token_url=oauth2_urls.token or ""
                    )

                # Skip adding the scheme since we're just updating the existing one
                continue
            else:
                # Create a new OAuth2 scheme

                # Create OAuth2 flows based on flow_type
                flows = OAuth2Flows()
                flow_type = req.get("flow_type")
                scopes_dict = {scope: f"Scope: {scope}" for scope in req.get("scopes", [])}

                if flow_type == OAuth2FlowType.IMPLICIT:
                    # Create implicit flow
                    flows.implicit = ImplicitFlow(
                        scopes=scopes_dict,
                        authorization_url=oauth2_urls.authorization or ""
                    )

                elif flow_type == OAuth2FlowType.CLIENT_CREDENTIALS:
                    # Create client credentials flow
                    flows.client_credentials = ClientCredentialsFlow(
                        scopes=scopes_dict,
                        token_url=oauth2_urls.token or ""
                    )

                elif flow_type == OAuth2FlowType.AUTHORIZATION_CODE:
                    # Create authorization code flow
                    flows.authorization_code = AuthorizationCodeFlow(
                        scopes=scopes_dict,
                        authorization_url=oauth2_urls.authorization or "",
                        token_url=oauth2_urls.token or "",
                        refresh_url=oauth2_urls.refresh
                    )

                elif flow_type == OAuth2FlowType.PASSWORD:
                    # Create password flow
                    flows.password = PasswordFlow(
                        scopes=scopes_dict,
                        token_url=oauth2_urls.token or ""
                    )

                # Create the OAuth2 scheme with the flows
                scheme = OAuth2Scheme(
                    type=AuthType.OAUTH2,
                    name=req.get("name", ""),
                    description=req.get("description"),
                    flows=flows
                )

        elif auth_type == AuthType.OPENID:
            # Create OpenID scheme
            scheme = OpenIDScheme(
                type=AuthType.OPENID,
                name=req.get("name", ""),
                description=req.get("description"),
                openid_connect_url=req.get("openid_connect_url", "")
            )

        else:
            # Create custom scheme
            scheme = CustomScheme(
                type=AuthType.CUSTOM,
                name=req.get("name", ""),
                description=req.get("description")
            )

        security_schemes[source_description][scheme_name] = scheme

    return security_schemes
</fetch.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/auth/credentials/models.py:
<models.py>
# src/oak_runner/auth/credentials/models.py
from dataclasses import dataclass, field
from typing import Any

from oak_runner.auth.models import AuthValue, RequestAuthValue, SecurityScheme


@dataclass
class Credential:
    """Container object that groups together all information related to a single
    credential while it passes through the oak-runner pipeline.

    The instance is created early by the credentials provider with a
    `id` and optional free-form `metadata`.  Subsequent pipeline stages
    (transform, validate, etc.) enrich the same instance in place instead of
    returning new structures, which keeps the data flow simple and avoids
    excessive object creation.

    Attributes
    ----------
    id: str
        identifier for the credential (Not unique, may be the same for multiple credentials)
    metadata: Dict[str, Any]
        Arbitrary auxiliary data coming from the provider. Can include issuer
        information, requested scopes, location hints, etc.
    auth_value: AuthValue | None
        Normalised credential value as it appears in the OpenAPI document.
    request_auth_value: RequestAuthValue | None
        Credential value formatted for direct injection into an outgoing HTTP
        request (e.g., header, query param, cookie).
    security_scheme: SecurityScheme | None
        The OpenAPI `SecurityScheme` object that this credential fulfils.
    """
    id: str
    metadata: dict[str, Any] = field(default_factory=dict)

    # Fields that are populated by the provider and/or transformer
    auth_value: AuthValue | None = None
    request_auth_value: RequestAuthValue | None = None
    security_scheme: SecurityScheme | None = None
</models.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/auth/credentials/provider.py:
<provider.py>
# src/oak_runner/auth/credentials/provider.py
"""
Credential Provider System - A flexible, extensible credential management system
with composite parts, fetch strategies, caching, and validation pipelines.
"""

import logging

import requests

from oak_runner.auth.auth_parser import AuthRequirement
from oak_runner.auth.credentials.fetch import (
    EnvironmentVariableFetchStrategy,
    FetchOptions,
    FetchStrategy,
)
from oak_runner.auth.credentials.models import Credential
from oak_runner.auth.credentials.transform import (
    CredentialToRequestAuthValueTransformer,
    CredentialTransformer,
)
from oak_runner.auth.credentials.validate import CredentialValidator, ValidCredentialValidator
from oak_runner.auth.models import RequestAuthValue, SecurityOption
from oak_runner.utils import deprecated

logger = logging.getLogger(__name__)

# Main credential provider class
class CredentialProvider:
    """
    Main credential provider that orchestrates fetching, caching, 
    validation, and transformation.
    """

    def __init__(
        self,
        strategy: FetchStrategy,
        validators: list[CredentialValidator] | None = None,
        transformers: list[CredentialTransformer] | None = None
    ):
        self.strategy: FetchStrategy = strategy
        self.validators: list[CredentialValidator] = validators or []
        self.transformers: list[CredentialTransformer] = transformers or []

    ## Public API ##
    def get_credential(self, request: SecurityOption, fetch_options: FetchOptions | None = None) -> list[Credential]:
        # Fetch credential
        logger.debug(f"Fetching credential for {request=}")
        credentials = self.strategy.fetch([request], fetch_options)

        # Validate
        if not self._are_valid_credentials(credentials):
            logger.warning(f"Failed to fetch valid credentials for {request=}")
            # Return empty list instead of exception, this is the old behaviour
            return []

        # Transform
        credentials = self._transform_credentials(credentials)
        return credentials

    def get_credentials(self, requests: list[SecurityOption], fetch_options: FetchOptions | None = None) -> list[Credential]:
        credentials = []
        for request in requests:
            credentials.extend(self.get_credential(request, fetch_options))
        return credentials

    # Deprecated API #
    @deprecated("Use get_credentials() instead, this will be removed in a future release")
    def resolve_credentials(self, security_options: list[SecurityOption], source_name: str | None = None) -> list[RequestAuthValue]:
        creds = self.get_credentials(security_options, FetchOptions(source_name=source_name))
        return [cred.request_auth_value for cred in creds]

    ## Private API ##
    def _are_valid_credentials(self, credentials: list[Credential]) -> bool:
        """Run all validators on the credentials."""
        for credential in credentials:
            for validator in self.validators:
                if not validator.validate(credential):
                    return False
        return True

    def _transform_credentials(self, credentials: list[Credential]) -> list[Credential]:
        """Apply all transformers to the credentials."""
        results = []
        for credential in credentials:
            result = credential
            for transformer in self.transformers:
                result = transformer.transform(result)
            results.append(result)
        return results

    def __str__(self) -> str:
        return f"CredentialProvider(strategy={self.strategy}, validators={self.validators}, transformers={self.transformers})"


# Factory for common configurations
class CredentialProviderFactory:
    """Factory for creating common credential provider configurations."""

    @staticmethod
    def create_default(
        env_mapping: dict[str, str],
        http_client: requests.Session | None = None,
        auth_requirements: list[AuthRequirement] | None = None
    ) -> CredentialProvider:
        """Create a default credential provider with EnvironmentVariableFetchStrategy"""
        return CredentialProvider(
            strategy=EnvironmentVariableFetchStrategy(env_mapping, http_client, auth_requirements),
            validators=[ValidCredentialValidator()],
            transformers=[CredentialToRequestAuthValueTransformer()]
        )
</provider.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/auth/credentials/__init__.py:
<__init__.py>
# src/oak_runner/auth/credentials/__init__.py
</__init__.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/auth/credentials/validate.py:
<validate.py>
# src/oak_runner/auth/credentials/validate.py
import logging
from abc import ABC, abstractmethod

from oak_runner.auth.credentials.models import Credential

logger = logging.getLogger(__name__)

class CredentialValidator(ABC):
    """Abstract base class for credential validators."""

    @abstractmethod
    def validate(self, credential: Credential) -> bool:
        """Validate a credential."""
        raise NotImplementedError


class ValidCredentialValidator(CredentialValidator):
    """Validator that checks to see if we have a valid credential."""

    def validate(self, credential: Credential) -> bool:
        # Check if we have a security scheme and auth_value set, if not this isnt valid
        if not credential.security_scheme:
            logger.warning(f"Credential has no security scheme: {credential}")
            return False
        if not credential.auth_value:
            logger.warning(f"Credential has no auth value: {credential}")
            return False
        return True
</validate.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/auth/credentials/transform.py:
<transform.py>
# src/oak_runner/auth/credentials/transform.py
import base64
import logging
from abc import ABC, abstractmethod

from oak_runner.auth.auth_parser import (
    AuthLocation,
    AuthType,
)
from oak_runner.auth.credentials.models import Credential
from oak_runner.auth.models import (
    AuthLocation,
    BasicAuth,
    BearerAuth,
    RequestAuthValue,
)

logger = logging.getLogger(__name__)

class CredentialTransformer(ABC):
    """Abstract base class for credential transformers."""

    @abstractmethod
    def transform(self, credential: Credential) -> Credential:
        """Transform a credential."""
        raise NotImplementedError


class CredentialToRequestAuthValueTransformer(CredentialTransformer):
    """Transform Credential and create RequestAuthValue."""

    def transform(self, credential: Credential) -> Credential:
        match credential.auth_value.type:
            case AuthType.API_KEY:
                credential.request_auth_value = RequestAuthValue(
                    location=credential.security_scheme.location,
                    name=credential.security_scheme.name,
                    auth_value=credential.auth_value.api_key
                )
            case AuthType.HTTP:  # Handle HTTP auth types
                if isinstance(credential.auth_value, BearerAuth):
                    credential.request_auth_value = RequestAuthValue(
                        location=credential.security_scheme.location,
                        name="Authorization",
                        auth_value=f"Bearer {credential.auth_value.token}"
                    )
                elif isinstance(credential.auth_value, BasicAuth):
                    # Basic auth requires base64 encoding of username:password
                    auth_string = f"{credential.auth_value.username}:{credential.auth_value.password}"
                    encoded = base64.b64encode(auth_string.encode()).decode()
                    credential.request_auth_value = RequestAuthValue(
                        location=credential.security_scheme.location,
                        name="Authorization",
                        auth_value=f"Basic {encoded}"
                    )

            case AuthType.OAUTH2 | AuthType.OPENID:
                credential.request_auth_value = RequestAuthValue(
                    location=AuthLocation.HEADER,
                    name="Authorization",
                    auth_value=f"Bearer {credential.auth_value.access_token}"
                )
            case AuthType.CUSTOM:
                if hasattr(credential.auth_value, 'api_key'):
                    credential.request_auth_value = RequestAuthValue(
                        location=credential.security_scheme.location,
                        name=credential.security_scheme.name,
                        auth_value=credential.auth_value.api_key
                    )
            case _:
                logger.warning(f"No conversion available for auth type: {credential.auth_value.type}")

        return credential
</transform.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/__init__.py:
<__init__.py>
# src/oak_runner/__init__.py
"""
OAK Runner

A library for executing Arazzo workflows step-by-step and OpenAPI operations.
"""

from .models import (
    ActionType,
    ExecutionState,
    StepStatus,
    WorkflowExecutionResult,
    WorkflowExecutionStatus,
)
from .runner import OAKRunner

__all__ = ["OAKRunner", "StepStatus", "ExecutionState", "ActionType", "WorkflowExecutionStatus", "WorkflowExecutionResult"]
</__init__.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/extractor/openapi_extractor.py:
<openapi_extractor.py>
# src/oak_runner/extractor/openapi_extractor.py
"""
OpenAPI Parameter and Response Extractor for OAK Runner

This module provides functionality to extract input parameters and output schemas
from an OpenAPI specification for a given API operation.
"""

import copy
import logging
import re
from typing import Any

import jsonpointer

from oak_runner.auth.models import SecurityOption
from oak_runner.executor.operation_finder import OperationFinder

# Configure logging (using the same logger as operation_finder for consistency)
logger = logging.getLogger("oak_runner.extractor")


def _format_security_options_to_dict_list(
    security_options_list: list[SecurityOption],
    operation_info: dict[str, Any] # For logging context
) -> list[dict[str, list[str]]]:
    """
    Converts a list of SecurityOption objects into a list of dictionaries
    representing OpenAPI security requirements.

    Args:
        security_options_list: The list of SecurityOption objects.
        operation_info: The operation details dictionary for logging context.

    Returns:
        A list of dictionaries, where each dictionary represents an OR security option,
        and its key-value pairs represent ANDed security schemes.
    """
    formatted_requirements = []
    if not security_options_list:
        return formatted_requirements

    for sec_opt in security_options_list:
        current_option_dict = {}
        if sec_opt.requirements:  # Check if the list is not None and not empty
            for sec_req in sec_opt.requirements:
                try:
                    current_option_dict[sec_req.scheme_name] = sec_req.scopes
                except AttributeError as e:
                    op_path = operation_info.get('path', 'unknown_path')
                    op_method = operation_info.get('http_method', 'unknown_method').upper()
                    logger.warning(
                        f"Missing attributes on SecurityRequirement object for operation {op_method} {op_path}. Error: {e}"
                    )

        # Handle OpenAPI's concept of an empty security requirement object {},
        # (optional authentication), represented by an empty list of requirements.
        if sec_opt.requirements == []: # Explicitly check for an empty list
            formatted_requirements.append({})
        elif current_option_dict: # Add if populated from non-empty requirements
            formatted_requirements.append(current_option_dict)

    return formatted_requirements


def _resolve_ref(spec: dict[str, Any], ref: str) -> dict[str, Any]:
    """
    Resolves a JSON pointer $ref, returning the referenced dictionary.
    """
    logger.debug(f"Attempting to resolve ref: {ref}")
    try:
        # Ensure the ref starts with '#/' as expected for internal refs
        if not ref.startswith('#/'):
            # Currently only supporting internal references
            raise ValueError(f"Invalid or unsupported $ref format: {ref}. Only internal refs starting with '#/' are supported.")
        try:
            # Remove the leading '#' before resolving
            resolved_data = jsonpointer.resolve_pointer(spec, ref[1:])
            # If the resolved part itself contains a $ref, resolve it recursively
            if isinstance(resolved_data, dict) and '$ref' in resolved_data:
                # Prevent infinite loops for recursive refs (simple check)
                if resolved_data['$ref'] == ref:
                    logger.warning(f"Detected self-referencing $ref, stopping recursion: {ref}")
                    return resolved_data  # Return as is, let caller handle
                return _resolve_ref(spec, resolved_data['$ref'])
            if not isinstance(resolved_data, dict):
                logger.warning(f"Resolved $ref '{ref}' is not a dictionary, returning empty dict.")
                return {}
            # Return a deep copy to prevent modification of the original spec component
            logger.debug(f"Resolved ref '{ref}' to: {resolved_data}")
            return copy.deepcopy(resolved_data)
        except jsonpointer.JsonPointerException as e:
            logger.error(f"Could not resolve reference '{ref}': {e}")
            raise
        except Exception as e:
            logger.error(f"An unexpected error occurred during $ref resolution for {ref}: {e}")
            raise
    except ValueError as e:
        logger.error(f"Invalid or unsupported $ref format: {e}")
        raise


def _resolve_schema_refs(schema_part: Any, full_spec: dict[str, Any], visited_refs: set[str] | None = None) -> Any:
    """Recursively resolves all $ref pointers within a schema fragment, handling circular references."""
    # Initialize visited_refs for the current resolution path if it's the first call in a chain
    current_visited_refs = visited_refs if visited_refs is not None else set()

    # Make a deep copy to avoid modifying original spec or intermediate dicts/lists during this call's scope
    current_part = copy.deepcopy(schema_part)

    if isinstance(current_part, dict):
        if '$ref' in current_part:
            ref_path = current_part['$ref']
            if ref_path in current_visited_refs:
                logger.debug(f"Circular reference detected for '{ref_path}'. Returning original $ref dict.")
                # Return the original reference dict to break the cycle
                return current_part

            try:
                # Add current ref_path to a new set for the next level of recursion to avoid cross-branch pollution
                next_level_visited_refs = current_visited_refs.copy()
                next_level_visited_refs.add(ref_path)

                resolved_content = _resolve_ref(full_spec, ref_path) # Resolve from ORIGINAL full_spec
                # Recursively resolve within the newly resolved content, passing the updated visited set
                result = _resolve_schema_refs(resolved_content, full_spec, next_level_visited_refs)
                return result
            except (jsonpointer.JsonPointerException, ValueError, KeyError) as e:
                logger.warning(f"Could not resolve nested $ref '{ref_path}': {e}")
                return current_part  # Return the copied dict with the unresolved $ref on error
        else:
            # Process dictionary items recursively on the copied dict
            # Pass the current_visited_refs, as these are part of the same parent schema's resolution path
            for k, v in list(current_part.items()): # Iterate over a copy of items if modifying dict during iteration
                current_part[k] = _resolve_schema_refs(v, full_spec, current_visited_refs)
            return current_part  # Return the modified copy
    elif isinstance(current_part, list):
        # Process list items recursively on the copied list
        # Pass the current_visited_refs for the same reason as above
        for i, item in enumerate(list(current_part)): # Iterate over a copy of items
            current_part[i] = _resolve_schema_refs(item, full_spec, current_visited_refs)
        return current_part  # Return the modified copy
    else:
        # Return the copy of non-dict/list items (base case)
        return current_part


def extract_operation_io(
    spec: dict[str, Any],
    http_path: str,
    http_method: str,
    input_max_depth: int | None = None,
    output_max_depth: int | None = None
) -> dict[str, dict[str, Any]]:
    """
    Finds the specified operation within the spec and extracts input parameters
    structured as an OpenAPI object schema and the full schema for the success
    (200 or 201) response.

    Args:
        spec: The full OpenAPI specification dictionary.
        http_path: The HTTP path of the target operation (e.g., '/users/{id}').
        http_method: The HTTP method of the target operation (e.g., 'get', 'post').
        input_max_depth: If set, limits the depth of the input structure.
        output_max_depth: If set, limits the depth of the output structure.

    Returns:
        A dictionary containing 'inputs', 'outputs', and 'security_requirements'. 
        Returns the full, unsimplified dict structure if both max depth arguments are None.
        'inputs' is structured like an OpenAPI schema object:
            {'type': 'object', 'properties': {param_name: {param_schema_or_simple_type}, ...}}
            Non-body params map to {'type': openapi_type_string}.
            The JSON request body schema is included under the 'body' key if present.
        'outputs' contains the full resolved schema for the 200 JSON response.
        'security_requirements' contains the security requirements for the operation.

        Example:
        {
            "inputs": {
                "type": "object",
                "properties": {
                    "userId": {"type": "integer"},   # Non-body param
                    "limit": {"type": "integer"},
                    "body": {                     # Full resolved schema for JSON request body
                        "type": "object",
                        "properties": {
                            "items": {"type": "array", "items": {"type": "string"}},
                            "customer_notes": {"type": "string"}
                        },
                        "required": ["items"]
                    }
                }
            },
            "outputs": { # Full resolved schema for 200 JSON response
                 "type": "object",
                 "properties": {
                      "id": {"type": "string", "format": "uuid"},
                      "status": {"type": "string", "enum": ["pending", "shipped"]}
                 }
            },
            "security_requirements": [
                # List of SecurityOption objects
            ]
        }
    """
    # Find the operation first using OperationFinder
    # Wrap the spec for OperationFinder
    source_name = spec.get("info", {}).get("title", "default_spec")
    source_descriptions = {source_name: spec}
    finder = OperationFinder(source_descriptions)
    operation_info = finder.find_by_http_path_and_method(http_path, http_method.lower())

    if not operation_info:
        logger.warning(f"Operation {http_method.upper()} {http_path} not found in the spec.")
        # Return early if operation not found
        return {"inputs": {}, "outputs": {}, "security_requirements": []}

    # Initialize with new structure for inputs
    extracted_details: dict[str, Any] = {
        "inputs": {"type": "object", "properties": {}, "required": []},
        "outputs": {},
        "security_requirements": []
    }
    operation = operation_info.get("operation")
    if not operation or not isinstance(operation, dict):
        logger.warning("Operation object missing or invalid in operation_info.")
        return extracted_details

    # Extract security requirements using OperationFinder
    security_options_list: list[SecurityOption] = finder.extract_security_requirements(operation_info)

    extracted_details["security_requirements"] = _format_security_options_to_dict_list(
        security_options_list, operation_info
    )

    all_parameters = []
    seen_params = set()

    # Check for path-level parameters first
    path_item_ref = f"#/paths/{operation_info.get('path', '').lstrip('/')}"
    try:
        escaped_path = operation_info.get('path', '').lstrip('/').replace('~', '~0').replace('/', '~1')
        path_item_ref = f"#/paths/{escaped_path}"
        path_item = jsonpointer.resolve_pointer(spec, path_item_ref[1:])
        if path_item and isinstance(path_item, dict) and 'parameters' in path_item:
            for param in path_item['parameters']:
                try:
                    resolved_param = param
                    if '$ref' in param:
                        resolved_param = _resolve_ref(spec, param['$ref'])
                    param_key = (resolved_param.get('name'), resolved_param.get('in'))
                    if param_key not in seen_params:
                        all_parameters.append(resolved_param)
                        seen_params.add(param_key)
                except (jsonpointer.JsonPointerException, ValueError, KeyError) as e:
                    logger.warning(f"Skipping path-level parameter due to resolution/format error: {e}")
    except jsonpointer.JsonPointerException:
        logger.debug(f"Could not find or resolve path item: {path_item_ref}")

    # Add/override with operation-level parameters
    if 'parameters' in operation:
        for param in operation['parameters']:
            try:
                resolved_param = param
                if '$ref' in param:
                    resolved_param = _resolve_ref(spec, param['$ref'])
                param_key = (resolved_param.get('name'), resolved_param.get('in'))
                existing_index = next((i for i, p in enumerate(all_parameters) if (p.get('name'), p.get('in')) == param_key), None)
                if existing_index is not None:
                    all_parameters[existing_index] = resolved_param
                elif param_key not in seen_params:
                    all_parameters.append(resolved_param)
                    seen_params.add(param_key)
            except (jsonpointer.JsonPointerException, ValueError, KeyError) as e:
                logger.warning(f"Skipping operation-level parameter due to resolution/format error: {e}")

    # --- Ensure all URL path parameters are present and required ---
    # Find all {param} in the http_path
    url_param_names = re.findall(r'{([^}/]+)}', http_path)
    for url_param in url_param_names:
        param_key = (url_param, 'path')
        if param_key not in seen_params:
            all_parameters.append({
                'name': url_param,
                'in': 'path',
                'required': True,
                'schema': {'type': 'string'}
            })
            seen_params.add(param_key)
    # --- End ensure URL params ---

    # Process collected parameters into simplified inputs
    for param in all_parameters:
        param_name = param.get('name')
        param_in = param.get('in')
        param_schema = param.get('schema')
        if param_name and param_in != 'body':  # Body handled separately
            if not param_schema:
                logger.warning(f"Parameter '{param_name}' in '{param_in}' is missing schema, mapping type to 'any'")
                param_type = 'any'
            else:
                # Resolve schema ref if present
                if isinstance(param_schema, dict) and '$ref' in param_schema:
                    try:
                        param_schema = _resolve_ref(spec, param_schema['$ref'])
                    except (jsonpointer.JsonPointerException, ValueError) as ref_e:
                        logger.warning(f"Could not resolve schema $ref for parameter '{param_name}': {ref_e}")
                        param_schema = {}  # Fallback to empty schema
            openapi_type = 'string'  # Default OpenAPI type
            if isinstance(param_schema, dict):
                oapi_type_from_schema = param_schema.get('type')
                # Map to basic OpenAPI types
                if oapi_type_from_schema in ['string', 'integer', 'number', 'boolean', 'array', 'object']:
                    openapi_type = oapi_type_from_schema
                # TODO: More nuanced mapping (e.g., number format to float/double?)?

            # Add to properties as { 'type': 'openapi_type_string' }
            # Required status will be tracked in the top-level 'required' list
            is_required = param.get('required', False) # Default to false if not present
            extracted_details["inputs"]["properties"][param_name] = {
                "type": openapi_type
                # Removed "required": is_required from here
            }
            if is_required:
                # Add to top-level required list if not already present
                if param_name not in extracted_details["inputs"]["required"]:
                    extracted_details["inputs"]["required"].append(param_name)

    # Process Request Body for inputs
    if 'requestBody' in operation:
        try:
            request_body = operation['requestBody']
            if '$ref' in request_body:
                request_body = _resolve_ref(spec, request_body['$ref'])

            # Check for application/json content
            json_content = request_body.get('content', {}).get('application/json', {})
            body_schema = json_content.get('schema')

            if body_schema:
                if '$ref' in body_schema:
                    body_schema = _resolve_ref(spec, body_schema['$ref'])

                # Recursively resolve nested refs within the body schema
                fully_resolved_body_schema = _resolve_schema_refs(body_schema, spec)

                # --- Flatten body properties into inputs ---
                if isinstance(fully_resolved_body_schema, dict) and fully_resolved_body_schema.get("type") == "object":
                    body_properties = fully_resolved_body_schema.get("properties", {})
                    for prop_name, prop_schema in body_properties.items():
                        if prop_name in extracted_details["inputs"]["properties"]:
                            # Handle potential name collisions (e.g., param 'id' and body field 'id')
                            # Current approach: Body property overwrites if name collides. Log warning.
                            logger.warning(f"Body property '{prop_name}' overwrites existing parameter with the same name.")
                        extracted_details["inputs"]["properties"][prop_name] = prop_schema

                    # Add required body properties to the main 'required' list
                    body_required = fully_resolved_body_schema.get('required', [])
                    for req_prop_name in body_required:
                        if req_prop_name not in extracted_details["inputs"]["required"]:
                            extracted_details["inputs"]["required"].append(req_prop_name)
                else:
                    # If body is not an object (e.g., array, primitive) or has no properties, don't flatten.
                    # Log a warning as we are not adding it under 'body' key either per the requirement.
                    logger.warning(f"Request body for {http_method.upper()} {http_path} is not an object with properties. Skipping flattening.")
                # --- End flatten ---

                # Removed code that added the schema under 'body'
                # Removed code that checked 'required' on the nested 'body' object

        except (jsonpointer.JsonPointerException, ValueError, KeyError) as e:
            logger.warning(f"Skipping request body processing due to error: {e}")

    # Process 200 or 201 Response for outputs
    if 'responses' in operation:
        responses = operation.get('responses', {})
        # Prioritize 200, fallback to 201 for success output schema
        success_response = responses.get('200') or responses.get('201')
        if success_response:
            try:
                resolved_response = success_response
                if isinstance(success_response, dict) and '$ref' in success_response:
                    resolved_response = _resolve_ref(spec, success_response['$ref'])

                # Check for application/json content in the resolved successful response
                json_content = resolved_response.get('content', {}).get('application/json', {})
                response_schema = json_content.get('schema')

                if response_schema:
                    if '$ref' in response_schema:
                        response_schema = _resolve_ref(spec, response_schema['$ref'])

                    # Recursively resolve nested refs within the response schema
                    logger.debug(f"Output schema BEFORE recursive resolve: {response_schema}")
                    fully_resolved_output_schema = _resolve_schema_refs(response_schema, spec)
                    logger.debug(f"Output schema AFTER recursive resolve: {fully_resolved_output_schema}")
                    extracted_details["outputs"] = fully_resolved_output_schema

            except (jsonpointer.JsonPointerException, ValueError, KeyError) as e:
                logger.warning(f"Skipping success response processing due to error: {e}")
        else:
            logger.debug("No '200' or '201' response found for this operation.")

    # --- Limit output depth (conditionally) ---
    if input_max_depth is not None:
        if isinstance(extracted_details.get("inputs"), (dict, list)):
            extracted_details["inputs"] = _limit_dict_depth(extracted_details["inputs"], input_max_depth)
    if output_max_depth is not None:
        if isinstance(extracted_details.get("outputs"), (dict, list)):
            extracted_details["outputs"] = _limit_dict_depth(extracted_details["outputs"], output_max_depth)

    # If both max depths are None, return the full, unsimplified details
    return extracted_details


def _limit_dict_depth(data: dict | list | Any, max_depth: int, current_depth: int = 0) -> dict | list | Any:
    """Recursively limits the depth of a dictionary or list structure."""

    if isinstance(data, dict):
        if current_depth >= max_depth:
            return data.get('type', 'object') # Limit hit for dict
        else:
            # Recurse into dict
            limited_dict = {}
            for key, value in data.items():
                limited_dict[key] = _limit_dict_depth(value, max_depth, current_depth + 1)
            return limited_dict
    elif isinstance(data, list):
        if current_depth >= max_depth:
            return 'array' # Limit hit for list
        else:
            # Recurse into list
            limited_list = []
            for item in data:
                limited_list.append(_limit_dict_depth(item, max_depth, current_depth + 1))
            return limited_list
    else:
        # It's a primitive, return the value itself regardless of depth
        return data
</openapi_extractor.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/utils.py:
<utils.py>
# src/oak_runner/utils.py
"""
Utility functions for OAK Runner

This module provides utility functions for the OAK Runner.
"""
import json
import logging
import os
import re
import warnings
from typing import Any

import jsonpointer
import yaml

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("arazzo-runner")


def load_arazzo_doc(arazzo_path: str) -> dict:
    """
    Load and parse the Arazzo document

    Args:
        arazzo_path: Path to the Arazzo document

    Returns:
        arazzo_doc: Parsed Arazzo document
    """
    with open(arazzo_path) as f:
        content = f.read()
        if arazzo_path.endswith((".yaml", ".yml")):
            return yaml.safe_load(content)
        else:
            return json.loads(content)


def load_source_descriptions(arazzo_doc: dict, arazzo_path: str, base_path: str, http_client) -> dict[str, Any]:
    """
    Load referenced OpenAPI descriptions

    Args:
        arazzo_doc: Parsed Arazzo document
        base_path: Base path for resolving relative paths
        http_client: HTTP client to use for loading remote sources

    Returns:
        source_descriptions: Dictionary of loaded source descriptions
    """
    source_descriptions = {}
    source_descriptions_list = arazzo_doc.get("sourceDescriptions", [])

    for source in source_descriptions_list:
        source_name = source.get("name")
        source_url = source.get("url")
        source_type = source.get("type", "openapi")

        if not source_name or not source_url:
            continue

        # Handle local file references
        if not (source_url.startswith("http://") or source_url.startswith("https://")):
            # Try four approaches to finding the file path
            candidate_paths = []
            # 1. If base_path exists, use it
            if base_path:
                candidate_paths.append(os.path.join(base_path, source_url))
            # 2. Try using a path relative to the arazzo_path if available
            if arazzo_path:
                arazzo_dir = os.path.dirname(os.path.abspath(arazzo_path))
                candidate_paths.append(os.path.join(arazzo_dir, source_url))
            # 3. Try using a path relative to the current path
            current_path = os.path.abspath(os.getcwd())
            candidate_paths.append(os.path.join(current_path, source_url))
            # 4. If current path contains '/tools/oak-runner', set base_path up 2 levels
            if "/tools/oak-runner" in current_path:
                base_path_2up = os.path.abspath(os.path.join(current_path, "../.."))
                candidate_paths.append(os.path.join(base_path_2up, source_url))
            # Try each candidate path
            source_path = None
            for path in candidate_paths:
                if os.path.exists(path):
                    source_path = path
                    break
            if not source_path:
                raise FileNotFoundError(f"Could not find source file for {source_name} using any known base path candidates: {candidate_paths}")
            try:
                with open(source_path) as f:
                    content = f.read()
                    if source_path.endswith((".yaml", ".yml")):
                        source_descriptions[source_name] = yaml.safe_load(content)
                    else:
                        source_descriptions[source_name] = json.loads(content)
            except (FileNotFoundError, json.JSONDecodeError, yaml.YAMLError) as e:
                logger.error(f"Error loading source description {source_name}: {e}")
        else:
            # Handle remote URLs
            try:
                response = http_client.get(source_url)
                response.raise_for_status()
                content_type = response.headers.get("Content-Type", "")

                if "yaml" in content_type or "yml" in content_type:
                    source_descriptions[source_name] = yaml.safe_load(response.text)
                else:
                    source_descriptions[source_name] = response.json()
            except Exception as e:
                logger.error(f"Error loading remote source description {source_name}: {e}")

    return source_descriptions


def dump_state(state, label: str = "Current Execution State"):
    """
    Helper method to dump the current state for debugging

    Args:
        state: Execution state to dump
        label: Optional label for the state dump
    """
    logger.debug(f"=== {label} ===")
    logger.debug(f"Workflow ID: {state.workflow_id}")
    logger.debug(f"Current Step ID: {state.current_step_id}")
    logger.debug(f"Inputs: {state.inputs}")
    logger.debug("Step Outputs:")
    for step_id, outputs in state.step_outputs.items():
        logger.debug(f"  {step_id}: {outputs}")
    logger.debug(f"Workflow Outputs: {state.workflow_outputs}")
    logger.debug(f"Status: {state.status}")


def evaluate_json_pointer(data: dict, pointer_path: str) -> Any | None:
    """
    Evaluate a JSON pointer against the provided data.

    Args:
        data: The data to evaluate the pointer against
        pointer_path: The JSON pointer path (e.g., "/products/0/name")

    Returns:
        The resolved value or None if the pointer cannot be resolved
    """
    try:
        if not pointer_path:
            return data

        # For root pointer, return the entire data
        if pointer_path == "/":
            return data

        # Create a JSON pointer resolver
        pointer = jsonpointer.JsonPointer(pointer_path)
        result = pointer.resolve(data)
        return result
    except (jsonpointer.JsonPointerException, TypeError) as e:
        logger.debug(f"Error resolving JSON pointer {pointer_path}: {e}")
        return None


def extract_json_pointer_from_expression(expression: str) -> tuple[str | None, str | None]:
    """
    Extract JSON pointer from an expression like $response.body#/path/to/value

    Args:
        expression: The expression containing a JSON pointer

    Returns:
        A tuple of (container_path, pointer_path) or (None, None) if not a valid pointer expression
    """
    if not isinstance(expression, str):
        return (None, None)

    # Handle the form $response.body#/path/to/value
    match = re.match(r"^\$([a-zA-Z0-9_.]+)#(/.*)", expression)
    if match:
        container_path, pointer_path = match.groups()
        return (container_path, pointer_path)

    # Handle expressions like $response.body.path.to.value by converting to JSON pointer
    # This supports workflows that don't explicitly use # JSON pointer syntax
    match = re.match(r"^\$([a-zA-Z0-9_]+)\.([a-zA-Z0-9_.]+)", expression)
    if match and "#" not in expression:
        container, path = match.groups()
        # Convert dot notation to JSON pointer format
        pointer_path = "/" + path.replace(".", "/")
        return (container, pointer_path)

    # Handle the standard form $response.body#/path
    match = re.match(r"^\$([a-zA-Z0-9_.]+)\.([a-zA-Z0-9_]+)#(/.*)", expression)
    if match:
        container, property_name, pointer_path = match.groups()
        return (f"{container}.{property_name}", pointer_path)

    return (None, None)


def load_openapi_file(openapi_path: str) -> dict[str, Any]:
    """Loads a single OpenAPI specification from a local file path.

    Args:
        openapi_path: The local file path of the OpenAPI specification.

    Returns:
        The parsed OpenAPI specification as a dictionary.

    Raises:
        ValueError: If the file cannot be parsed.
        FileNotFoundError: If the local file does not exist.
    """
    logger.debug(f"Loading OpenAPI specification from local path: {openapi_path}")

    try:
        if not os.path.isfile(openapi_path):
            raise FileNotFoundError(f"OpenAPI file not found: {openapi_path}")

        with open(openapi_path) as f:
            content = f.read()

        # Try parsing as YAML, then JSON
        try:
            return yaml.safe_load(content)
        except yaml.YAMLError:
            try:
                return json.loads(content)
            except json.JSONDecodeError as json_err:
                logger.error(f"Failed to parse OpenAPI spec as YAML or JSON from {openapi_path}: {json_err}")
                raise ValueError(f"Failed to parse OpenAPI spec from {openapi_path}") from json_err

    except FileNotFoundError:
        logger.error(f"OpenAPI file not found: {openapi_path}")
        raise
    except Exception as e:
        logger.error(f"An unexpected error occurred while loading OpenAPI spec from {openapi_path}: {e}")
        raise ValueError(f"Could not load OpenAPI spec from {openapi_path}") from e


def set_log_level(level: str):
    """
    Set the log level

    Args:
        level: Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
    """
    numeric_level = getattr(logging, level.upper(), None)
    if not isinstance(numeric_level, int):
        raise ValueError(f"Invalid log level: {level}")
    logger.setLevel(numeric_level)

    # Also set for submodules
    logging.getLogger("arazzo-runner.evaluator").setLevel(numeric_level)
    logging.getLogger("arazzo-runner.executor").setLevel(numeric_level)
    logging.getLogger("arazzo-runner.http").setLevel(numeric_level)


def sanitize_for_env_var(text: str) -> str:
    """
    Sanitize a string for use in environment variable names.
    
    Args:
        text: The text to sanitize
        
    Returns:
        Sanitized text suitable for environment variables
    """
    # Convert to uppercase
    sanitized = text.upper()

    # Replace hyphens with underscores
    sanitized = sanitized.replace('-', '_')

    # Replace other non-alphanumeric characters with underscores
    sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', sanitized)

    # Replace multiple consecutive underscores with a single underscore
    sanitized = re.sub(r'_+', '_', sanitized)

    # Remove leading and trailing underscores
    sanitized = sanitized.strip('_')

    return sanitized


def extract_api_title_prefix(title: str) -> str | None:
    """
    Derives an API title prefix from the OpenAPI spec's info.title.
    The prefix is the first non-skip word of the title, uppercased, with non-alphanumeric
    characters (excluding underscore) replaced by underscores.

    Skip words: 'the', 'a', 'an', 'openapi', 'api', 'swagger'

    Args:
        title: The info.title string from the OpenAPI spec.

    Returns:
        The sanitized API title prefix, or None if title is empty or not suitable.
    """
    if not title or not title.strip():
        logger.debug("API title is empty or not provided, no prefix will be generated.")
        return None

    # List of words to skip if they appear as the first word
    SKIP_WORDS = {'the', 'a', 'an', 'openapi', 'api', 'swagger'}

    # Split the title into words and remove any empty strings
    words = [word for word in title.strip().split() if word]

    for word in words:
        if word.lower() not in SKIP_WORDS:
            return sanitize_for_env_var(word)

    return words[0]


def create_env_var_name(
    var_name: str,
    prefix: str | None = None
) -> str:
    """
    Create a standardized environment variable name with an optional prefix.
    
    Args:
        var_name: The base variable name
        prefix: Optional prefix (e.g., "MY_API_")
        
    Returns:
        A properly formatted environment variable name
    """
    # Sanitize the base variable name
    sanitized_var_name = sanitize_for_env_var(var_name)

    # Construct parts of the environment variable name
    parts = []

    # Add prefix if provided
    if prefix:
        parts.append(prefix)

    # Add the sanitized variable name
    parts.append(sanitized_var_name)

    # Join all parts with underscores
    env_var_name = "_".join(parts)

    return env_var_name


def deprecated(reason: str):
    """
    Decorator to mark a function as deprecated.
    
    Args:
        reason: Reason for deprecation
    """
    def decorator(func):
        def wrapper(*args, **kwargs):
            warnings.warn(
                f"{func.__name__} is deprecated: {reason}",
                DeprecationWarning,
                stacklevel=2
            )
            return func(*args, **kwargs)
        return wrapper
    return decorator
</utils.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/http.py:
<http.py>
# src/oak_runner/http.py
"""
HTTP Client for OAK Runner

This module provides HTTP request handling for the OAK Runner.
"""
import logging
from typing import Any

import requests

from oak_runner.auth.credentials.fetch import FetchOptions
from oak_runner.auth.credentials.provider import CredentialProvider
from oak_runner.auth.models import AuthLocation, RequestAuthValue, SecurityOption

# Configure logging
logger = logging.getLogger("arazzo-runner.http")


class HTTPExecutor:
    """HTTP client for executing API requests in Arazzo workflows"""

    def __init__(self, http_client=None, auth_provider: CredentialProvider | None = None):
        """
        Initialize the HTTP client

        Args:
            http_client: Optional HTTP client (defaults to requests.Session)
        """
        self.http_client = http_client or requests.Session()
        self.auth_provider: CredentialProvider | None = auth_provider

    def _get_content_type_category(self, content_type: str | None) -> str:
        """
        Categorize the content type to determine how to handle the request body.
        
        Args:
            content_type: The content type string from the request body
            
        Returns:
            One of: 'multipart', 'json', 'form', 'raw', or 'unknown'
        """
        if not content_type:
            return 'unknown'

        content_type_lower = content_type.lower()

        if "multipart/form-data" in content_type_lower:
            return 'multipart'
        elif "json" in content_type_lower:
            return 'json'
        elif "form" in content_type_lower or "x-www-form-urlencoded" in content_type_lower:
            return 'form'
        else:
            return 'raw'

    def execute_request(
        self, method: str, url: str, parameters: dict[str, Any], request_body: dict | None, security_options: list[SecurityOption] | None = None, source_name: str | None = None
    ) -> dict:
        """
        Execute an HTTP request using the configured client

        Args:
            method: HTTP method (GET, POST, PUT, DELETE, etc.)
            url: URL to request
            parameters: Dictionary of parameters by location (path, query, header, cookie)
            request_body: Optional request body
            security_options: Optional list of security options for authentication
            source_name: Source API name to distinguish between APIs with conflicting scheme names

        Returns:
            response: Dictionary with status_code, headers, body
        """
        # Replace path parameters in the URL
        path_params = parameters.get("path", {})
        for name, value in path_params.items():
            url = url.replace(f"{{{name}}}", str(value))

        # Prepare query parameters
        query_params = parameters.get("query", {})

        # Prepare headers
        headers = parameters.get("header", {})

        # Prepare cookies
        cookies = parameters.get("cookie", {})

        # Log security options
        if security_options:
            logger.debug(f"Security options: {security_options}")
            for i, option in enumerate(security_options):
                logger.debug(f"Option {i} requirements: {option}")

        # Apply authentication headers from auth_provider if available
        self._apply_auth_to_request(url, headers, query_params, cookies, security_options, source_name)

        # Prepare request body
        data = None
        json_data = None
        files = None

        if request_body:
            content_type = request_body.get("contentType")
            payload = request_body.get("payload")
            content_category = self._get_content_type_category(content_type)

            # Handle explicit None payload
            if payload is None:
                if content_type:
                    # Content type specified but no payload - set header but no body
                    headers["Content-Type"] = content_type
                    logger.debug(f"Content type '{content_type}' specified but payload is None - sending empty body with header")
                # If no content_type either, just send empty body (no header needed)

            elif content_category == 'multipart':
                # Path 1: Multipart form data with file uploads
                files = {}
                data = {}
                for key, value in payload.items():
                    # A field is treated as a file upload if its value is an object
                    # containing 'content' and 'filename' keys.
                    if isinstance(value, dict) and "content" in value and "filename" in value:
                        # requests expects a tuple: (filename, file_data, content_type)
                        file_content = value["content"]
                        file_name = value["filename"] if value.get("filename") else "attachment"
                        file_type = value.get("contentType", "application/octet-stream")
                        files[key] = (file_name, file_content, file_type)
                        logger.debug(f"Preparing file '{file_name}' for upload.")
                    elif isinstance(value, (bytes, bytearray)):
                        # Fallback: treat raw bytes as a file with a generic name
                        files[key] = ("attachment", value, "application/octet-stream")
                        logger.debug(f"Preparing raw-bytes payload as file for key '{key}'.")
                    else:
                        data[key] = value
                # Do NOT set Content-Type header here; `requests` will do it with the correct boundary

            elif content_category == 'json':
                # Path 2: JSON content
                headers["Content-Type"] = content_type
                json_data = payload

            elif content_category == 'form':
                # Path 3: Form-encoded content
                headers["Content-Type"] = content_type
                if isinstance(payload, dict):
                    data = payload
                else:
                    logger.warning(f"Form content type specified, but payload is not a dictionary: {type(payload)}. Sending as raw data.")
                    data = payload

            elif content_category == 'raw':
                # Path 4: Other explicit content types (raw data)
                headers["Content-Type"] = content_type
                if isinstance(payload, (str, bytes)):
                    data = payload
                else:
                    # Attempt to serialize other types? Or raise error? Let's log and convert to string for now.
                    logger.warning(f"Payload type {type(payload)} not directly supported for raw data. Converting to string.")
                    data = str(payload)

            elif content_category == 'unknown' and payload is not None:
                # Path 5: No content type specified but payload exists - try to infer
                if isinstance(payload, dict):
                    headers["Content-Type"] = "application/json"
                    json_data = payload
                    logger.debug("No content type specified, inferring application/json for dict payload")
                elif isinstance(payload, (bytes, bytearray)):
                    data = payload
                    logger.debug("No content type specified, sending raw bytes")
                elif isinstance(payload, str):
                    data = payload
                    logger.debug("No content type specified, sending raw string")
                else:
                    logger.warning(f"Payload provided but contentType is missing and type {type(payload)} cannot be inferred; body not sent.")

        # Log request details for debugging
        logger.debug(f"Making {method} request to {url}")
        logger.debug(f"Request headers: {headers}")
        if query_params:
            logger.debug(f"Query parameters: {query_params}")
        if cookies:
            logger.debug(f"Cookies: {cookies}")

        # Execute the request
        response = self.http_client.request(
            method=method,
            url=url,
            params=query_params,
            headers=headers,
            cookies=cookies,
            data=data,
            json=json_data,
            files=files,
        )

        # Process the response
        try:
            response_json = response.json()
        except Exception as e:
            logger.debug(f"No JSON in response (or broken JSON): {e}")
            response_json = None

        # Decide final body representation (binary vs text)
        if response_json is not None:
            body_value = response_json
        else:
            ct = response.headers.get("Content-Type", "").lower()
            if any(x in ct for x in ["audio/", "video/", "image/", "application/octet-stream"]):
                body_value = response.content  # keep raw bytes
                logger.debug(f"Preserving binary response ({len(response.content)} bytes) for content-type {ct}")
            else:
                body_value = response.text

        return {
            "status_code": response.status_code,
            "headers": dict(response.headers),
            "body": body_value,
        }

    def _apply_auth_to_request(
        self,
        url: str,
        headers: dict[str, str],
        query_params: dict[str, str],
        cookies: dict[str, str],
        security_options: list[SecurityOption] | None = None,
        source_name: str | None = None,
    ) -> None:
        """
        Apply authentication values from auth_provider to the request

        Args:
            url: The request URL
            headers: Headers dictionary to modify
            query_params: Query parameters dictionary to modify
            cookies: Cookies dictionary to modify
            security_options: List of security options to use for authentication
        """
        if not self.auth_provider:
            logger.debug("No auth_provider available, skipping auth application")
            return

        try:
            # If security options are provided, use them to resolve credentials
            if security_options:
                logger.debug(f"Resolving credentials for security options: {security_options}")

                # Get auth values for the security requirements
                fetch_options = FetchOptions(
                    source_name=source_name
                )
                credentials: list[Credential] = self.auth_provider.get_credentials(security_options, fetch_options)
                if not credentials:
                    logger.debug("No credentials resolved for the security requirements")
                    return

                # Apply each auth value to the request
                for credential in credentials:
                    auth_value: RequestAuthValue = credential.request_auth_value
                    if auth_value.location == AuthLocation.QUERY:
                        query_params[auth_value.name] = auth_value.auth_value
                        logger.debug(f"Applied '{auth_value.name}' as query parameter")
                    elif auth_value.location == AuthLocation.HEADER:
                        headers[auth_value.name] = auth_value.auth_value
                        logger.debug(f"Applied '{auth_value.name}' as header")
                    elif auth_value.location == AuthLocation.COOKIE:
                        cookies[auth_value.name] = auth_value.auth_value
                        logger.debug(f"Applied '{auth_value.name}' as cookie")
                    else:
                        # Default to header for unknown locations
                        headers[auth_value.name] = auth_value.auth_value
                        logger.debug(f"Applied '{auth_value.name}' as header (default)")

            # Also check for direct auth values in auth_provider
            if hasattr(self.auth_provider, "get_auth_value"):
                for header_name in ["Authorization", "Api-Key", "X-Api-Key", "Token"]:
                    if header_name not in headers:
                        auth_value = self.auth_provider.get_auth_value(header_name)
                        if auth_value:
                            headers[header_name] = auth_value
                            logger.debug(f"Applied {header_name} from auth_provider")
        except Exception as e:
            logger.error(f"Error applying auth to request: {e}")
            # Don't re-raise, just log and continue
</http.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/executor/output_extractor.py:
<output_extractor.py>
# src/oak_runner/executor/output_extractor.py
"""
Output Extractor for OAK Runner

This module provides functionality to extract outputs from API responses.
"""

import logging
from typing import Any

from ..evaluator import ExpressionEvaluator
from ..models import ExecutionState
from ..utils import evaluate_json_pointer, extract_json_pointer_from_expression

# Configure logging
logger = logging.getLogger("arazzo-runner.executor")


class OutputExtractor:
    """Extracts outputs from API responses"""

    def __init__(self, source_descriptions: dict[str, Any]):
        """
        Initialize the output extractor

        Args:
            source_descriptions: OpenAPI source descriptions
        """
        self.source_descriptions = source_descriptions

    def extract_outputs(self, step: dict, response: dict, state: ExecutionState) -> dict[str, Any]:
        """
        Extract outputs from the response based on step definitions

        Args:
            step: Step definition
            response: API response
            state: Current execution state

        Returns:
            Dictionary of extracted outputs
        """
        outputs = {}

        # Detailed logging of response structure for troubleshooting
        status_code = response.get('status_code')
        logger.info(f"Response status code: {status_code}")
        logger.debug(f"Response headers: {response.get('headers')}")
        logger.debug(f"Response body: {response.get('body')}")

        # Add error context information for non-2xx status codes
        if status_code is not None and (status_code < 200 or status_code >= 300):
            outputs['oak_error_context'] = {
                'http_code': status_code,
                'http_response': response.get('body')
            }
            logger.warning(f"Non-2xx status code: {status_code}. Adding error context to outputs.")

        # Cache direct ID values from response for potential future use
        # We extract these first but don't add them to outputs yet - we'll only add
        # them if they're actually needed based on workflow definitions
        cached_ids = {}
        if isinstance(response.get("body"), dict):
            body = response.get("body", {})
            # Extract IDs from response body
            for key, value in body.items():
                if key.endswith("Id") and isinstance(value, str):
                    cached_ids[key] = value

            # Also extract IDs from resource URLs in response
            for key, value in body.items():
                if (
                    (key == "self" or key.endswith("Url") or key.endswith("Uri"))
                    and isinstance(value, str)
                    and "/" in value
                ):
                    # Get base type name from URL path
                    path_parts = value.rstrip("/").split("/")
                    if len(path_parts) >= 2:
                        # Try to determine the resource type from the path
                        resource_type = path_parts[
                            -2
                        ]  # e.g., "customers" from "/customers/CUST123"
                        if resource_type.endswith("s"):  # Handle plural form
                            resource_type = resource_type[:-1]  # Remove trailing 's'

                        id_key = f"{resource_type}Id"
                        cached_ids[id_key] = path_parts[-1]

        # Special handling for JSON pointer expressions in outputs
        for output_name, output_expr in step.get("outputs", {}).items():
            value = None

            # Check if this is a JSON pointer expression
            if isinstance(output_expr, str) and "#/" in output_expr:
                container_path, pointer_path = extract_json_pointer_from_expression(output_expr)

                if container_path and pointer_path:
                    # Handle response.body#/path case
                    if container_path == "response.body":
                        body = response.get("body", {})
                        value = evaluate_json_pointer(body, pointer_path)
                        logger.debug(f"JSON Pointer extracted output {output_name}: {value}")
                        outputs[output_name] = value
                        continue  # Skip normal evaluation for this output

            # Handle dot notation by converting to JSON pointer
            if (
                isinstance(output_expr, str)
                and output_expr.startswith("$response.body.")
                and "#" not in output_expr
            ):
                # Convert dot notation to JSON pointer
                path = output_expr.replace("$response.body.", "")
                pointer_path = "/" + path.replace(".", "/")

                body = response.get("body", {})
                value = evaluate_json_pointer(body, pointer_path)
                if value is not None:
                    logger.debug(
                        f"Dot notation converted to JSON pointer for {output_name}: {value}"
                    )
                    outputs[output_name] = value
                    continue

            # If the expression is requesting an ID we've cached, use that
            if output_name in cached_ids:
                outputs[output_name] = cached_ids[output_name]
                logger.debug(f"Using cached ID for {output_name}: {cached_ids[output_name]}")
                continue

            # Normal expression evaluation
            value = ExpressionEvaluator.evaluate_expression(
                output_expr,
                state,
                self.source_descriptions,
                {
                    "statusCode": response["status_code"],
                    "response": response,
                    "headers": response["headers"],
                    "body": response["body"],
                },
            )

            if value is not None:
                outputs[output_name] = value
                logger.debug(f"Extracted output {output_name}: {value}")

        # Log if no outputs were extracted
        if not outputs:
            logger.warning(f"No outputs were successfully extracted for step {step.get('stepId')}")

        return outputs
</output_extractor.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/executor/parameter_processor.py:
<parameter_processor.py>
# src/oak_runner/executor/parameter_processor.py
"""
Parameter Processor for OAK Runner

This module provides functionality to process parameters and request bodies.
"""

import json
import logging
import re
from typing import Any

from ..evaluator import ExpressionEvaluator
from ..models import ExecutionState

# Configure logging
logger = logging.getLogger("arazzo-runner.executor")


class ParameterProcessor:
    """
    Processes parameters and request bodies for API operations
    """

    def __init__(self, source_descriptions: dict[str, Any]):
        """
        Initialize the parameter processor
        Args:
            source_descriptions: OpenAPI source descriptions
        """
        self.source_descriptions = source_descriptions

    @staticmethod
    def _resolve_ref(ref: str, root: dict) -> dict:
        """
        Resolve a JSON reference (e.g., '#/components/parameters/foo') in an OpenAPI document.
        Args:
            ref: The $ref string.
            root: The OpenAPI root document.
        Returns:
            The referenced object.
        """
        if not ref.startswith('#/'):
            raise ValueError(f"Only local refs are supported, got: {ref}")
        parts = ref.lstrip('#/').split('/')
        obj = root
        for part in parts:
            obj = obj[part]
        # Recursively resolve nested $ref
        if isinstance(obj, dict) and '$ref' in obj:
            return ParameterProcessor._resolve_ref(obj['$ref'], root)
        return obj

    def prepare_parameters(self, step: dict, state: ExecutionState) -> dict[str, Any]:
        """
        Prepare parameters for an operation execution

        Args:
            step: Step definition
            state: Current execution state

        Returns:
            Dictionary of prepared parameters
        """
        parameters = {}

        # Process parameters from the step definition
        for param in step.get("parameters", []):
            name = param.get("name")
            location = param.get("in")
            value = param.get("value")

            # Process the value to resolve any expressions
            if isinstance(value, str):
                if value.startswith("$"):
                    # Try array access handler first for common patterns
                    array_value = ExpressionEvaluator.handle_array_access(value, state)
                    if array_value is not None:
                        value = array_value
                    else:
                        # Fall back to standard expression evaluation
                        value = ExpressionEvaluator.evaluate_expression(
                            value, state, self.source_descriptions
                        )
                elif "{" in value and "}" in value:
                    # Template with expressions
                    def replace_expr(match):
                        expr = match.group(1)
                        eval_value = ExpressionEvaluator.evaluate_expression(
                            expr, state, self.source_descriptions
                        )
                        return "" if eval_value is None else str(eval_value)

                    value = re.sub(r"\{([^}]+)\}", replace_expr, value)
                # Special handling for "Bearer $dependencies.x.y" format - common in authorization headers
                elif " $" in value:
                    parts = value.split(" $", 1)
                    prefix = parts[0] + " "
                    expr = "$" + parts[1]

                    # Add more debugging for dependency expressions
                    if "dependencies" in expr:
                        logger.debug(f"Processing dependency expression: {expr}")
                        logger.debug(f"Dependencies available: {state.dependency_outputs}")
                        if "." in expr:
                            parts = expr.split(".")
                            if len(parts) >= 3:
                                dep_id = parts[1]
                                output_key = parts[2]
                                logger.debug(
                                    f"Looking for dependency {dep_id}, output {output_key}"
                                )

                                if dep_id in state.dependency_outputs:
                                    logger.debug(
                                        f"Found dependency {dep_id} with outputs: {state.dependency_outputs[dep_id]}"
                                    )
                                    if output_key in state.dependency_outputs[dep_id]:
                                        logger.debug(
                                            f"Found output {output_key} with value: {state.dependency_outputs[dep_id][output_key]}"
                                        )
                                    else:
                                        logger.debug(
                                            f"Output {output_key} not found in dependency {dep_id}"
                                        )
                                else:
                                    logger.debug(
                                        f"Dependency {dep_id} not found in available dependencies"
                                    )

                    # Evaluate the expression part
                    expr_value = ExpressionEvaluator.evaluate_expression(
                        expr, state, self.source_descriptions
                    )

                    # More debugging about evaluation result
                    if "dependencies" in expr:
                        logger.debug(f"Evaluated dependency expression {expr} to: {expr_value}")

                    if expr_value is not None:
                        value = prefix + str(expr_value)
                    else:
                        logger.warning(
                            f"Expression {expr} evaluated to None - keeping original value: {value}"
                        )
            elif isinstance(value, dict):
                value = ExpressionEvaluator.process_object_expressions(
                    value, state, self.source_descriptions
                )
            elif isinstance(value, list):
                value = ExpressionEvaluator.process_array_expressions(
                    value, state, self.source_descriptions
                )

            # Log the parameter evaluation process for debugging
            logger.debug(
                f"Parameter: {name}, Original value: {param.get('value')}, Evaluated value: {value}"
            )

            # Log if the value couldn't be properly evaluated
            if isinstance(value, str) and "$" in value and (value.startswith("$") or "{$" in value):
                logger.warning(
                    f"Parameter '{name}' value '{value}' still contains expression syntax after evaluation"
                )

            # Organize parameters by location
            if location == "path":
                parameters.setdefault("path", {})[name] = value
            elif location == "query":
                parameters.setdefault("query", {})[name] = value
            elif location == "header":
                parameters.setdefault("header", {})[name] = value
            elif location == "cookie":
                parameters.setdefault("cookie", {})[name] = value
            else:
                # For workflow inputs
                parameters[name] = value

        return parameters

    def _process_multipart_payload(self, payload: dict[str, Any]) -> dict[str, Any]:
        """
        Wraps binary data in a payload dictionary for multipart/form-data upload.

        Args:
            payload: The dictionary payload to process.

        Returns:
            A new dictionary with byte values wrapped in the file structure.
        """
        processed_payload: dict[str, Any] = {}
        for key, value in payload.items():
            if isinstance(value, (bytes, bytearray)):
                logger.debug(f"Wrapping binary data in field '{key}' for multipart upload.")
                processed_payload[key] = {
                    "content": value,
                    "filename": "attachment",  # Using a generic filename
                    "contentType": "application/octet-stream",
                }
            else:
                processed_payload[key] = value
        return processed_payload

    def prepare_request_body(self, request_body: dict, state: ExecutionState) -> dict:
        """
        Prepare request body for an operation execution

        Args:
            request_body: Request body definition
            state: Current execution state

        Returns:
            Dictionary with prepared request body
        """
        content_type = request_body.get("contentType")
        payload = request_body.get("payload")

        # First, evaluate the entire payload object to resolve any expressions
        if isinstance(payload, dict):
            payload = ExpressionEvaluator.process_object_expressions(
                payload, state, self.source_descriptions
            )

        # Handle multipart/form-data specifically for file uploads
        if content_type and "multipart/form-data" in content_type.lower():
            processed_payload = self._process_multipart_payload(payload)
            # Return a new request body object with the processed payload
            return {"contentType": content_type, "payload": processed_payload}

        # Handle other payload types
        elif isinstance(payload, str):
            # String payload with possible template expressions
            try:
                # First handle any template expressions in the string regardless of format
                if "{" in payload and "}" in payload:
                    # First convert any expressions like "{$inputs.value}" to their evaluated values
                    def replace_expr(match):
                        expr = match.group(1)
                        if expr.startswith("$"):
                            # It's an expression, evaluate it
                            value = ExpressionEvaluator.evaluate_expression(
                                expr, state, self.source_descriptions
                            )
                            logger.debug(f"Evaluated template expression {expr} to value: {value}")

                            # Return JSON-compatible value or string representation
                            if value is None:
                                return "null"
                            elif isinstance(value, (dict, list)):
                                # Keep actual data structure for direct inclusion in JSON
                                try:
                                    # First try to stringify to ensure it's JSON-safe
                                    json_str = json.dumps(value)
                                    # Return the original value (not the string) for use in the template
                                    return value
                                except Exception as e:
                                    logger.error(f"Cannot serialize value to JSON: {e}")
                                    # Fallback to string representation
                                    return str(value)
                            else:
                                # For primitives, return the raw value (not JSON string)
                                return value
                        else:
                            # Not an expression, return as is
                            return "{" + expr + "}"

                    # Handle expressions in the template
                    try:
                        # Special handling for payload strings that look like JSON
                        if payload.strip().startswith("{") and payload.strip().endswith("}"):
                            # Try direct JSON parsing first
                            try:
                                # Fix common JSON errors like missing commas
                                fixed_payload = re.sub(r'"\s*\n\s*"', '",\n"', payload)
                                json_data = json.loads(fixed_payload)

                                # Directly process the object expressions
                                processed_json = ExpressionEvaluator.process_object_expressions(
                                    json_data, state, self.source_descriptions
                                )

                                if content_type == "application/json":
                                    payload = processed_json  # Keep as dict for JSON
                                else:
                                    payload = json.dumps(processed_json)  # Convert back to string
                                logger.debug(
                                    "Successfully processed JSON payload with nested expressions"
                                )

                            except json.JSONDecodeError:
                                # If direct parsing fails, try traditional template substitution
                                logger.debug(
                                    "Direct JSON parsing failed, trying template replacement"
                                )

                                # Replace expressions in template string
                                templated_payload = re.sub(r"\{(\$[^}]+)\}", replace_expr, payload)
                                logger.debug(f"Template-processed payload: {templated_payload}")

                                try:
                                    # Fix common JSON issues and try parsing
                                    fixed_payload = re.sub(
                                        r'"\s*\n\s*"', '",\n"', templated_payload
                                    )
                                    json_payload = json.loads(fixed_payload)

                                    # Keep as dict if needed for JSON
                                    if content_type == "application/json":
                                        payload = json_payload
                                    else:
                                        payload = json.dumps(json_payload)
                                except json.JSONDecodeError as e:
                                    logger.warning(
                                        f"JSON decode error after template processing: {e}"
                                    )
                                    # Use the templated string as-is
                                    payload = templated_payload
                        else:
                            # Not JSON-like, process as regular template string
                            templated_payload = re.sub(r"\{(\$[^}]+)\}", replace_expr, payload)
                            payload = templated_payload
                            logger.debug(f"Processed non-JSON template: {payload}")
                    except Exception as template_error:
                        logger.error(f"Template processing error: {template_error}")
                        # Fall back to the original payload
                        logger.debug("Using original payload due to processing error")

                # If no template expressions, but looks like JSON, try to parse it
                elif payload.strip().startswith("{") and payload.strip().endswith("}"):
                    try:
                        # Fix common JSON issues
                        fixed_payload = re.sub(r'"\s*\n\s*"', '",\n"', payload)

                        # Parse the JSON
                        json_payload = json.loads(fixed_payload)

                        # Process any expressions in the parsed JSON
                        processed_payload = ExpressionEvaluator.process_object_expressions(
                            json_payload, state, self.source_descriptions
                        )

                        # Keep as object for JSON content types
                        if content_type == "application/json":
                            payload = processed_payload
                        else:
                            payload = json.dumps(processed_payload)
                    except json.JSONDecodeError as e:
                        logger.warning(f"JSON decode error in non-templated payload: {e}")
                        # Keep original as string
            except Exception as e:
                logger.error(f"Error processing payload: {e}")
                logger.error(f"Original payload: {payload}")
                # If all processing fails, use the original payload
        elif isinstance(payload, dict):
            # Process nested dictionary values, evaluating expressions
            payload = ExpressionEvaluator.process_object_expressions(
                payload, state, self.source_descriptions
            )
            logger.debug(f"Processed dict payload: {payload}")
        elif isinstance(payload, list):
            # Process nested list values, evaluating expressions
            payload = ExpressionEvaluator.process_array_expressions(
                payload, state, self.source_descriptions
            )
            logger.debug(f"Processed list payload: {payload}")
        elif isinstance(payload, str) and payload.startswith("$"):
            # Direct expression payload
            payload = ExpressionEvaluator.evaluate_expression(
                payload, state, self.source_descriptions
            )
            logger.debug(f"Processed expression payload: {payload}")

        # Handle replacements
        replacements = request_body.get("replacements", [])
        if replacements and isinstance(payload, (dict, list)):
            for replacement in replacements:
                target = replacement.get("target")
                value = replacement.get("value")

                if isinstance(value, str) and value.startswith("$"):
                    # Evaluate expression
                    value = ExpressionEvaluator.evaluate_expression(
                        value, state, self.source_descriptions
                    )

                # Apply the replacement to the payload
                try:
                    if target.startswith("/"):
                        # JSON Pointer
                        parts = target.split("/")[1:]  # Skip the first empty element

                        # Navigate to the target location
                        current = payload
                        for i, part in enumerate(parts):
                            if i == len(parts) - 1:
                                # Last part, set the value
                                current[part] = value
                            else:
                                # Navigate deeper
                                current = current[part]
                except (KeyError, IndexError, TypeError) as e:
                    logger.error(f"Error applying replacement to target {target}: {e}")

        return {"contentType": content_type, "payload": payload}

    def prepare_operation_parameters(
        self, operation_details: dict, inputs: dict[str, Any]
    ) -> dict[str, Any]:
        logger.debug("Preparing operation parameters...")
        """
        Prepare parameters and request body for a direct operation execution.

        This method maps raw input values to operation parameters and request body
        defined in the OpenAPI specification, without using ExecutionState or
        evaluating complex Arazzo expressions.

        Args:
            operation_details: The OpenAPI operation definition dictionary (from OperationFinder).
            inputs: The raw input dictionary provided by the user.

        Returns:
            Dictionary containing prepared parameters and body, structured as:
            {
                'path': { 'param_name': value, ... },
                'query': { 'param_name': value, ... },
                'header': { 'param_name': value, ... },
                'cookie': { 'param_name': value, ... },
                'body': request_body_value_or_object
            }

        Raises:
            ValueError: If a required parameter is missing from inputs or if the request body is required but not provided.
        """
        prepared_params: dict[str, Any] = {
            "path": {},
            "query": {},
            "header": {},
            "cookie": {},
            "body": None,
        }
        used_input_keys = set()

        # 1. Merge parameters from path-item and operation level, operation-level wins
        op_params = operation_details.get("operation", {}).get("parameters", [])
        path_params = operation_details.get("parameters", [])
        param_map = {}
        all_params = path_params + op_params
        for param in all_params:
            # --- $ref resolution ---
            if isinstance(param, dict) and "$ref" in param:
                source_name = operation_details.get("source")
                if not source_name or source_name not in self.source_descriptions:
                    raise ValueError(f"Cannot resolve $ref: source '{source_name}' not found in source_descriptions.")
                param = self._resolve_ref(param["$ref"], self.source_descriptions[source_name])
            key = (param.get("name"), param.get("in"))
            param_map[key] = param
        merged_params = list(param_map.values())

        # 2. Parse path template for {param} tokens and ensure all are present as required path params
        path_template = operation_details.get("path", "")
        path_param_names = set(re.findall(r"{([^}]+)}", path_template))
        for param_name in path_param_names:
            key = (param_name, "path")
            if key not in param_map:
                param_map[key] = {
                    "name": param_name,
                    "in": "path",
                    "required": True,
                }
        merged_params = list(param_map.values())

        # 3. Process all parameters (path, query, header, cookie)
        for param_def in merged_params:
            name = param_def.get("name")
            location = param_def.get("in")
            required = param_def.get("required", False)

            if not name or not location:
                logger.warning(f"Skipping parameter definition missing name or location: {param_def}")
                continue

            if name in inputs:
                value = inputs[name]
                if location in prepared_params:
                    prepared_params[location][name] = value
                    used_input_keys.add(name)
                    logger.debug(f"Mapped input '{name}' to {location} parameter.")
                else:
                    logger.warning(f"Unsupported parameter location '{location}' for parameter '{name}'.")
            elif required:
                logger.error(f"Required parameter '{name}' (in: {location}) missing from inputs.")
                raise ValueError(f"Required parameter '{name}' (in: {location}) is missing.")
            else:
                logger.debug(f"Optional parameter '{name}' (in: {location}) not provided in inputs.")

        # 4. Process request body
        operation = operation_details.get("operation")  # Get operation dict or None
        request_body_ref = None
        if operation and isinstance(operation, dict):
            request_body_ref = operation.get("requestBody")

        # If not found in operation (or operation doesn't exist), check the top level
        if not request_body_ref:
            request_body_ref = operation_details.get("requestBody")

        request_body_def = None
        if request_body_ref:
            logger.debug(f"Found request body reference: {request_body_ref}")
            # Resolve $ref if necessary (simplified back to original for now, assuming source handled elsewhere if needed)
            if isinstance(request_body_ref, dict) and "$ref" in request_body_ref:
                ref_path = request_body_ref["$ref"]
                try:
                    # Assuming _resolve_ref can find the source description if needed,
                    # or that refs are local within the current file.
                    request_body_def = self._resolve_ref(ref_path)
                    logger.debug(f"Resolved requestBody $ref '{ref_path}' to: {request_body_def}")
                except ValueError as e:
                    logger.error(f"Failed to resolve requestBody $ref '{ref_path}': {e}")
                    request_body_def = None
                except Exception as e:
                    logger.error(f"Unexpected error resolving requestBody $ref '{ref_path}': {e}")
                    request_body_def = None
            elif isinstance(request_body_ref, dict):
                request_body_def = request_body_ref
                logger.debug(f"Using inline requestBody definition: {request_body_def}")
            else:
                logger.warning(f"Unexpected format for requestBody reference: {request_body_ref}")

        # Determine potential body keys (inputs not used for path/query/header/cookie)
        potential_body_keys = set(inputs.keys()) - used_input_keys
        payload_dict = None
        determined_content_type = None
        body_required = False

        # Only process body if the spec defines one
        if request_body_def and isinstance(request_body_def, dict):
            body_required = request_body_def.get("required", False)

            if potential_body_keys:
                payload_dict = {k: inputs[k] for k in potential_body_keys}
                logger.debug(f"Identified potential request body payload from unused inputs: {list(potential_body_keys)}")
                used_input_keys.update(potential_body_keys) # Mark these inputs as used

                # Determine content type based on the spec's definition
                content_schema = request_body_def.get("content", {})
                if content_schema and isinstance(content_schema, dict):
                     # Prioritize application/json, otherwise take the first key
                    if "application/json" in content_schema:
                        determined_content_type = "application/json"
                    elif content_schema:
                        determined_content_type = next(iter(content_schema.keys()), None)

                    if determined_content_type:
                        logger.debug(f"Determined request body content type: {determined_content_type}")
                    else:
                         logger.warning("Could not determine content type from requestBody definition, even though payload was identified.")
                else:
                    logger.warning("requestBody definition found, but 'content' map is missing or invalid.")

                # If multipart/form-data, wrap bytes payloads for file uploads
                if (
                    determined_content_type
                    and isinstance(determined_content_type, str)
                    and "multipart/form-data" in determined_content_type.lower()
                    and isinstance(payload_dict, dict)
                ):
                    payload_to_store = self._process_multipart_payload(payload_dict)
                else:
                    payload_to_store = payload_dict

                # Store payload and content type (only if payload was identified)
                prepared_params["body"] = {
                    "payload": payload_to_store,
                    "contentType": determined_content_type,
                }
            # Check requirement if spec defines a body but no payload was found
            elif body_required:
                 logger.error("Required request body is missing from inputs (spec defines body, but no unused inputs found).")
                 raise ValueError("Required request body is missing.")
            else: # Optional body defined in spec, but no payload provided
                 logger.debug("Optional request body defined in spec, but not provided or identified in inputs.")

        # If spec does NOT define a request body, but we HAVE potential body keys -> Log warning
        elif potential_body_keys:
             logger.warning(
                 f"Inputs provided but not used for parameters and no requestBody defined in spec: {list(potential_body_keys)}. These inputs are being ignored."
             )
             # Body remains None in prepared_params

        # Final check for requirement (redundant if logic above is correct, but safe)
        if body_required and prepared_params.get("body") is None:
             # This case should theoretically be caught above, but acts as a safeguard
            logger.error("Consistency check failed: Required body specified, but no body was prepared.")
            raise ValueError("Required request body was specified but could not be prepared from inputs.")

        logger.debug(f"Prepared parameters: {prepared_params}")
        return prepared_params

    # --- DEPRECATION NOTICE ---
    # Consider refactoring prepare_parameters or merging logic if significant overlap
    # exists after full implementation of prepare_operation_parameters.
    def prepare_parameters(self, step: dict, state: ExecutionState) -> dict[str, Any]:
        """
        Prepare parameters for an operation execution

        Args:
            step: Step definition
            state: Current execution state

        Returns:
            Dictionary of prepared parameters
        """
        parameters = {}

        # Process parameters from the step definition
        for param in step.get("parameters", []):
            name = param.get("name")
            location = param.get("in")
            value = param.get("value")

            # Process the value to resolve any expressions
            if isinstance(value, str):
                if value.startswith("$"):
                    # Try array access handler first for common patterns
                    array_value = ExpressionEvaluator.handle_array_access(value, state)
                    if array_value is not None:
                        value = array_value
                    else:
                        # Fall back to standard expression evaluation
                        value = ExpressionEvaluator.evaluate_expression(
                            value, state, self.source_descriptions
                        )
                elif "{" in value and "}" in value:
                    # Template with expressions
                    def replace_expr(match):
                        expr = match.group(1)
                        eval_value = ExpressionEvaluator.evaluate_expression(
                            expr, state, self.source_descriptions
                        )
                        return "" if eval_value is None else str(eval_value)

                    value = re.sub(r"\{([^}]+)\}", replace_expr, value)
                # Special handling for "Bearer $dependencies.x.y" format - common in authorization headers
                elif " $" in value:
                    parts = value.split(" $", 1)
                    prefix = parts[0] + " "
                    expr = "$" + parts[1]

                    # Add more debugging for dependency expressions
                    if "dependencies" in expr:
                        logger.debug(f"Processing dependency expression: {expr}")
                        logger.debug(f"Dependencies available: {state.dependency_outputs}")
                        if "." in expr:
                            parts = expr.split(".")
                            if len(parts) >= 3:
                                dep_id = parts[1]
                                output_key = parts[2]
                                logger.debug(
                                    f"Looking for dependency {dep_id}, output {output_key}"
                                )

                                if dep_id in state.dependency_outputs:
                                    logger.debug(
                                        f"Found dependency {dep_id} with outputs: {state.dependency_outputs[dep_id]}"
                                    )
                                    if output_key in state.dependency_outputs[dep_id]:
                                        logger.debug(
                                            f"Found output {output_key} with value: {state.dependency_outputs[dep_id][output_key]}"
                                        )
                                    else:
                                        logger.debug(
                                            f"Output {output_key} not found in dependency {dep_id}"
                                        )
                                else:
                                    logger.debug(
                                        f"Dependency {dep_id} not found in available dependencies"
                                    )

                    # Evaluate the expression part
                    expr_value = ExpressionEvaluator.evaluate_expression(
                        expr, state, self.source_descriptions
                    )

                    # More debugging about evaluation result
                    if "dependencies" in expr:
                        logger.debug(f"Evaluated dependency expression {expr} to: {expr_value}")

                    if expr_value is not None:
                        value = prefix + str(expr_value)
                    else:
                        logger.warning(
                            f"Expression {expr} evaluated to None - keeping original value: {value}"
                        )
            elif isinstance(value, dict):
                value = ExpressionEvaluator.process_object_expressions(
                    value, state, self.source_descriptions
                )
            elif isinstance(value, list):
                value = ExpressionEvaluator.process_array_expressions(
                    value, state, self.source_descriptions
                )

            # Log the parameter evaluation process for debugging
            logger.debug(
                f"Parameter: {name}, Original value: {param.get('value')}, Evaluated value: {value}"
            )

            # Log if the value couldn't be properly evaluated
            if isinstance(value, str) and "$" in value and (value.startswith("$") or "{$" in value):
                logger.warning(
                    f"Parameter '{name}' value '{value}' still contains expression syntax after evaluation"
                )

            # Organize parameters by location
            if location == "path":
                parameters.setdefault("path", {})[name] = value
            elif location == "query":
                parameters.setdefault("query", {})[name] = value
            elif location == "header":
                parameters.setdefault("header", {})[name] = value
            elif location == "cookie":
                parameters.setdefault("cookie", {})[name] = value
            else:
                # For workflow inputs
                parameters[name] = value

        return parameters
</parameter_processor.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/executor/__init__.py:
<__init__.py>
# src/oak_runner/executor/__init__.py
"""
Executor module for OAK Runner

This module provides functions for executing workflow steps in Arazzo workflows.
"""

from .action_handler import ActionHandler
from .operation_finder import OperationFinder
from .output_extractor import OutputExtractor
from .parameter_processor import ParameterProcessor
from .step_executor import StepExecutor
from .success_criteria import SuccessCriteriaChecker

__all__ = [
    "StepExecutor",
    "OperationFinder",
    "ParameterProcessor",
    "OutputExtractor",
    "SuccessCriteriaChecker",
    "ActionHandler",
]
</__init__.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/executor/success_criteria.py:
<success_criteria.py>
# src/oak_runner/executor/success_criteria.py
"""
Success Criteria Checker for OAK Runner

This module provides functionality to check if API responses meet success criteria.
"""

import logging
import re
from typing import Any

import jsonpath_ng.ext as jsonpath

from ..evaluator import ExpressionEvaluator
from ..models import ExecutionState

# Configure logging
logger = logging.getLogger("arazzo-runner.executor")


class SuccessCriteriaChecker:
    """Checks if API responses meet success criteria"""

    def __init__(self, source_descriptions: dict[str, Any]):
        """
        Initialize the success criteria checker

        Args:
            source_descriptions: OpenAPI source descriptions
        """
        self.source_descriptions = source_descriptions

    def check_success_criteria(self, step: dict, response: dict, state: ExecutionState) -> bool:
        """
        Check if the response meets the success criteria

        Args:
            step: Step definition
            response: Response to check
            state: Current execution state

        Returns:
            True if success criteria are met, False otherwise
        """
        criteria = step.get("successCriteria", [])

        if not criteria:
            # Default success criterion: status code 2xx
            return 200 <= response["status_code"] < 300

        # Log all success criteria for debugging
        logger.debug(f"Checking success criteria: {criteria}")

        # Context for evaluating expressions
        context = {
            "statusCode": response["status_code"],
            "response": response,
            "headers": response["headers"],
            "body": response["body"],
        }

        # Check all criteria
        for criterion in criteria:
            condition = criterion.get("condition")
            criterion_type = criterion.get("type", "simple")
            criterion_context = None

            logger.debug(f"Evaluating criterion: {condition} (type: {criterion_type})")

            # Special handling for conditions that use JSON pointer syntax
            if (
                criterion_type == "simple"
                and isinstance(condition, str)
                and condition.startswith("$response.body#/")
            ):
                try:
                    # Handle patterns like $response.body#/path/to/value == value
                    import jsonpointer

                    # Parse left and right sides of the comparison
                    match = re.match(r"^\$response\.body#(/.*?)\s*([=!<>]+)\s*(.*)$", condition)
                    if match:
                        pointer_path, operator, right_value = match.groups()
                        body = response.get("body", {})

                        # Get the value from the response using the JSON pointer
                        pointer = jsonpointer.JsonPointer(pointer_path)
                        left_value = pointer.resolve(body)

                        # Eval the right side if it looks like a variable
                        if right_value.startswith("$"):
                            right_value = ExpressionEvaluator.evaluate_expression(
                                right_value, state, self.source_descriptions, context
                            )
                        else:
                            # Try to evaluate literals (booleans, numbers, etc.)
                            try:
                                right_value = eval(right_value)
                            except:
                                # If it's not a valid Python expression, treat as string
                                right_value = right_value.strip("\"'")

                        # Perform the comparison
                        logger.debug(
                            f"JSON Pointer comparison: {left_value} {operator} {right_value}"
                        )
                        if operator == "==":
                            result = left_value == right_value
                        elif operator == "!=":
                            result = left_value != right_value
                        elif operator == ">":
                            result = left_value > right_value
                        elif operator == "<":
                            result = left_value < right_value
                        elif operator == ">=":
                            result = left_value >= right_value
                        elif operator == "<=":
                            result = left_value <= right_value
                        else:
                            logger.warning(
                                f"Unsupported operator in JSON pointer condition: {operator}"
                            )
                            result = False

                        if not result:
                            logger.debug(f"Criterion failed: {condition}")
                            return False

                        # Skip the normal evaluation since we handled it manually
                        continue
                except Exception as e:
                    logger.error(f"Error evaluating JSON pointer condition {condition}: {e}")

            if "context" in criterion:
                context_expr = criterion.get("context")
                criterion_context = ExpressionEvaluator.evaluate_expression(
                    context_expr, state, self.source_descriptions, context
                )
            else:
                criterion_context = context

            if criterion_type == "simple":
                # Evaluate the condition as a simple expression
                result = ExpressionEvaluator.evaluate_simple_condition(
                    condition, state, self.source_descriptions, context
                )
                if not result:
                    logger.debug(f"Simple criterion failed: {condition}")
                    return False
            elif criterion_type == "regex":
                # Evaluate the condition as a regex pattern
                if not criterion_context or not condition:
                    return False

                # Convert context to string if needed
                ctx_str = str(criterion_context)

                # Check if the regex pattern matches
                match = re.search(condition, ctx_str)
                if not match:
                    return False
            elif criterion_type == "jsonpath":
                # Evaluate the condition as a JSONPath expression
                if not criterion_context or not condition:
                    logger.warning("JSONPath condition failed: empty context or condition")
                    return False

                # Handle jsonpath condition
                result = self._evaluate_jsonpath_condition(condition, criterion_context)
                if not result:
                    return False
            elif criterion_type == "xpath":
                # Evaluate the condition as an XPath expression
                # This would require an XML parser like lxml
                logger.warning("XPath evaluation not implemented")
                return False

        # All criteria passed
        return True

    def _evaluate_jsonpath_condition(self, condition: str, context: Any) -> bool:
        """
        Evaluate a JSONPath condition

        Args:
            condition: JSONPath condition to evaluate
            context: Context to evaluate against

        Returns:
            True if condition is met, False otherwise
        """
        try:
            logger.info(f"Evaluating JSONPath condition: {condition}")
            logger.info(f"Context type: {type(context)}, Content: {context}")

            # Special handling for count expressions that JSONPath library might have trouble with
            if condition.startswith("$[?count(@.") and ")" in condition:
                # Parse count expression like $[?count(@.products) > 0]
                match = re.match(
                    r"\$\[\?count\(\@\.([a-zA-Z0-9_]+)\) *([<>=!]+) *(\d+)\]", condition
                )
                if match:
                    property_name, operator, value_str = match.groups()
                    value = int(value_str)

                    # Get property value
                    array_value = None
                    if isinstance(context, dict) and property_name in context:
                        array_value = context[property_name]

                    # Count items if it's a list
                    if isinstance(array_value, list):
                        count = len(array_value)
                    elif array_value is not None:
                        # If not a list but not None, treat as 1 item
                        count = 1
                    else:
                        # If property doesn't exist or is None, count is 0
                        count = 0

                    logger.info(
                        f"Count expression: property={property_name}, count={count}, op={operator}, value={value}"
                    )

                    # Evaluate the comparison
                    if operator == "==":
                        result = count == value
                    elif operator == "!=":
                        result = count != value
                    elif operator == ">":
                        result = count > value
                    elif operator == "<":
                        result = count < value
                    elif operator == ">=":
                        result = count >= value
                    elif operator == "<=":
                        result = count <= value
                    else:
                        logger.warning(f"Unsupported operator in count expression: {operator}")
                        return False

                    if not result:
                        logger.warning(f"Count comparison failed: {count} {operator} {value}")
                        return False

                    logger.info(f"Count comparison succeeded: {count} {operator} {value}")
                    return True

            # If not a special case, use standard JSONPath library
            jsonpath_expr = jsonpath.parse(condition)
            matches = [match.value for match in jsonpath_expr.find(context)]

            logger.info(f"JSONPath matches: {matches}")

            if not matches:
                logger.warning(f"JSONPath condition returned no matches: {condition}")
                return False

            logger.info(f"JSONPath condition succeeded: {condition}")
            return True
        except Exception as e:
            logger.error(f"Error evaluating JSONPath expression {condition}: {e}")
            logger.error(f"Context was: {context}")
            # For debugging, try simple approach if standard fails
            try:
                if condition == "$[?count(@.products) > 0]" and isinstance(context, dict):
                    products = context.get("products", [])
                    count = len(products) if isinstance(products, list) else 0
                    logger.info(f"Fallback count check: {count} > 0 = {count > 0}")
                    return count > 0
            except Exception as fallback_err:
                logger.error(f"Fallback approach also failed: {fallback_err}")
            return False
</success_criteria.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/executor/step_executor.py:
<step_executor.py>
# src/oak_runner/executor/step_executor.py
"""
Step Executor for OAK Runner

This module provides the main StepExecutor class that orchestrates the execution of workflow steps.
"""

import logging
import re
from typing import Any

from oak_runner.models import ExecutionState, RuntimeParams

from ..evaluator import ExpressionEvaluator
from ..http import HTTPExecutor
from .action_handler import ActionHandler
from .operation_finder import OperationFinder
from .output_extractor import OutputExtractor
from .parameter_processor import ParameterProcessor
from .server_processor import ServerProcessor
from .success_criteria import SuccessCriteriaChecker

# Configure logging
logger = logging.getLogger("arazzo-runner.executor")


class StepExecutor:
    """Executes workflow steps in Arazzo workflows"""

    def __init__(
        self,
        http_client: HTTPExecutor,
        source_descriptions: dict[str, Any],
        testing_mode: bool = False,
    ):
        """
        Initialize the step executor

        Args:
            http_client: HTTP client for executing API requests
            source_descriptions: OpenAPI source descriptions
            testing_mode: If True, enable test-specific behaviors like fallback outputs
        """
        self.http_client = http_client
        self.source_descriptions = source_descriptions
        self.testing_mode = testing_mode

        # Initialize components
        self.operation_finder = OperationFinder(source_descriptions)
        self.parameter_processor = ParameterProcessor(source_descriptions)
        self.output_extractor = OutputExtractor(source_descriptions)
        self.success_checker = SuccessCriteriaChecker(source_descriptions)
        self.action_handler = ActionHandler(source_descriptions)
        self.server_processor = ServerProcessor(source_descriptions)


    def execute_step(self, step: dict, state: ExecutionState) -> dict:
        """
        Execute a single workflow step

        Args:
            step: Step definition from the Arazzo document
            state: Current execution state

        Returns:
            result: Step execution result
        """
        step_id = step.get("stepId")

        # Determine what to execute: operation or workflow
        if "operationId" in step:
            return self._execute_operation_by_id(step, state)
        elif "operationPath" in step:
            return self._execute_operation_by_path(step, state)
        elif "workflowId" in step:
            # Nested workflows do not directly use HTTPExecutor with server configs at this level
            return self._execute_nested_workflow(step, state)
        else:
            raise ValueError(f"Step {step_id} does not specify an operation or workflow to execute")

    def _execute_operation_by_id(self, step: dict, state: ExecutionState) -> dict:
        """Execute an operation by its operationId"""
        operation_id = step.get("operationId")
        if not operation_id:
            raise ValueError("Missing operationId in step definition")

        # Find the operation in the source descriptions
        operation_info = self.operation_finder.find_by_id(operation_id)
        if not operation_info:
            raise ValueError(f"Operation {operation_id} not found in source descriptions")

        # Prepare parameters
        parameters = self.parameter_processor.prepare_parameters(step, state)

        # Prepare request body if present
        request_body = None
        if "requestBody" in step:
            request_body = self.parameter_processor.prepare_request_body(
                step.get("requestBody"), state
            )

        # Extract security requirements
        security_options = self.operation_finder.extract_security_requirements(operation_info)
        source_name = operation_info.get("source", "default")

        # Resolve final URL
        base_server_url = operation_info.get("url") # This is the relative path template
        final_url_template = self.server_processor.resolve_server_params(
            source_name=source_name,
            operation_url_template=base_server_url, # Pass it as operation_url_template
            server_runtime_params=state.runtime_params.servers if state.runtime_params else None
        )

        if not final_url_template:
            error_msg = f"Could not determine final URL for operationId '{operation_id}'"
            logger.error(error_msg)
            return {"success": False, "response": {"error": error_msg, "status_code": 0}, "outputs": {}}

        # Execute the HTTP request
        response = self.http_client.execute_request(
            method=operation_info.get("method"),
            url=final_url_template,
            parameters=parameters,
            request_body=request_body,
            security_options=security_options,
            source_name=source_name,
        )

        # Check success criteria
        success = self.success_checker.check_success_criteria(step, response, state)

        # Extract outputs
        outputs = self.output_extractor.extract_outputs(step, response, state)

        return {"success": success, "response": response, "outputs": outputs}

    def _execute_operation_by_path(self, step: dict, state: ExecutionState) -> dict:
        """Execute an operation by its operationPath"""
        operation_path = step.get("operationPath") # This is the method:path string e.g. GET:/pets
        step_id = step.get("stepId", "unknown")

        logger.debug(f"Processing operationPath value: {operation_path} for step {step_id}")

        # Evaluate the operation path if it contains expressions
        if operation_path.startswith("{") and operation_path.endswith("}"):
            operation_path = ExpressionEvaluator.evaluate_expression(
                operation_path[1:-1], state, self.source_descriptions
            )
            logger.debug(f"Evaluated operationPath expression to: {operation_path}")

        # Parse the operation path to find the source and JSON pointer
        match = re.match(r"([^#]+)#(.+)", operation_path)
        if not match:
            error_msg = f"Invalid operation path: {operation_path}"
            logger.error(error_msg)
            raise ValueError(error_msg)

        source_url, json_pointer = match.groups() # Use source_name_from_path for clarity
        logger.debug(f"Parsed operationPath - source: {source_url}, pointer: {json_pointer}")

        # Print the raw JSON pointer for debugging
        logger.debug(f"Raw JSON pointer in operationPath: {json_pointer}")

        # Try to decode it manually to see what's happening
        decoded = json_pointer.replace("~1", "/").replace("~0", "~")
        logger.debug(f"Manually decoded pointer: {decoded}")

        # Find the operation in the source descriptions
        operation_info = self.operation_finder.find_by_path(source_url, json_pointer)

        source_name = operation_info.get("source")

        if not operation_info:
            # Enhanced logging moved from original code to here for when operation_info is None
            logger.error(f"Failed to find operation for path: {operation_path}")
            for name, desc in self.source_descriptions.items():
                paths = desc.get("paths", {})
                logger.debug(f"Source '{name}' has {len(paths)} paths: {list(paths.keys())}")
                for path_key, methods in paths.items():
                    for method_key, op_details in methods.items():
                        if method_key.lower() in ["get", "post", "put", "delete", "patch"]:
                            op_id_log = op_details.get("operationId", "[No operationId]")
                            logger.debug(f"  - {method_key.upper()} {path_key} (operationId: {op_id_log})")
            raise ValueError(f"Operation not found at path {operation_path}")

        logger.debug(
            f"Found operation: {operation_info.get('method')} {operation_info.get('url')}"
        )

        # Prepare parameters
        parameters = self.parameter_processor.prepare_parameters(step, state)

        # Prepare request body if present
        request_body = None
        if "requestBody" in step:
            request_body = self.parameter_processor.prepare_request_body(
                step.get("requestBody"), state
            )

        # Extract security requirements
        security_options = self.operation_finder.extract_security_requirements(operation_info)

        # Resolve final URL
        relative_operation_path_template = operation_info.get("url")
        final_url_template = self.server_processor.resolve_server_params(
            source_name=source_name,
            operation_url_template=relative_operation_path_template, # Pass it as operation_url_template
            server_runtime_params=state.runtime_params.servers if state.runtime_params else None
        )

        if not final_url_template:
            error_msg = f"Could not determine final URL for operationPath '{operation_path}'"
            logger.error(error_msg)
            return {"success": False, "response": {"error": error_msg, "status_code": 0}, "outputs": {}}

        # Execute the HTTP request
        response = self.http_client.execute_request(
            method=operation_info.get("method"),
            url=final_url_template,
            parameters=parameters,
            request_body=request_body,
            security_options=security_options,
            source_name=source_name,
        )

        # Check success criteria
        success = self.success_checker.check_success_criteria(step, response, state)

        # Extract outputs
        outputs = self.output_extractor.extract_outputs(step, response, state)

        return {"success": success, "response": response, "outputs": outputs}

    def _execute_nested_workflow(self, step: dict, state: ExecutionState) -> dict:
        """
        Execute a nested workflow

        This is a placeholder - the actual implementation will be in the runner
        since it needs access to the runner instance to start and execute the
        nested workflow.
        """
        raise NotImplementedError("Nested workflow execution is handled by the runner")

    def determine_next_action(self, step: dict, success: bool, state: ExecutionState) -> dict:
        """
        Determine the next action based on step success/failure

        Returns:
            action: Dictionary with action type and parameters
        """
        return self.action_handler.determine_next_action(step, success, state)

    def execute_operation(
        self,
        inputs: dict[str, Any],
        operation_id: str | None = None,
        operation_path: str | None = None,
        runtime_params: RuntimeParams | None = None,
    ) -> dict:
        """
        Execute a single API operation directly, outside of a workflow context.

        Args:
            inputs: Input parameters for the operation.
            operation_id: The operationId of the operation to execute.
            operation_path: The path and method (e.g., 'GET /users/{userId}') of the operation.
                          Provide either operation_id or operation_path, not both.
            runtime_params: Runtime parameters for execution (e.g., server variables).

        Returns:
            A dictionary containing the response status_code, headers, and body.
            Example: {'status_code': 200, 'headers': {...}, 'body': ...}

        Raises:
            ValueError: If neither or both operation_id and operation_path are provided,
                        or if the operation cannot be found, or if operation_path is invalid.
            requests.exceptions.HTTPError: If the API call results in an HTTP error status (4xx or 5xx).
        """
        # Validate inputs
        if not operation_id and not operation_path:
            raise ValueError("Either operation_id or operation_path must be provided.")
        if operation_id and operation_path:
            raise ValueError("Provide either operation_id or operation_path, not both.")

        log_identifier = f"ID='{operation_id}'" if operation_id else f"Path='{operation_path}'"
        logger.debug(f"Attempting to execute operation directly: {log_identifier}")

        # Find the operation definition
        try:
            if operation_id:
                operation_details = self.operation_finder.find_by_id(operation_id)
            else:  # operation_path must be set
                try:
                    # Restore splitting logic
                    method, path = operation_path.split(" ", 1)
                    path = path.strip()
                    method = method.strip().upper()
                    if not path or not method:
                        raise ValueError("Path and method cannot be empty after splitting.")
                    operation_details = self.operation_finder.find_by_http_path_and_method(http_path=path, http_method=method)
                except ValueError as e:
                    logger.error(f"Invalid operation_path format: '{operation_path}'. Expected 'METHOD /path'. Error: {e}")
                    raise ValueError(f"Invalid operation_path format: '{operation_path}'. Expected 'METHOD /path'.") from e

        except Exception as e:
            logger.error(f"Operation not found ({log_identifier}): {e}")
            raise ValueError(f"Operation not found: {e}") from e

        # Check if operation was found
        if not operation_details:
            log_identifier = f"ID='{operation_id}'" if operation_id else f"Path='{operation_path}'"
            logger.error(f"Operation not found: {log_identifier}")
            # Use a specific exception or re-raise appropriately
            raise ValueError(f"Operation not found: {log_identifier}")

        log_identifier = operation_details.get("operationId", f"{operation_details.get('method')} {operation_details.get('path')}")

        # Prepare Parameters and Request Body
        try:
            prepared_params = self.parameter_processor.prepare_operation_parameters(
                operation_details=operation_details,
                inputs=inputs
            )
            logger.debug(f"Prepared parameters for direct execution: {prepared_params}")
        except ValueError as e:
            logger.error(f"Error preparing parameters for {log_identifier}: {e}")
            # Reraise as ValueError, potentially add more context if needed
            raise ValueError(f"Error preparing parameters: {e}") from e

        # Handle Authentication
        security_options = self.operation_finder.extract_security_requirements(operation_details)
        logger.debug(f"Resolved security options for {log_identifier}: {security_options}")

        # Resolve final URL
        source_name = operation_details.get("source", "default") # Get source_name
        base_server_url = operation_details.get("url") # This is the relative path template

        final_url_template = self.server_processor.resolve_server_params(
            source_name=source_name,
            operation_url_template=base_server_url, # Pass it as operation_url_template
            server_runtime_params=runtime_params.servers if runtime_params else None
        )

        if not final_url_template:
            error_msg = f"Could not determine final URL for operation {log_identifier}. Operation path was '{base_server_url}' and source was '{source_name}'."
            logger.error(error_msg)
            return {"success": False, "response": {"error": error_msg, "status_code": 0}, "outputs": {}}

        # Execute Request
        method = operation_details.get("method")
        url = final_url_template # Base URL, path params handled by http_client
        request_body_payload = prepared_params.get('body') # Extract body from prepared params
        logger.debug(f"Request body payload: {request_body_payload}")

        if not method or not url:
            logger.error(f"Missing method or url in operation details for {log_identifier}")
            raise ValueError("Operation details are incomplete (missing method or url).")

        try:
            logger.debug(f"Executing direct API call: {method} {url}")
            response_data = self.http_client.execute_request(
                method=method,
                url=url,
                parameters=prepared_params, # Pass the whole structure
                request_body=request_body_payload,
                security_options=security_options
            )
            logger.debug(f"Direct operation execution completed ({log_identifier}) - Status: {response_data.get('status_code')}")
            return response_data
        except Exception as e:
            # Catch potential exceptions during HTTP execution (e.g., network errors, auth failures handled by HTTPExecutor)
            logger.error(f"Error during direct execution of {log_identifier}: {e}", exc_info=True)
            # Re-raise or handle appropriately - for now, re-raise
            # Consider wrapping in a custom exception if needed
            raise e
</step_executor.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/executor/operation_finder.py:
<operation_finder.py>
# src/oak_runner/executor/operation_finder.py
"""
Operation Finder for OAK Runner

This module provides functionality to find operations in OpenAPI specifications.
"""

import logging
import re
from typing import Any

import jsonpointer

from oak_runner.auth.models import SecurityOption, SecurityRequirement

# Configure logging
logger = logging.getLogger("arazzo-runner.executor")


class OperationFinder:
    """Finds operations in source descriptions by ID or path"""

    def __init__(self, source_descriptions: dict[str, Any]):
        """
        Initialize the operation finder

        Args:
            source_descriptions: OpenAPI source descriptions
        """
        self.source_descriptions = source_descriptions

    def find_by_id(self, operation_id: str) -> dict | None:
        """
        Find an operation in source descriptions by its operationId

        Args:
            operation_id: Operation ID to find

        Returns:
            Dictionary with operation details or None if not found
        """
        for source_name, source_desc in self.source_descriptions.items():
            # Search through paths and operations
            paths = source_desc.get("paths", {})
            for path, path_item in paths.items():
                for method, operation in path_item.items():
                    if (
                        method in ["get", "post", "put", "delete", "patch"]
                        and operation.get("operationId") == operation_id
                    ):
                        # Found the operation
                        try:
                            servers = source_desc.get("servers")
                            if not servers or not isinstance(servers, list):
                                raise ValueError("Missing or invalid 'servers' list in OpenAPI spec.")
                            base_url = servers[0].get("url")
                            if not base_url or not isinstance(base_url, str):
                                raise ValueError("Missing or invalid 'url' in the first server object.")
                        except (IndexError, ValueError) as e:
                            # Catch IndexError if servers list is empty or ValueError from explicit raises
                            raise ValueError(f"Could not determine base URL from OpenAPI spec servers: {e}") from e

                        return {
                            "source": source_name,
                            "path": path,
                            "method": method,
                            "url": base_url + path,
                            "operation": operation,
                        }

        return None

    def find_by_http_path_and_method(self, http_path: str, http_method: str) -> dict | None:
        """
        Find an operation in source descriptions by its HTTP path and method.

        Args:
            http_path: The HTTP path (e.g., '/users/{id}').
            http_method: The HTTP method (e.g., 'GET', 'POST'). Case-insensitive.

        Returns:
            Dictionary with operation details or None if not found.
        """
        target_method = http_method.lower() # Ensure case-insensitive comparison
        logger.debug(f"Finding operation by HTTP path and method: Path='{http_path}', Method='{target_method}'")

        for source_name, source_desc in self.source_descriptions.items():
            paths = source_desc.get("paths", {})
            if http_path in paths:
                path_item = paths[http_path]
                if target_method in path_item:
                    operation = path_item[target_method]
                    # Found the operation
                    try:
                        servers = source_desc.get("servers")
                        if not servers or not isinstance(servers, list):
                            raise ValueError("Missing or invalid 'servers' list in OpenAPI spec.")
                        base_url = servers[0].get("url")
                        if not base_url or not isinstance(base_url, str):
                            raise ValueError("Missing or invalid 'url' in the first server object.")
                    except (IndexError, ValueError) as e:
                        # Catch IndexError if servers list is empty or ValueError from explicit raises
                        raise ValueError(f"Could not determine base URL from OpenAPI spec servers: {e}") from e

                    logger.debug(f"Found operation in '{source_name}' for {target_method.upper()} {http_path}")
                    return {
                        "source": source_name,
                        "path": http_path,
                        "method": target_method,
                        "url": base_url + http_path, # Base URL + path
                        "operation": operation,
                        "operationId": operation.get("operationId") # Include operationId if available
                    }
                else:
                    logger.debug(f"Method '{target_method}' not found for path '{http_path}' in source '{source_name}'")
            else:
                 # Check for paths with variables
                 for path_key, path_item in paths.items():
                     if '{' in path_key and self._paths_match(path_key, http_path):
                          if target_method in path_item:
                                operation = path_item[target_method]
                                try:
                                    servers = source_desc.get("servers")
                                    if not servers or not isinstance(servers, list):
                                        raise ValueError("Missing or invalid 'servers' list in OpenAPI spec.")
                                    base_url = servers[0].get("url")
                                    if not base_url or not isinstance(base_url, str):
                                        raise ValueError("Missing or invalid 'url' in the first server object.")
                                except (IndexError, ValueError) as e:
                                    # Catch IndexError if servers list is empty or ValueError from explicit raises
                                    raise ValueError(f"Could not determine base URL from OpenAPI spec servers: {e}") from e

                                logger.debug(f"Found operation (template match) in '{source_name}' for {target_method.upper()} {path_key} matching {http_path}")
                                return {
                                    "source": source_name,
                                    "path": path_key, # Return the template path
                                    "method": target_method,
                                    "url": base_url + path_key, # Use template path for URL construction
                                    "operation": operation,
                                    "operationId": operation.get("operationId")
                                }

        logger.warning(f"Operation not found for {target_method.upper()} {http_path}")
        return None

    def _paths_match(self, template_path: str, concrete_path: str) -> bool:
        """Check if a concrete path matches a template path (e.g., /users/{id})."""
        template_segments = template_path.strip('/').split('/')
        concrete_segments = concrete_path.strip('/').split('/')

        if len(template_segments) != len(concrete_segments):
            return False

        for template_seg, concrete_seg in zip(template_segments, concrete_segments, strict=False):
            if template_seg.startswith('{') and template_seg.endswith('}'):
                continue  # Variable segment matches anything
            if template_seg != concrete_seg:
                return False # Segments must match exactly

        return True

    def find_by_path(self, source_url: str, json_pointer: str) -> dict | None:
        """
        Find an operation in source descriptions by its path

        Args:
            source_url: Source description name or URL
            json_pointer: JSON Pointer to the operation

        Returns:
            Dictionary with operation details or None if not found
        """
        # Log the inputs for debugging
        logger.debug(
            f"Finding operation by path: source_url={source_url}, json_pointer={json_pointer}"
        )

        # Find the source description
        source_desc = self._find_source_description(source_url)
        if not source_desc:
            logger.error(f"Could not find source description for {source_url}")
            return None

        source_name = source_url  # We'll use the provided URL as the name for simplicity

        # Parse the JSON pointer to extract the operation path and method
        operation_info = self._parse_operation_pointer(json_pointer, source_name, source_desc)

        return operation_info

    def _find_source_description(self, source_url: str) -> dict | None:
        """
        Find a source description by URL or name

        Args:
            source_url: Source URL or name

        Returns:
            Source description or None if not found
        """
        # First, try to match by exact name
        if source_url in self.source_descriptions:
            logger.debug(f"Found source description by exact name: {source_url}")
            return self.source_descriptions[source_url]

        # If not an exact match, try to find by URL or name similarity
        for name, desc in self.source_descriptions.items():
            if name in source_url or source_url.endswith(name):
                logger.debug(f"Found source description by partial match: {name}")
                return desc

        return None

    def _parse_operation_pointer(
        self, json_pointer: str, source_name: str, source_desc: dict
    ) -> dict | None:
        """
        Parse a JSON pointer to an operation and extract the relevant details

        Args:
            json_pointer: JSON Pointer to the operation
            source_name: Name of the source
            source_desc: Source description

        Returns:
            Operation details or None if parsing fails
        """
        try:
            paths_obj = source_desc.get("paths", {})
            logger.debug(f"Available paths in {source_name}: {list(paths_obj.keys())}")

            # Ensure the pointer starts with a slash
            if not json_pointer.startswith("/"):
                json_pointer = "/" + json_pointer

            logger.debug(f"Processing JSON Pointer: {json_pointer}")

            # First approach: Extract path and method using regex pattern for standard paths like /paths/<path>/<method>
            operation_info = self._extract_path_method_with_regex(
                json_pointer, source_name, source_desc
            )
            if operation_info:
                return operation_info

            # Second approach: Use full JSON pointer resolution with jsonpointer library
            operation_info = self._resolve_with_jsonpointer(json_pointer, source_name, source_desc)
            if operation_info:
                return operation_info

            # Third approach: Handle special cases for complex paths with path parameters
            operation_info = self._handle_special_cases(json_pointer, source_name, source_desc)
            if operation_info:
                return operation_info

            logger.error(f"Could not parse operation pointer: {json_pointer}")
            return None
        except Exception as e:
            logger.error(f"Error parsing operation pointer: {e}")
            logger.exception("Detailed exception information:")
            return None

    def _extract_path_method_with_regex(
        self, json_pointer: str, source_name: str, source_desc: dict
    ) -> dict | None:
        """
        Extract path and method from a JSON pointer using regex patterns

        Args:
            json_pointer: JSON Pointer to the operation
            source_name: Name of the source
            source_desc: Source description

        Returns:
            Operation details or None if extraction fails
        """
        try:
            # Common pattern for operations in OpenAPI specs: /paths/<path>/<method>
            # The path part may contain encoded forward slashes as ~1
            path_method_pattern = r"/paths(/[^/]+)/([a-z]+)"
            match = re.search(path_method_pattern, json_pointer)

            if match:
                # Get the encoded path and method
                encoded_path, method = match.groups()
                logger.debug(f"Regex matched. Encoded path: {encoded_path}, method: {method}")

                # Decode the path (replace ~1 with / and ~0 with ~)
                decoded_path = encoded_path.replace("~1", "/").replace("~0", "~")
                logger.debug(f"Decoded path: {decoded_path}")

                # Try to find the operation in the source description
                paths_obj = source_desc.get("paths", {})
                operation = paths_obj.get(decoded_path, {}).get(method)

                if operation:
                    # Get the base URL
                    try:
                        servers = source_desc.get("servers")
                        if not servers or not isinstance(servers, list):
                            raise ValueError("Missing or invalid 'servers' list in OpenAPI spec.")
                        base_url = servers[0].get("url")
                        if not base_url or not isinstance(base_url, str):
                            raise ValueError("Missing or invalid 'url' in the first server object.")
                    except (IndexError, ValueError) as e:
                        # Catch IndexError if servers list is empty or ValueError from explicit raises
                        raise ValueError(f"Could not determine base URL from OpenAPI spec servers: {e}") from e

                    # Return the operation details
                    return {
                        "source": source_name,
                        "path": decoded_path,
                        "method": method,
                        "url": base_url + decoded_path,
                        "operation": operation,
                    }
                else:
                    logger.debug(
                        f"Operation not found at decoded path: {decoded_path}, method: {method}"
                    )
            else:
                logger.debug(f"Regex pattern did not match: {json_pointer}")

            return None
        except Exception as e:
            logger.error(f"Error in regex extraction: {e}")
            return None

    def _resolve_with_jsonpointer(
        self, json_pointer: str, source_name: str, source_desc: dict
    ) -> dict | None:
        """
        Resolve a JSON pointer using the jsonpointer library

        Args:
            json_pointer: JSON Pointer to the operation
            source_name: Name of the source
            source_desc: Source description

        Returns:
            Operation details or None if resolution fails
        """
        try:
            # Check if the pointer starts with /paths/
            if not json_pointer.startswith("/paths/"):
                logger.debug(f"Pointer does not start with /paths/: {json_pointer}")
                return None

            # Try to resolve the pointer directly
            operation = jsonpointer.resolve_pointer(source_desc, json_pointer)

            if not isinstance(operation, dict):
                logger.debug(f"Resolved pointer is not a dictionary: {operation}")
                return None

            # We need to determine the path and method
            # Extract from the pointer: /paths/~1foo~1bar/get -> path=/foo/bar, method=get
            parts = json_pointer.split("/")
            if len(parts) >= 4:  # /paths/<path>/<method>
                # The last part should be the method
                method = parts[-1]

                # Combine all middle parts as the path with proper decoding
                path_parts = [p.replace("~1", "/").replace("~0", "~") for p in parts[2:-1]]
                path = "/" + "/".join(path_parts)

                # Verify we have a valid HTTP method
                if method not in ["get", "post", "put", "delete", "patch"]:
                    logger.debug(f"Invalid HTTP method: {method}")
                    return None

                # Get the base URL
                try:
                    servers = source_desc.get("servers")
                    if not servers or not isinstance(servers, list):
                        raise ValueError("Missing or invalid 'servers' list in OpenAPI spec.")
                    base_url = servers[0].get("url")
                    if not base_url or not isinstance(base_url, str):
                        raise ValueError("Missing or invalid 'url' in the first server object.")
                except (IndexError, ValueError) as e:
                    # Catch IndexError if servers list is empty or ValueError from explicit raises
                    raise ValueError(f"Could not determine base URL from OpenAPI spec servers: {e}") from e

                # Verify this is actually a valid operation
                paths_obj = source_desc.get("paths", {})
                if path not in paths_obj or method not in paths_obj.get(path, {}):
                    logger.debug(f"Path/method combination not found: {path}/{method}")
                    # Try normalized path (without trailing slash)
                    norm_path = path.rstrip("/")
                    if norm_path in paths_obj and method in paths_obj.get(norm_path, {}):
                        path = norm_path
                    else:
                        return None

                return {
                    "source": source_name,
                    "path": path,
                    "method": method,
                    "url": base_url + path,
                    "operation": operation,
                }
            else:
                logger.debug(f"Invalid pointer format: {json_pointer}")
                return None

        except (jsonpointer.JsonPointerException, KeyError) as e:
            logger.debug(f"JSON pointer resolution failed: {e}")
            return None
        except Exception as e:
            logger.error(f"Error in jsonpointer resolution: {e}")
            return None

    def _handle_special_cases(
        self, json_pointer: str, source_name: str, source_desc: dict
    ) -> dict | None:
        """
        Handle special cases for complex paths with path parameters

        Args:
            json_pointer: JSON Pointer to the operation
            source_name: Name of the source
            source_desc: Source description

        Returns:
            Operation details or None if handling fails
        """
        try:
            # Special handling for complex paths like /paths/~1{param}~1resource/get
            # where we need to find the matching path template in the OpenAPI spec

            # Check if this looks like a path with parameters
            if "~1{" in json_pointer:
                logger.debug(f"Handling special case for path with parameters: {json_pointer}")

                # Extract the method from the end of the pointer
                parts = json_pointer.split("/")
                if len(parts) < 3:
                    return None

                method = parts[-1]
                if method not in ["get", "post", "put", "delete", "patch"]:
                    return None

                # Decode the path parts
                pointer_path_parts = [p.replace("~1", "/").replace("~0", "~") for p in parts[2:-1]]
                pointer_path = "/" + "/".join(pointer_path_parts)
                logger.debug(f"Decoded path with parameters: {pointer_path}")

                # The path might have parameters in the form {param}
                # We need to find a matching path template in the OpenAPI spec
                paths_obj = source_desc.get("paths", {})

                # Convert path with parameters to a regex pattern
                # Replace {param} with a wildcard pattern that matches any segment
                path_pattern = re.escape(pointer_path).replace("\\{[^\\}]+\\}", "[^/]+")
                logger.debug(f"Path pattern: {path_pattern}")

                # Check each path in the spec for a match
                for spec_path in paths_obj.keys():
                    # Convert the spec path to a pattern by replacing {param} with wildcards
                    spec_pattern = re.escape(spec_path).replace("\\{[^\\}]+\\}", "[^/]+")

                    # Check if the templates match
                    if (
                        spec_pattern == path_pattern
                        or re.match(spec_pattern, pointer_path)
                        or re.match(path_pattern, spec_path)
                    ):
                        logger.debug(f"Found matching path template: {spec_path}")

                        # Check if the method exists
                        operation = paths_obj.get(spec_path, {}).get(method)
                        if operation:
                            # Get the base URL
                            try:
                                servers = source_desc.get("servers")
                                if not servers or not isinstance(servers, list):
                                    raise ValueError("Missing or invalid 'servers' list in OpenAPI spec.")
                                base_url = servers[0].get("url")
                                if not base_url or not isinstance(base_url, str):
                                    raise ValueError("Missing or invalid 'url' in the first server object.")
                            except (IndexError, ValueError) as e:
                                # Catch IndexError if servers list is empty or ValueError from explicit raises
                                raise ValueError(f"Could not determine base URL from OpenAPI spec servers: {e}") from e

                            return {
                                "source": source_name,
                                "path": spec_path,
                                "method": method,
                                "url": base_url + spec_path,
                                "operation": operation,
                            }

                # If path parameter matching didn't work, try direct name matching
                # This is a simpler approach that might work for common cases
                # For example, if the pointer is /paths/~1{id}~1resource/get
                # then look for a path like "/{id}/resource" in the spec

                for spec_path, path_item in paths_obj.items():
                    if method in path_item:
                        # Simple string comparison to check if they're similar
                        if "{" in spec_path and "}" in spec_path:
                            logger.debug(f"Checking path template match: {spec_path}")

                            # Compare path segments (ignoring the actual parameter names)
                            pointer_segments = pointer_path.split("/")
                            spec_segments = spec_path.split("/")

                            if len(pointer_segments) == len(spec_segments):
                                match = True
                                for i, (p_seg, s_seg) in enumerate(
                                    zip(pointer_segments, spec_segments, strict=False)
                                ):
                                    # Skip empty segments
                                    if not p_seg and not s_seg:
                                        continue

                                    # If spec has a parameter, it matches anything
                                    if "{" in s_seg and "}" in s_seg:
                                        continue

                                    # If pointer has a parameter, it matches anything
                                    if "{" in p_seg and "}" in p_seg:
                                        continue

                                    # Otherwise, segments must match exactly
                                    if p_seg != s_seg:
                                        match = False
                                        break

                                if match:
                                    logger.debug(
                                        f"Found matching path by segment analysis: {spec_path}"
                                    )

                                    # Get the operation and base URL
                                    operation = path_item.get(method)
                                    try:
                                        servers = source_desc.get("servers")
                                        if not servers or not isinstance(servers, list):
                                            raise ValueError("Missing or invalid 'servers' list in OpenAPI spec.")
                                        base_url = servers[0].get("url")
                                        if not base_url or not isinstance(base_url, str):
                                            raise ValueError("Missing or invalid 'url' in the first server object.")
                                    except (IndexError, ValueError) as e:
                                        # Catch IndexError if servers list is empty or ValueError from explicit raises
                                        raise ValueError(f"Could not determine base URL from OpenAPI spec servers: {e}") from e

                                    return {
                                        "source": source_name,
                                        "path": spec_path,
                                        "method": method,
                                        "url": base_url + spec_path,
                                        "operation": operation,
                                    }

            # The code below contains some fallback handling for pattern matching.
            # This is a last resort approach for finding operations, but it should
            # be generic and not contain API-specific special cases.
            #
            # NOTE: As per our development guidelines in CLAUDE.md:
            # "NEVER include spec-specific or data-specific handling in core implementation code"
            #
            # Instead of hardcoding specific patterns for known APIs (like XKCD),
            # we should implement generic algorithms that work for all cases.
            #
            # For the specific examples below, we're using a general approach that:
            # 1. Identifies patterns in the JSON pointer
            # 2. Looks for matching paths in the OpenAPI spec
            # 3. Returns the operation information if found

            # Common pattern: JSON pointer with simple path like /paths/~1resource/get
            simple_path_match = re.match(r"/paths/~1([^/~]+)/([a-z]+)$", json_pointer)
            if simple_path_match:
                resource, method = simple_path_match.groups()
                resource_path = f"/{resource}"
                paths_obj = source_desc.get("paths", {})
                if resource_path in paths_obj and method in paths_obj[resource_path]:
                    logger.debug(
                        f"Found operation using simple path pattern: {resource_path}/{method}"
                    )
                    try:
                        servers = source_desc.get("servers")
                        if not servers or not isinstance(servers, list):
                            raise ValueError("Missing or invalid 'servers' list in OpenAPI spec.")
                        base_url = servers[0].get("url")
                        if not base_url or not isinstance(base_url, str):
                            raise ValueError("Missing or invalid 'url' in the first server object.")
                    except (IndexError, ValueError) as e:
                        # Catch IndexError if servers list is empty or ValueError from explicit raises
                        raise ValueError(f"Could not determine base URL from OpenAPI spec servers: {e}") from e

                    return {
                        "source": source_name,
                        "path": resource_path,
                        "method": method,
                        "url": base_url + resource_path,
                        "operation": paths_obj[resource_path][method],
                    }

            # Common pattern: JSON pointer with path parameter like /paths/~1{param}~1resource/get
            param_path_match = re.match(r"/paths/~1\{([^}]+)\}~1([^/~]+)/([a-z]+)$", json_pointer)
            if param_path_match:
                param_name, resource, method = param_path_match.groups()
                param_path = f"/{{{param_name}}}/{resource}"
                paths_obj = source_desc.get("paths", {})
                if param_path in paths_obj and method in paths_obj[param_path]:
                    logger.debug(
                        f"Found operation using parameter path pattern: {param_path}/{method}"
                    )
                    try:
                        servers = source_desc.get("servers")
                        if not servers or not isinstance(servers, list):
                            raise ValueError("Missing or invalid 'servers' list in OpenAPI spec.")
                        base_url = servers[0].get("url")
                        if not base_url or not isinstance(base_url, str):
                            raise ValueError("Missing or invalid 'url' in the first server object.")
                    except (IndexError, ValueError) as e:
                        # Catch IndexError if servers list is empty or ValueError from explicit raises
                        raise ValueError(f"Could not determine base URL from OpenAPI spec servers: {e}") from e

                    return {
                        "source": source_name,
                        "path": param_path,
                        "method": method,
                        "url": base_url + param_path,
                        "operation": paths_obj[param_path][method],
                    }

            return None
        except Exception as e:
            logger.error(f"Error handling special cases: {e}")
            return None

    def extract_security_requirements(self, operation_info: dict) -> list[SecurityOption]:
        """
        Extract security requirements from operation info and source descriptions.
        Args:
            operation_info: Operation information from operation finder
        Returns:
            List of SecurityOption objects (empty list if none found)
        """
        operation = operation_info.get("operation", {})
        source_name = operation_info.get("source")
        path = operation_info.get("path")

        # 1. Check for operation-level security requirements
        if "security" in operation:
            logger.debug(f"Found operation-level security requirements for {operation.get('operationId')}")
            raw_options = operation.get("security", [])
            return self._convert_to_security_options(raw_options)

        # 2. Check for path-level security requirements (OpenAPI 3.x)
        if source_name in self.source_descriptions and path:
            paths_obj = self.source_descriptions[source_name].get("paths", {})
            path_obj = paths_obj.get(path, {})
            if isinstance(path_obj, dict) and "security" in path_obj:
                logger.debug(f"Found path-level security requirements for path {path} in API {source_name}")
                raw_options = path_obj.get("security", [])
                return self._convert_to_security_options(raw_options)

        # 3. Check for global security requirements in the source description
        if source_name in self.source_descriptions:
            source_desc = self.source_descriptions.get(source_name, {})
            if "security" in source_desc:
                logger.debug(f"Found global security requirements for API {source_name}")
                raw_options = source_desc.get("security", [])
                return self._convert_to_security_options(raw_options)

        # 4. No security requirements found
        logger.debug("No security requirements found")
        return []

    def _convert_to_security_options(self, raw_options: list) -> list[SecurityOption]:
        """
        Convert raw security options from OpenAPI spec to SecurityOption model instances
        Args:
            raw_options: List of raw security option objects from OpenAPI spec
        Returns:
            List of SecurityOption objects
        """
        security_options = []
        for raw_option in raw_options:
            option = SecurityOption()
            for scheme_name, scopes in raw_option.items():
                requirement = SecurityRequirement(
                    scheme_name=scheme_name,
                    scopes=scopes
                )
                option.requirements.append(requirement)
            security_options.append(option)
        return security_options

    def get_operations_for_workflow(self, workflow: dict) -> list[dict]:
        """
        Find all operation references in a workflow dict (Arazzo format).
        Returns a list of operation_info dicts as returned by find_by_id/find_by_path.
        """
        operations = []
        steps = workflow.get("steps", [])
        for step in steps:
            if "operationId" in step:
                op_info = self.find_by_id(step["operationId"])
                if op_info:
                    operations.append(op_info)
            elif "operationPath" in step:
                # operationPath format: <source>#<json_pointer>
                match = re.match(r"([^#]+)#(.+)", step["operationPath"])
                if match:
                    source_url, json_pointer = match.groups()
                    op_info = self.find_by_path(source_url, json_pointer)
                    if op_info:
                        operations.append(op_info)
        return operations
</operation_finder.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/executor/action_handler.py:
<action_handler.py>
# src/oak_runner/executor/action_handler.py
"""
Action Handler for OAK Runner

This module provides functionality to determine the next action after a step execution.
"""

import logging
import re
from typing import Any

import jsonpath_ng.ext as jsonpath

from ..evaluator import ExpressionEvaluator
from ..models import ActionType, ExecutionState

# Configure logging
logger = logging.getLogger("arazzo-runner.executor")


class ActionHandler:
    """Determines the next action after a step execution"""

    def __init__(self, source_descriptions: dict[str, Any]):
        """
        Initialize the action handler

        Args:
            source_descriptions: OpenAPI source descriptions
        """
        self.source_descriptions = source_descriptions

    def determine_next_action(self, step: dict, success: bool, state: ExecutionState) -> dict:
        """
        Determine the next action based on step success/failure

        Args:
            step: Step definition
            success: Whether the step succeeded
            state: Current execution state

        Returns:
            Dictionary with action type and parameters
        """
        step_id = step.get("stepId", "unknown")
        logger.debug(f"Determining next action for step {step_id}, success={success}")

        if success:
            # Check onSuccess actions
            actions = step.get("onSuccess", [])
            logger.info(f"Step {step_id} has {len(actions)} onSuccess actions")

            for i, action in enumerate(actions):
                action_name = action.get("name", f"action_{i}")
                action_type = action.get("type", "unknown")
                logger.info(f"Checking action {action_name} of type {action_type}")

                # Check if action has criteria
                if "criteria" in action:
                    criteria = action.get("criteria", [])
                    logger.info(f"Action {action_name} has {len(criteria)} criteria")
                    criteria_met = self._check_action_criteria(criteria, state)

                    if not criteria_met:
                        logger.info(f"Action {action_name} criteria not met, skipping")
                        continue
                    else:
                        logger.info(f"Action {action_name} criteria met, executing")

                # Process the action
                action_type = action.get("type")
                if action_type == "end":
                    logger.info(f"Action {action_name} ends the workflow")
                    return {"type": ActionType.END}
                elif action_type == "goto":
                    if "workflowId" in action:
                        target = action.get("workflowId")
                        logger.info(f"Action {action_name} goes to workflow {target}")
                        return {"type": ActionType.GOTO, "workflow_id": target}
                    elif "stepId" in action:
                        target = action.get("stepId")
                        logger.info(f"Action {action_name} goes to step {target}")
                        return {"type": ActionType.GOTO, "step_id": target}

            # No matching action, continue to next step
            logger.info(f"No matching action for step {step_id}, continuing to next step")
            return {"type": ActionType.CONTINUE}
        else:
            # Check onFailure actions
            actions = step.get("onFailure", [])
            logger.debug(f"Step {step_id} has {len(actions)} onFailure actions")

            for i, action in enumerate(actions):
                action_name = action.get("name", f"failure_action_{i}")
                action_type = action.get("type", "unknown")
                logger.info(f"Checking failure action {action_name} of type {action_type}")

                # Check if action has criteria
                if "criteria" in action:
                    criteria = action.get("criteria", [])
                    logger.info(f"Failure action {action_name} has {len(criteria)} criteria")
                    criteria_met = self._check_action_criteria(criteria, state)

                    if not criteria_met:
                        logger.info(f"Failure action {action_name} criteria not met, skipping")
                        continue
                    else:
                        logger.info(f"Failure action {action_name} criteria met, executing")

                # Process the action
                action_type = action.get("type")
                if action_type == "end":
                    logger.info(f"Failure action {action_name} ends the workflow")
                    return {"type": ActionType.END}
                elif action_type == "goto":
                    if "workflowId" in action:
                        target = action.get("workflowId")
                        logger.info(f"Failure action {action_name} goes to workflow {target}")
                        return {"type": ActionType.GOTO, "workflow_id": target}
                    elif "stepId" in action:
                        target = action.get("stepId")
                        logger.info(f"Failure action {action_name} goes to step {target}")
                        return {"type": ActionType.GOTO, "step_id": target}
                elif action_type == "retry":
                    retry_after = action.get("retryAfter", 0)
                    retry_limit = action.get("retryLimit", 1)
                    logger.info(
                        f"Failure action {action_name} retries (after={retry_after}, limit={retry_limit})"
                    )

                    result = {
                        "type": ActionType.RETRY,
                        "retry_after": retry_after,
                        "retry_limit": retry_limit,
                    }

                    if "workflowId" in action:
                        target = action.get("workflowId")
                        logger.info(f"Retry action targets workflow {target}")
                        result["workflow_id"] = target
                    elif "stepId" in action:
                        target = action.get("stepId")
                        logger.info(f"Retry action targets step {target}")
                        result["step_id"] = target

                    return result

            # No matching action, end the workflow with failure
            logger.warning(
                f"No matching failure action for step {step_id}, ending workflow with failure"
            )
            return {"type": ActionType.END}

    def _check_action_criteria(self, criteria: list[dict], state: ExecutionState) -> bool:
        """
        Check if action criteria are met

        Args:
            criteria: List of criteria to check
            state: Current execution state

        Returns:
            True if all criteria are met, False otherwise
        """
        logger.info(f"Checking {len(criteria)} action criteria")

        # Context for evaluating criteria
        context = {}

        # Check each criterion
        for i, criterion in enumerate(criteria):
            condition = criterion.get("condition")
            criterion_type = criterion.get("type", "simple")

            logger.info(f"Criterion {i+1}: type={criterion_type}, condition={condition}")

            # Evaluate context if specified
            if "context" in criterion:
                context_expr = criterion.get("context")
                context_value = ExpressionEvaluator.evaluate_expression(
                    context_expr, state, self.source_descriptions, context
                )
                logger.info(f"Context expression {context_expr} evaluated to: {context_value}")
                if context_value is None:
                    logger.warning(f"Context expression {context_expr} evaluated to None")
                    return False
                local_context = context_value
            else:
                # Use default context for current action
                local_context = context

            # Check criterion based on type
            if criterion_type == "simple":
                result = ExpressionEvaluator.evaluate_simple_condition(
                    condition, state, self.source_descriptions, context
                )
                if not result:
                    logger.warning(f"Simple condition failed: {condition}")
                    return False
                logger.info(f"Simple condition passed: {condition}")

            elif criterion_type == "jsonpath":
                if not local_context or not condition:
                    logger.warning("JSONPath condition failed: empty context or condition")
                    return False

                try:
                    logger.info(f"Evaluating JSONPath condition: {condition}")
                    logger.info(f"Context type: {type(local_context)}, Content: {local_context}")

                    # Special handling for count expressions (common in workflows)
                    if condition.startswith("$[?count(@.") and ")" in condition:
                        # Parse count expression like $[?count(@.products) > 0]
                        match = re.match(
                            r"\$\[\?count\(\@\.([a-zA-Z0-9_]+)\) *([<>=!]+) *(\d+)\]", condition
                        )
                        if match:
                            property_name, operator, value_str = match.groups()
                            value = int(value_str)

                            # Get property value
                            array_value = None
                            if isinstance(local_context, dict) and property_name in local_context:
                                array_value = local_context[property_name]

                            # Count items if it's a list
                            if isinstance(array_value, list):
                                count = len(array_value)
                            elif array_value is not None:
                                # If not a list but not None, treat as 1 item
                                count = 1
                            else:
                                # If property doesn't exist or is None, count is 0
                                count = 0

                            logger.info(
                                f"Count evaluation: property={property_name}, count={count}, op={operator}, value={value}"
                            )

                            # Evaluate the comparison
                            if operator == "==":
                                result = count == value
                            elif operator == "!=":
                                result = count != value
                            elif operator == ">":
                                result = count > value
                            elif operator == "<":
                                result = count < value
                            elif operator == ">=":
                                result = count >= value
                            elif operator == "<=":
                                result = count <= value
                            else:
                                logger.warning(f"Unsupported operator: {operator}")
                                return False

                            if not result:
                                logger.warning(
                                    f"Count comparison failed: {count} {operator} {value}"
                                )
                                return False

                            logger.info(f"Count comparison succeeded: {count} {operator} {value}")
                            continue

                    # Standard JSONPath evaluation
                    jsonpath_expr = jsonpath.parse(condition)
                    matches = [match.value for match in jsonpath_expr.find(local_context)]

                    logger.info(f"JSONPath matches: {matches}")

                    if not matches:
                        logger.warning(f"JSONPath condition failed: no matches for {condition}")
                        return False

                    logger.info(f"JSONPath condition passed: {condition}")
                except Exception as e:
                    logger.error(f"Error evaluating JSONPath expression: {e}")
                    return False

            elif criterion_type == "regex":
                if not local_context or not condition:
                    logger.warning("Regex condition failed: empty context or condition")
                    return False

                # Convert context to string if needed
                ctx_str = str(local_context)

                # Check if the regex pattern matches
                match = re.search(condition, ctx_str)
                if not match:
                    logger.warning(f"Regex condition failed: {condition} did not match {ctx_str}")
                    return False

                logger.info(f"Regex condition passed: {condition}")

            else:
                logger.warning(f"Unsupported criterion type: {criterion_type}")
                return False

        # All criteria passed
        logger.info("All criteria passed")
        return True
</action_handler.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/executor/server_processor.py:
<server_processor.py>
# src/oak_runner/executor/server_processor.py
"""
ServerProcessor: Handles server configuration and URL resolution logic for OAK Runner.
"""
import logging
import os
import re
from typing import Any
from urllib.parse import urljoin, urlparse

from ..models import ServerConfiguration

logger = logging.getLogger("oak_runner.server_processor")

from ..utils import create_env_var_name, extract_api_title_prefix


class ServerProcessor:
    """
    Component to encapsulate all server configuration and URL resolution logic.
    """
    def __init__(self, source_descriptions: dict[str, Any]) -> None:
        self.source_descriptions = source_descriptions

    @staticmethod
    def resolve_server_base_url(server_config: ServerConfiguration, server_runtime_params: dict[str, str] | None = None) -> str:
        """
        Resolves the templated server URL using provided parameters, environment variables,
        or default values for a given ServerConfiguration.

        Args:
            server_config: The ServerConfiguration object.
            runtime_params: A dictionary of runtime parameters to substitute.

        Returns:
            The resolved server base URL as a string.

        Raises:
            ValueError: If a required variable in the template cannot be resolved.
        """
        resolved_url = server_config.url_template

        # Find all variable placeholders like {var_name} in the URL template
        template_vars_in_url: set[str] = set(re.findall(r"{(.*?)}", server_config.url_template))

        for var_name in template_vars_in_url:
            server_var_details = server_config.variables.get(var_name)

            if not server_var_details:
                # This implies a mismatch: a variable placeholder in the URL template
                # does not have a corresponding definition in the 'variables' section.
                # OpenAPI spec dictates that all variables in the URL template MUST be defined.
                raise ValueError(
                    f"Variable '{var_name}' in URL template '{server_config.url_template}' has no corresponding "
                    f"definition in server variables. This may indicate an invalid OpenAPI document."
                )

            resolved_value: str | None = None

            # Construct the environment variable name using the utility function
            prefix = f"{server_config.api_title_prefix}_OAK_SERVER" if server_config.api_title_prefix else "OAK_SERVER"
            env_var_name = create_env_var_name(
                var_name=var_name,
                prefix=prefix
            )

            # 1. Try to use value from runtime_params (keyed by env_var_name)
            if server_runtime_params and env_var_name in server_runtime_params:
                resolved_value = server_runtime_params[env_var_name]
                if resolved_value is not None:
                    logger.debug(f"Server variable '{var_name}' (using key '{env_var_name}'): resolved from runtime_params.")

            # 2. Else, if not resolved from runtime_params (or if value was None), try os.getenv
            if resolved_value is None:
                env_os_value = os.getenv(env_var_name)
                if env_os_value is not None:
                    resolved_value = env_os_value
                    logger.debug(f"Server variable '{var_name}' (using env var '{env_var_name}'): resolved from environment.")

            # 3. Else, if still not resolved, use ServerVariable.default_value
            if resolved_value is None and server_var_details.default_value is not None:
                resolved_value = server_var_details.default_value
                logger.debug(f"Server variable '{var_name}': resolved from default_value.")

            # 4. If still unresolved, this variable is mandatory and no value was found
            if resolved_value is None:
                raise ValueError(
                    f"Required server variable '{var_name}' could not be resolved for URL template "
                    f"'{server_config.url_template}'. No value found in runtime_params (key: '{env_var_name}'), "
                    f"environment (variable: '{env_var_name}'), or as a default."
                )

            # 5. If ServerVariable.enum_values is set, log a warning if the resolved value is not in the enum
            if server_var_details.enum_values and resolved_value not in server_var_details.enum_values:
                logger.warning(
                    f"Value '{resolved_value}' for server variable '{var_name}' is not in its defined "
                    f"enum values: {server_var_details.enum_values}. URL: '{server_config.url_template}'"
                )

            # Substitute the resolved value into the URL template
            resolved_url = resolved_url.replace(f"{{{var_name}}}", resolved_value)

        return resolved_url

    @staticmethod
    def extract_server_configurations(spec_dict: dict[str, Any]) -> list[ServerConfiguration]:
        """
        Extracts server configurations from a loaded OpenAPI specification dictionary.

        Args:
            spec_dict: The loaded OpenAPI specification as a dictionary.

        Returns:
            A list of ServerConfiguration objects parsed from the spec.
        """
        logger.debug('extracting server confs...')
        server_configs: list[ServerConfiguration] = []
        api_title = spec_dict.get('info', {}).get('title')
        api_title_prefix = extract_api_title_prefix(api_title) if api_title else None
        raw_server_list = spec_dict.get('servers', [])
        if not isinstance(raw_server_list, list):
            logger.warning("'servers' field in OpenAPI spec is not a list. Skipping server configuration parsing.")
            return []
        for i, server_data in enumerate(raw_server_list):
            if not isinstance(server_data, dict):
                logger.warning(f"Server entry at index {i} is not a dictionary. Skipping this entry.")
                continue
            try:
                config_data = server_data.copy()
                config_data['api_title_prefix'] = api_title_prefix
                server_config_instance = ServerConfiguration(**config_data)
                server_configs.append(server_config_instance)
                logger.debug(f"Successfully parsed server configuration for URL: {server_config_instance.url_template}")
            except Exception as e:
                logger.error(f"Failed to parse server entry at index {i} (URL: {server_data.get('url', 'N/A')}): {e}", exc_info=True)
        if not server_configs and raw_server_list:
            logger.debug("Found server entries in spec, but none could be parsed into ServerConfiguration objects.")
        elif not raw_server_list:
            logger.debug("No 'servers' defined in the OpenAPI specification.")
        else:
            logger.debug(f"Successfully extracted {len(server_configs)} server configuration(s).")
        return server_configs

    @staticmethod
    def format_server_config_details(config: ServerConfiguration) -> str:
        """
        Formats the details of a ServerConfiguration for user-friendly display.

        This includes the URL template, its description, and for each variable:
        its name, description, default value, possible enum values, and the
        exact environment variable name that can be used to set it.

        Args:
            config: The ServerConfiguration object to format.

        Returns:
            A string containing the formatted details.
        """
        details: list[str] = []

        details.append(f"Server URL Template: {config.url_template}")
        if config.description:
            details.append(f"  Description: {config.description}")

        if config.api_title_prefix:
            details.append(f"  (Associated API Title Prefix for ENV VARS: {config.api_title_prefix})")

        if not config.variables:
            details.append("  This server URL has no dynamic variables.")
        else:
            details.append("  Variables:")
            for var_name, var_details in config.variables.items():
                details.append(f"    - Variable: '{var_name}'")
                if var_details.description:
                    details.append(f"      Description: {var_details.description}")

                prefix = f"{config.api_title_prefix}_OAK_SERVER" if config.api_title_prefix else "OAK_SERVER"
                env_var_name = create_env_var_name(
                    var_name=var_name,
                    prefix=prefix
                )
                details.append(f"      Set via ENV: {env_var_name}")

                if var_details.default_value is not None:
                    details.append(f"      Default: '{var_details.default_value}'")
                else:
                    details.append("      Default: (none)")

                if var_details.enum_values:
                    details.append(f"      Possible Values: {', '.join(var_details.enum_values)}")
                else:
                    details.append("      Possible Values: (any)")

        return "\n".join(details)

    @staticmethod
    def url_contains_template_vars_in_host(url_string: str | None) -> bool:
        if not url_string:
            return False
        parsed_url = urlparse(url_string)
        if parsed_url.netloc:
            if re.search(r"\{[^}]+\}", parsed_url.netloc):
                return True
        return False

    def get_env_mappings(self) -> dict[str, dict[str, dict[str, str]]]:
        """
        Extract environment variable mappings for all server variables across all source descriptions.
        
        Returns:
            A nested dictionary structure mapping source names to server URLs to variable mappings.
            Format:
            {
                "source_name": {
                    "server_url": {
                        "variable_name": "ENV_VAR_NAME"
                    }
                }
            }
        """
        env_mappings = {}

        # Process each source description
        for source_name, spec_dict in self.source_descriptions.items():
            # Extract server configurations for this source
            server_configs = self.extract_server_configurations(spec_dict)

            if not server_configs:
                continue

            # Initialize the mapping for this source
            source_mappings = {}

            # Process each server configuration
            for i, server_config in enumerate(server_configs):
                # Use URL template as the key for this server
                server_url = server_config.url_template

                # Skip servers without variables
                if not server_config.variables:
                    continue

                # Initialize the mapping for this server
                server_mappings = {}

                # Process each variable in this server configuration
                for var_name, var_details in server_config.variables.items():
                    # Create the environment variable name
                    prefix = f"{server_config.api_title_prefix}_OAK_SERVER" if server_config.api_title_prefix else "OAK_SERVER"
                    env_var_name = create_env_var_name(
                        var_name=var_name,
                        prefix=prefix
                    )

                    # Store the mapping
                    server_mappings[var_name] = env_var_name

                # Only add this server if it has variables
                if server_mappings:
                    source_mappings[server_url] = server_mappings

            # Only add this source if it has servers with variables
            if source_mappings:
                env_mappings[source_name] = source_mappings

        return env_mappings

    def resolve_server_params(
        self,
        operation_url_template: str | None,
        server_runtime_params: dict[str, str] | None,
        source_name: str,
    ) -> str:
        """Resolve the final URL for an operation, including server variable resolution.

        Example:
            Given an OpenAPI spec for `source_name` "MY_API" with a server:
            `operation_url_template: "https://{env}.api.com/v1/users/{userId}"`
            And an operation with `operation_url_template="/users/{userId}"`.

            Result: "https://dev.api.com/v1/users/{userId}"
        """
        if not operation_url_template:
            logger.error("operation_url_template is None or empty.")
            raise ValueError("Operation URL template cannot be None or empty.")

        has_vars_in_host = self.url_contains_template_vars_in_host(operation_url_template)

        # Case 1: operation_url_template is a full URL and has NO server variables in its host. Use as is.
        if not has_vars_in_host:
            return operation_url_template

        # Case 2: We need to use server configurations from the spec.
        if not source_name:
            logger.error(f"No source_name provided. Cannot resolve URL '{operation_url_template}' which requires server configuration.")
            raise ValueError(f"Cannot resolve URL '{operation_url_template}': source_name is required to load server configurations.")

        spec_doc: Any | None = self.source_descriptions.get(source_name)
        if not spec_doc:
            logger.error(f"Spec not found for source_name='{source_name}'. Cannot resolve URL '{operation_url_template}'.")
            raise ValueError(f"Cannot resolve URL '{operation_url_template}': OpenAPI spec for source '{source_name}' not found.")

        server_configs: list[ServerConfiguration] = ServerProcessor.extract_server_configurations(spec_doc)
        if not server_configs:
            logger.error(f"No server configurations found in spec for source_name='{source_name}'. Cannot resolve URL '{operation_url_template}'.")
            raise ValueError(f"Cannot resolve URL '{operation_url_template}': No server configurations found in spec for '{source_name}'.")

        selected_config = server_configs[0]  # Default to the first server config

        try:
            resolved_server_base = self.resolve_server_base_url(server_config=selected_config, server_runtime_params=server_runtime_params)
        except ValueError as e:
            logger.error(f"Error resolving variables in ServerConfiguration ('{selected_config.url_template}'): {e}")
            raise ValueError(f"Failed to resolve server variables for server configuration '{selected_config.url_template}': {e}") from e

        # Extract the path, query, and fragment from operation_url_template.
        parsed_operation_url = urlparse(operation_url_template)
        operation_path_part = parsed_operation_url.path
        if parsed_operation_url.query:
            operation_path_part += "?" + parsed_operation_url.query
        if parsed_operation_url.fragment:
            operation_path_part += "#" + parsed_operation_url.fragment

        # Ensure operation_path_part is suitable for urljoin
        if operation_path_part and not operation_path_part.startswith("/"):
            operation_path_part = "/" + operation_path_part
        elif not operation_path_part:
            operation_path_part = "/"

        final_url = urljoin(resolved_server_base.rstrip('/') + '/', operation_path_part.lstrip('/'))
        return final_url
</server_processor.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/evaluator.py:
<evaluator.py>
# src/oak_runner/evaluator.py
"""
Expression Evaluator for OAK Runner

This module provides functions for evaluating runtime expressions used in Arazzo workflows.
"""

import logging
import re
from typing import Any

from .models import ExecutionState

# Configure logging
logger = logging.getLogger("arazzo-runner.evaluator")


class ExpressionEvaluator:
    """Evaluates runtime expressions in Arazzo workflows"""

    @staticmethod
    def handle_array_access(expression: str, state: ExecutionState) -> Any | None:
        """
        Special handler for array access expressions like $steps.findPetsStep.outputs.availablePets[0].id
        Returns None if the expression doesn't match the expected pattern or value can't be found
        """
        # Check if this looks like an array access pattern
        if not (expression.startswith("$") and "[" in expression and "]" in expression):
            return None

        # Match the common pattern: $steps.{stepId}.outputs.{array}[{index}].{field}
        match = re.match(
            r"^\$steps\.([a-zA-Z0-9_]+)\.outputs\.([a-zA-Z0-9_]+)\[(\d+)\]\.([a-zA-Z0-9_]+)$",
            expression,
        )
        if match:
            step_id, array_name, index_str, field_name = match.groups()
            index = int(index_str)

            logger.info(
                f"Array access - Step: {step_id}, Array: {array_name}, Index: {index}, Field: {field_name}"
            )

            # Check if step exists
            if step_id not in state.step_outputs:
                logger.info(f"Step {step_id} not found in outputs")
                return None

            step_output = state.step_outputs[step_id]

            # Check if array exists in step outputs
            if array_name not in step_output:
                logger.info(f"Array {array_name} not found in step {step_id} outputs")
                return None

            array = step_output[array_name]

            # Check if array is a list and index is valid
            if not isinstance(array, list):
                logger.info(f"{array_name} is not a list: {type(array)}")
                return None

            if not (0 <= index < len(array)):
                logger.info(f"Index {index} out of range for array of length {len(array)}")
                return None

            item = array[index]

            # Check if item is a dict and has the field
            if not isinstance(item, dict):
                logger.info(f"Array item is not a dict: {type(item)}")
                return None

            if field_name not in item:
                logger.info(f"Field {field_name} not found in item: {list(item.keys())}")
                return None

            # Found the value!
            value = item[field_name]
            logger.info(f"Successfully extracted array value: {value}")
            return value

        # Try direct array index access: $steps.{stepId}.outputs.{array}[{index}]
        match = re.match(
            r"^\$steps\.([a-zA-Z0-9_]+)\.outputs\.([a-zA-Z0-9_]+)\[(\d+)\]$", expression
        )
        if match:
            step_id, array_name, index_str = match.groups()
            index = int(index_str)

            logger.info(
                f"Direct array access - Step: {step_id}, Array: {array_name}, Index: {index}"
            )

            # Check if step exists
            if step_id not in state.step_outputs:
                logger.info(f"Step {step_id} not found in outputs")
                return None

            step_output = state.step_outputs[step_id]

            # Check if array exists in step outputs
            if array_name not in step_output:
                logger.info(f"Array {array_name} not found in step {step_id} outputs")
                return None

            array = step_output[array_name]

            # Check if array is a list and index is valid
            if not isinstance(array, list):
                logger.info(f"{array_name} is not a list: {type(array)}")
                return None

            if not (0 <= index < len(array)):
                logger.info(f"Index {index} out of range for array of length {len(array)}")
                return None

            # Found the value!
            value = array[index]
            logger.info(f"Successfully extracted array item: {value}")
            return value

        # Try direct input array access: $inputs.{array}[{index}]
        match = re.match(r"^\$inputs\.([a-zA-Z0-9_]+)\[(\d+)\]$", expression)
        if match:
            array_name, index_str = match.groups()
            index = int(index_str)

            logger.info(f"Input array access - Array: {array_name}, Index: {index}")

            # Check if array exists in inputs
            if array_name not in state.inputs:
                logger.info(f"Array {array_name} not found in inputs")
                return None

            array = state.inputs[array_name]

            # Check if array is a list and index is valid
            if not isinstance(array, list):
                logger.info(f"{array_name} is not a list: {type(array)}")
                return None

            if not (0 <= index < len(array)):
                logger.info(f"Index {index} out of range for array of length {len(array)}")
                return None

            # Found the value!
            value = array[index]
            logger.info(f"Successfully extracted input array item: {value}")
            return value

        return None

    @staticmethod
    def evaluate_expression(
        expression: str,
        state: ExecutionState,
        source_descriptions: dict[str, Any] = None,
        additional_context: dict[str, Any] = None,
    ) -> Any:
        """
        Evaluate a runtime expression in the context of the current state.

        This evaluator supports:
        - Dot notation for object properties (e.g., $steps.loginStep.outputs.token)
        - Array indexing with brackets (e.g., $steps.findPetsStep.outputs.availablePets[0].id)
        - Nested object access
        - JSON Pointers (e.g., $response.body#/data/items)
        """
        if not isinstance(expression, str):
            return expression

        if not expression.startswith("$"):
            return expression

        # Build evaluation context
        context = {
            "inputs": state.inputs,
            "steps": state.step_outputs,
            "outputs": state.workflow_outputs,
            "dependencies": state.dependency_outputs,
            "sourceDescriptions": source_descriptions or {},
            "statusCode": additional_context.get("statusCode") if additional_context else None,
            "response": additional_context.get("response") if additional_context else None,
        }

        # Add additional context if provided
        if additional_context:
            for key, value in additional_context.items():
                if key not in context:  # Don't overwrite core context variables
                    context[key] = value

        try:
            # Save the original for error messages
            original_expression = expression

            # Handle special case: if the expression is a direct reference to a context variable
            if expression == "$statusCode" and "statusCode" in context:
                return context["statusCode"]
            if expression == "$response" and "response" in context:
                return context["response"]

            # Handle JSON Pointer syntax in expressions
            # Check for patterns like $response.body#/path/to/value
            json_pointer_match = re.match(r"^\$([a-zA-Z0-9_]+)\.([a-zA-Z0-9_]+)#(/.*)$", expression)
            if json_pointer_match:
                import jsonpointer

                container, property_name, pointer_path = json_pointer_match.groups()

                # Get the container object
                if container not in context:
                    logger.debug(
                        f"JSON Pointer evaluation failed: container '{container}' not found in context"
                    )
                    return None

                container_obj = context[container]

                # Get the property from the container
                if not isinstance(container_obj, dict) or property_name not in container_obj:
                    logger.debug(
                        f"JSON Pointer evaluation failed: property '{property_name}' not found in container"
                    )
                    return None

                property_value = container_obj[property_name]

                # Use jsonpointer to resolve the path
                try:
                    # For empty pointer or root, return the entire object
                    if pointer_path == "/":
                        return property_value

                    # Create a JSON pointer resolver
                    pointer = jsonpointer.JsonPointer(pointer_path)
                    result = pointer.resolve(property_value)
                    logger.debug(f"JSON Pointer evaluation: Found value for {expression}")
                    return result
                except (jsonpointer.JsonPointerException, TypeError) as e:
                    logger.debug(f"JSON Pointer evaluation failed: {e}")
                    return None

            # Try array access handler first
            array_value = ExpressionEvaluator.handle_array_access(expression, state)
            if array_value is not None:
                return array_value

            # For direct access to array elements like $steps.findPetsStep.outputs.availablePets[0].id
            # First check if this is a simple array access pattern we can handle directly
            array_access_pattern = r"^\$([a-zA-Z0-9_]+)\.([a-zA-Z0-9_]+)\.([a-zA-Z0-9_]+)\.([a-zA-Z0-9_]+)\[(\d+)\]\.([a-zA-Z0-9_]+)$"
            array_match = re.match(array_access_pattern, expression)

            if array_match:
                # This is a direct array access, let's handle it explicitly
                container, item, property1, property2, idx, field = array_match.groups()
                idx = int(idx)

                # Build the access path step by step
                if container in context:
                    container_obj = context[container]
                    if item in container_obj:
                        item_obj = container_obj[item]
                        if property1 in item_obj:
                            prop1_obj = item_obj[property1]
                            if property2 in prop1_obj:
                                prop2_obj = prop1_obj[property2]
                                if isinstance(prop2_obj, list) and 0 <= idx < len(prop2_obj):
                                    array_item = prop2_obj[idx]
                                    if isinstance(array_item, dict) and field in array_item:
                                        logger.debug(
                                            f"Direct array access: Found value {array_item[field]} at {expression}"
                                        )
                                        return array_item[field]

            # If direct pattern matching fails, fall back to general expression parsing
            # Handle array indexing with brackets
            array_indices = []
            # Extract bracketed indices
            bracket_pattern = r"\[(\d+)\]"

            # Find all array indices in the expression
            for match in re.finditer(bracket_pattern, expression):
                idx = int(match.group(1))
                array_indices.append((match.start(), match.end(), idx))

            # Replace brackets with placeholders that won't be split by dots
            # Using a format that won't appear in normal expressions
            modified_expression = expression
            adjustment = 0
            for start, end, idx in array_indices:
                adjusted_start = start + adjustment
                adjusted_end = end + adjustment
                placeholder = f"__ARRAY_INDEX_{idx}__"
                modified_expression = (
                    modified_expression[:adjusted_start]
                    + placeholder
                    + modified_expression[adjusted_end:]
                )
                # Adjust for length difference between original and replacement
                adjustment += len(placeholder) - (end - start)

            # Strip the leading $ and handle an **optional** extra dot after it.
            # Example necessity:
            #   Expression: $.steps.myStep.statusCode
            #   Without this fix → split produces ["", "steps", ...] and lookup of '' fails.
            #   With the fix     → path_parts becomes ["steps", "myStep", "statusCode"].
            path_str = modified_expression[1:]
            if path_str.startswith("."):
                # Remove redundant root dot so we don't get an empty token
                path_str = path_str[1:]

            path_parts = path_str.split(".")

            # Restore array indices in path parts
            for i, part in enumerate(path_parts):
                if part.startswith("__ARRAY_INDEX_") and part.endswith("__"):
                    try:
                        # Extract the index
                        idx = int(part[len("__ARRAY_INDEX_") : -2])
                        path_parts[i] = str(idx)
                    except (ValueError, IndexError):
                        pass

            # Navigate through the structure
            current = context
            path_so_far = "$"

            # Debug log the context and path parts for easier troubleshooting
            logger.debug(f"Evaluating expression: {original_expression}")
            logger.debug(f"Path parts: {path_parts}")
            logger.debug(f"Context keys: {list(context.keys())}")

            for i, part in enumerate(path_parts):
                path_so_far += f".{part}"

                if current is None:
                    logger.debug(f"Expression evaluation failed at {path_so_far}: parent is None")
                    return None

                if isinstance(current, dict):
                    # Handle dictionary access
                    if part == "outputs" and part not in current:
                        continue
                    if part in current:
                        current = current[part]
                    else:
                        logger.debug(
                            f"Expression evaluation failed at {path_so_far}: key '{part}' not found in dict {list(current.keys())}"
                        )
                        return None
                elif isinstance(current, list):
                    # Handle list indexing
                    if part.isdigit():
                        idx = int(part)
                        if 0 <= idx < len(current):
                            current = current[idx]
                        else:
                            logger.debug(
                                f"Expression evaluation failed at {path_so_far}: index {idx} out of range for list of length {len(current)}"
                            )
                            return None
                    else:
                        # Try to access the attribute for all items in the list
                        # This is useful for filtering lists
                        try:
                            current = [
                                item.get(part) if isinstance(item, dict) else getattr(item, part)
                                for item in current
                            ]
                        except (AttributeError, KeyError):
                            logger.debug(
                                f"Expression evaluation failed at {path_so_far}: cannot access '{part}' on list items"
                            )
                            return None
                elif hasattr(current, part):
                    # Handle object attribute access
                    current = getattr(current, part)
                else:
                    logger.debug(
                        f"Expression evaluation failed at {path_so_far}: cannot navigate further with '{part}'"
                    )
                    return None

            return current

        except Exception as e:
            logger.error(f"Error evaluating expression '{original_expression}': {e}")
            return None

    @staticmethod
    def evaluate_simple_condition(
        condition: str,
        state: ExecutionState,
        source_descriptions: dict[str, Any] = None,
        additional_context: dict[str, Any] = None,
    ) -> bool:
        """Evaluate a simple condition expression"""
        logger.debug(f"Evaluating condition: {condition}")

        # Special case handling for common patterns
        if (
            "==" in condition
            or "!=" in condition
            or "<" in condition
            or ">" in condition
            or "<=" in condition
            or ">=" in condition
        ):
            # Handle comparisons with JavaScript-style true/false literals
            condition = condition.replace(" == true", " == True").replace(" == false", " == False")
            condition = condition.replace(" != true", " != True").replace(" != false", " != False")

            # Handle null comparisons (null is None in Python)
            condition = condition.replace(" == null", " == None").replace(" != null", " != None")

            # Handle JavaScript-style OR operator
            condition = condition.replace("||", " or ")

            # Handle JavaScript-style AND operator
            condition = condition.replace("&&", " and ")

            logger.debug(f"Processed condition: {condition}")

            # Simple parsing for common comparison patterns
            left_right_match = re.match(r"^\s*([^<>=!]+)\s*([<>=!]+)\s*([^<>=!]+)\s*$", condition)
            if left_right_match:
                left_expr, operator, right_expr = left_right_match.groups()

                # Evaluate left side
                if left_expr.strip().startswith("$"):
                    left_value = ExpressionEvaluator.evaluate_expression(
                        left_expr.strip(), state, source_descriptions, additional_context
                    )
                else:
                    # Handle literals
                    if left_expr.strip() == "true":
                        left_value = True
                    elif left_expr.strip() == "false":
                        left_value = False
                    elif left_expr.strip() == "null":
                        left_value = None
                    else:
                        try:
                            left_value = eval(left_expr.strip())
                        except:
                            left_value = left_expr.strip()

                # Evaluate right side
                if right_expr.strip().startswith("$"):
                    right_value = ExpressionEvaluator.evaluate_expression(
                        right_expr.strip(), state, source_descriptions, additional_context
                    )
                else:
                    # Handle literals
                    if right_expr.strip() == "true":
                        right_value = True
                    elif right_expr.strip() == "false":
                        right_value = False
                    elif right_expr.strip() == "null":
                        right_value = None
                    else:
                        try:
                            right_value = eval(right_expr.strip())
                        except:
                            right_value = right_expr.strip()

                logger.debug(f"Comparison: {left_value} {operator} {right_value}")

                # Perform comparison
                if operator == "==":
                    return left_value == right_value
                elif operator == "!=":
                    return left_value != right_value
                elif operator == ">":
                    return left_value > right_value
                elif operator == "<":
                    return left_value < right_value
                elif operator == ">=":
                    return left_value >= right_value
                elif operator == "<=":
                    return left_value <= right_value

        # For complex conditions or if simple parsing fails, try the more general approach
        try:
            # Replace expressions with their values
            def replace_expr(match):
                expr = match.group(0)
                value = ExpressionEvaluator.evaluate_expression(
                    expr, state, source_descriptions, additional_context
                )
                if value is True:
                    return "True"
                elif value is False:
                    return "False"
                elif value is None:
                    return "None"
                else:
                    return repr(value)

            # Replace all expressions that start with $
            condition_with_values = re.sub(r"\$[a-zA-Z0-9_.]+", replace_expr, condition)

            # Replace JavaScript-style syntax with Python syntax
            condition_with_values = condition_with_values.replace("||", " or ").replace(
                "&&", " and "
            )
            condition_with_values = condition_with_values.replace(" == null", " == None").replace(
                " != null", " != None"
            )
            condition_with_values = condition_with_values.replace(" == true", " == True").replace(
                " == false", " == False"
            )
            condition_with_values = condition_with_values.replace(" != true", " != True").replace(
                " != false", " != False"
            )

            logger.debug(f"Processed condition for eval: {condition_with_values}")

            # Evaluate the condition
            result = eval(condition_with_values)
            return bool(result)
        except Exception as e:
            logger.error(f"Error evaluating condition {condition}: {e}")
            logger.error(
                f"Processed condition was: {condition_with_values if 'condition_with_values' in locals() else 'N/A'}"
            )
            return False

    @staticmethod
    def process_object_expressions(
        obj: dict, state: ExecutionState, source_descriptions: dict[str, Any] = None
    ) -> dict:
        """Process dictionary values, evaluating any expressions"""
        if not isinstance(obj, dict):
            return obj

        result = {}
        for key, value in obj.items():
            if isinstance(value, str) and value.startswith("$"):
                # Evaluate expression
                result[key] = ExpressionEvaluator.evaluate_expression(
                    value, state, source_descriptions
                )
            elif isinstance(value, dict):
                # Process nested dictionary
                result[key] = ExpressionEvaluator.process_object_expressions(
                    value, state, source_descriptions
                )
            elif isinstance(value, list):
                # Process nested list
                result[key] = ExpressionEvaluator.process_array_expressions(
                    value, state, source_descriptions
                )
            else:
                result[key] = value
        return result

    @staticmethod
    def process_array_expressions(
        arr: list, state: ExecutionState, source_descriptions: dict[str, Any] = None
    ) -> list:
        """Process list values, evaluating any expressions"""
        if not isinstance(arr, list):
            return arr

        result = []
        for item in arr:
            if isinstance(item, str) and item.startswith("$"):
                # Evaluate expression
                result.append(
                    ExpressionEvaluator.evaluate_expression(item, state, source_descriptions)
                )
            elif isinstance(item, dict):
                # Process nested dictionary
                result.append(
                    ExpressionEvaluator.process_object_expressions(item, state, source_descriptions)
                )
            elif isinstance(item, list):
                # Process nested list
                result.append(
                    ExpressionEvaluator.process_array_expressions(item, state, source_descriptions)
                )
            else:
                result.append(item)
        return result
</evaluator.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/__main__.py:
<__main__.py>
# src/oak_runner/__main__.py
"""
Command-line interface for the Arazzo workflow runner.
"""

import argparse
import asyncio
import json
import logging
import sys
from typing import Any

from .models import RuntimeParams, StepStatus
from .runner import OAKRunner
from .utils import set_log_level

logger = logging.getLogger("oak-runner-cli")


def parse_inputs(inputs_str: str) -> dict[str, Any]:
    """Parse input string into a dictionary."""
    if not inputs_str:
        return {}
    try:
        inputs = json.loads(inputs_str)
        if not isinstance(inputs, dict):
            raise ValueError("Inputs must be a JSON object (dictionary).")
        return inputs
    except json.JSONDecodeError as e:
        raise ValueError(f"Invalid JSON format for inputs: {e}")


async def main():
    parser = argparse.ArgumentParser(description="Oak Runner")
    # Global arguments - defined *before* subparsers
    parser.add_argument(
        "--log-level",
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
        help="Set the logging level (default: WARNING)",
    )

    subparsers = parser.add_subparsers(dest="operation", required=True, help='Operation to perform')

    # Subparser for 'show-env-mappings'
    parser_env = subparsers.add_parser('show-env-mappings', help='Show environment variable mappings for authentication')
    # Group to require one path source
    env_path_group = parser_env.add_mutually_exclusive_group(required=True)
    env_path_group.add_argument(
        "arazzo_path",
        nargs='?', # Make positional optional *within the group*
        default=None, # Explicitly default to None
        help="Path to the Arazzo YAML file to provide context (use this OR --openapi-path)"
    )
    env_path_group.add_argument(
        "--openapi-path",
        help="Path to the OpenAPI spec file to provide context (use this OR arazzo_path)"
    )
    parser_env.set_defaults(func=handle_show_env_mappings)

    # Subparser for 'execute-workflow'
    parser_exec_wf = subparsers.add_parser('execute-workflow', help='Execute a complete workflow')
    parser_exec_wf.add_argument(
        "arazzo_path",
        help="Path to the Arazzo YAML file" # Required for this command
    )
    parser_exec_wf.add_argument("--workflow-id", required=True, help="ID of the workflow to execute")
    parser_exec_wf.add_argument("--inputs", help="JSON string of workflow inputs", default="{}")
    parser_exec_wf.add_argument(
        "--server-variables",
        default="{}",
        help="Runtime parameters for server variable resolution as a JSON string (e.g., '{\"MY_API_SERVER\": \"your-instance\"}')"
    )
    parser_exec_wf.set_defaults(func=handle_execute_workflow)

    # Subparser for 'execute-operation'
    parser_exec_op = subparsers.add_parser('execute-operation', help='Execute a single API operation directly')
    # Use mutually exclusive group for path specifiers
    path_group = parser_exec_op.add_mutually_exclusive_group(required=True)
    path_group.add_argument(
        "--arazzo-path",
        help="Path to the Arazzo YAML file containing the operation definition"
    )
    path_group.add_argument(
        "--openapi-path",
        help="Path to the OpenAPI spec file containing the operation definition"
    )
    # Operation identifiers
    exec_group = parser_exec_op.add_mutually_exclusive_group(required=True)
    exec_group.add_argument("--operation-id", help="ID of the operation to execute")
    exec_group.add_argument("--operation-path", help="HTTP method and path (e.g., 'GET /users/{id}')")
    parser_exec_op.add_argument("--inputs", default="{}", help="Inputs for the operation as a JSON string")
    parser_exec_op.add_argument(
        "--server-variables",
        default="{}",
        help="Runtime parameters for server variable resolution as a JSON string (e.g., '{\"MY_API_SERVER\": \"your-instance\"}')"
    )
    parser_exec_op.set_defaults(func=handle_execute_operation)

    # Subparser for 'list-workflows'
    parser_list_wf = subparsers.add_parser('list-workflows', help='List all workflow IDs in an Arazzo file')
    parser_list_wf.add_argument("arazzo_path", help="Path to the Arazzo YAML file")
    parser_list_wf.set_defaults(func=handle_list_workflows)

    # Subparser for 'describe-workflow'
    parser_desc_wf = subparsers.add_parser('describe-workflow', help='Show details of a specific workflow')
    parser_desc_wf.add_argument("arazzo_path", help="Path to the Arazzo YAML file")
    parser_desc_wf.add_argument("--workflow-id", required=True, help="ID of the workflow to describe")
    parser_desc_wf.set_defaults(func=handle_describe_workflow)

    # Subparser for 'generate-example'
    parser_gen_ex = subparsers.add_parser('generate-example', help='Generate an example execution command for a workflow')
    parser_gen_ex.add_argument("arazzo_path", help="Path to the Arazzo YAML file")
    parser_gen_ex.add_argument("--workflow-id", required=True, help="ID of the workflow to generate example for")
    parser_gen_ex.set_defaults(func=handle_generate_example)

    args = parser.parse_args()

    # Set log level early
    set_log_level(args.log_level)

    # Adjust log level based on command or explicit flag
    # Suppress logs for inspection commands unless overridden
    if args.operation in ["list-workflows", "describe-workflow", "generate-example"] and args.log_level == "INFO":
         # Suppress standard INFO/DEBUG logs for these commands by default
         set_log_level("WARNING")
    else:
         # Use the specified or default log level for other commands
         set_log_level(args.log_level)

    # Removed argument validation section as it's handled by argparse structure now

    runner = None # Runner will be initialized within handlers now
    try:
        # --- Command Execution ---
        # Call the function associated with the chosen subparser
        # Pass runner (always None initially) and args to the handler
        # The handler function is responsible for ensuring the runner is initialized.
        if hasattr(args, 'func'):
            await args.func(None, args) # Pass None for runner
        else:
            # Should not happen due to required=True on subparsers
            parser.print_help()
            sys.exit(1)

    except ValueError as e:
        logger.error(f"Input Error: {e}")
        sys.exit(1)
    except FileNotFoundError as e:
        logger.error(f"File Error: {e}")
        sys.exit(1)
    except Exception as e:
        logger.exception(f"An unexpected error occurred: {e}", exc_info=True)
        sys.exit(1)


async def handle_show_env_mappings(runner: OAKRunner | None, args: argparse.Namespace):
    # Runner is always passed as None now, initialize here based on args
    logger.info("Fetching environment variable mappings...")
    try:
        # Initialize runner based on provided path
        if args.arazzo_path:
            logger.info(f"Initializing runner with Arazzo file: {args.arazzo_path}")
            runner = OAKRunner.from_arazzo_path(args.arazzo_path)
        elif args.openapi_path:
            logger.info(f"Initializing runner with OpenAPI file: {args.openapi_path}")
            runner = OAKRunner.from_openapi_path(args.openapi_path)
        else:
            # This case should be prevented by the required mutually exclusive group
            logger.error("Cannot fetch environment mappings: No Arazzo or OpenAPI path specified.")
            sys.exit(1)

        mappings = OAKRunner.generate_env_mappings(runner.arazzo_doc, runner.source_descriptions)
        print(json.dumps(mappings, indent=2))
        sys.exit(0)
    except Exception as e:
        logger.error(f"Failed to get environment mappings: {e}", exc_info=True)
        sys.exit(1)


async def handle_execute_workflow(runner: OAKRunner | None, args: argparse.Namespace):
    # Runner is always passed as None now, initialize here
    logger.info("Executing workflow...")
    # Initialize runner
    if not args.arazzo_path:
         # Should be caught by argparse, but defensive check
         logger.error("Arazzo path is required for execute-workflow.")
         sys.exit(1)
    logger.info(f"Initializing runner with Arazzo file: {args.arazzo_path}")
    runner = OAKRunner.from_arazzo_path(args.arazzo_path)
    if not runner:
        logger.error("Runner initialization failed.")
        sys.exit(1)

    # Parse inputs
    try:
        inputs = json.loads(args.inputs)
    except json.JSONDecodeError:
        logger.error(f"Invalid JSON in inputs: {args.inputs}")
        sys.exit(1)

    if not args.workflow_id:
        logger.error("--workflow-id is required for execute-workflow.")
        sys.exit(1)

    # Start and execute the workflow using the new API
    # Parse server-variables and create RuntimeParams object
    try:
        server_params_dict = json.loads(args.server_variables)
        # Create a RuntimeParams object with the server parameters
        runtime_params = RuntimeParams(servers=server_params_dict)
    except json.JSONDecodeError:
        logger.error(f"Invalid JSON in server-variables: {args.server_variables}")
        sys.exit(1)

    try:
        result = runner.execute_workflow(
            args.workflow_id,
            inputs,
            runtime_params=runtime_params  # Pass the RuntimeParams object
        )
    except Exception as e:
        logger.error(f"Failed to execute workflow: {e}", exc_info=True)
        sys.exit(1)

    # Print outputs and determine success/failure
    print(f"\n=== Completed workflow: {args.workflow_id} ===")
    print(f"Outputs: {result}")

    # Check for failure in outputs (if possible)
    try:
        state = None
        for exec_id, st in runner.execution_states.items():
            if st.workflow_id == args.workflow_id:
                state = st
                break
        if not state:
            logger.error(f"Could not retrieve final execution state for {args.workflow_id}")
            sys.exit(1)
        last_step_id = list(state.status.keys())[-1] if state.status else None
        all_success = not (last_step_id and state.status.get(last_step_id) == StepStatus.FAILURE)
    except Exception as e:
        logger.error(f"Error determining final workflow status: {e}", exc_info=True)
        all_success = False

    sys.exit(0 if all_success else 1)


async def handle_execute_operation(runner: OAKRunner | None, args: argparse.Namespace): # Runner can be None initially
    """Handles the execute_operation command."""
    # Runner is always passed as None now, initialize here
    logger.info("Executing direct operation...")
    try:
        # Initialize runner based on provided path
        if args.arazzo_path:
             logger.info(f"Initializing runner with Arazzo file: {args.arazzo_path}")
             runner = OAKRunner.from_arazzo_path(args.arazzo_path)
        elif args.openapi_path:
             logger.info(f"Initializing runner with OpenAPI file: {args.openapi_path}")
             runner = OAKRunner.from_openapi_path(args.openapi_path)
        else:
             # This state should be prevented by the mutually exclusive group requirement
             logger.error("Cannot execute operation: No Arazzo or OpenAPI path specified.")
             sys.exit(1)

        # Ensure runner got initialized successfully
        if not runner:
            logger.error("Runner initialization failed.")
            sys.exit(1)

        inputs_dict = json.loads(args.inputs)
    except json.JSONDecodeError:
        logger.error(f"Invalid JSON in inputs: {args.inputs}")
        sys.exit(1)

    try:
        server_params_dict = json.loads(args.server_variables)
    except json.JSONDecodeError:
        logger.error(f"Invalid JSON in server-variables: {args.server_variables}")
        sys.exit(1)

    try:
        # Ensure runner is valid before calling execute_operation
        if not runner:
            logger.error("Runner was not initialized correctly.")
            sys.exit(1)

        # Correctly pass operation_id and operation_path based on args
        # REMOVED await as execute_operation is synchronous
        # Create a RuntimeParams object with the server parameters
        runtime_params = RuntimeParams(servers=server_params_dict)

        result = runner.execute_operation(
            operation_id=args.operation_id,  # Pass directly
            operation_path=args.operation_path, # Pass directly
            inputs=inputs_dict,
            runtime_params=runtime_params  # Pass the RuntimeParams object
        )
        # Remove 'headers' from result if present
        if isinstance(result, dict) and 'headers' in result:
            result = dict(result)  # Make a shallow copy to avoid mutating originals
            result.pop('headers')
        logger.info(f"Operation Result: {json.dumps(result, indent=2)}")
        # Determine exit code based on HTTP status (e.g., 2xx is success)
        status_code = result.get("status_code", 500)
        sys.exit(0 if 200 <= status_code < 300 else 1)

    except Exception as e:
        logger.error(f"Failed to execute operation: {e}", exc_info=True)
        sys.exit(1)


async def handle_list_workflows(runner: OAKRunner | None, args: argparse.Namespace):
    """Handles the list-workflows command using logic from old arazzo_runner."""
    # logger.info("Listing workflows...") # Suppressed for this command by default
    try:
        if not args.arazzo_path:
            logger.error("Arazzo path is required for list-workflows.")
            sys.exit(1)
        # Initialize runner to access Arazzo data
        runner = OAKRunner.from_arazzo_path(args.arazzo_path)
        # Check runner and runner.arazzo_doc
        if not runner or not runner.arazzo_doc:
            logger.error("Runner initialization or Arazzo doc loading failed.")
            sys.exit(1)

        # --- Logic copied from arazzo_runner ---
        workflows = runner.arazzo_doc.get("workflows", [])
        if not workflows:
            print("No workflows found in the Arazzo document.")
            sys.exit(0)
        print("Available Workflows:")
        for workflow in workflows:
            workflow_id = workflow.get("workflowId", "Unknown")
            description = workflow.get("description", "No description available")
            print(f"- {workflow_id}: {description}")
        # --- End copied logic ---
        sys.exit(0)
    except Exception as e:
        # Use logger for exceptions even if standard output is suppressed
        logging.getLogger("oak-runner-cli").error(f"Failed to list workflows: {e}", exc_info=True)
        sys.exit(1)


async def handle_describe_workflow(runner: OAKRunner | None, args: argparse.Namespace):
    """Handles the describe-workflow command using logic from old arazzo_runner."""
    # logger.info(f"Describing workflow: {args.workflow_id}") # Suppressed
    try:
        if not args.arazzo_path:
            logger.error("Arazzo path is required for describe-workflow.")
            sys.exit(1)
        if not args.workflow_id:
            # Use print for user-facing errors in suppressed-log commands
            print("Error: --workflow-id is required for the describe-workflow operation")
            sys.exit(1)

        # Initialize runner to access Arazzo data
        runner = OAKRunner.from_arazzo_path(args.arazzo_path)
        # Check runner and runner.arazzo_doc
        if not runner or not runner.arazzo_doc:
            logger.error("Runner initialization or Arazzo doc loading failed.")
            sys.exit(1)

        # --- Logic copied from arazzo_runner ---
        workflows = runner.arazzo_doc.get("workflows", [])
        target_workflow = None
        for workflow in workflows:
            if workflow.get("workflowId") == args.workflow_id:
                target_workflow = workflow
                break
        if not target_workflow:
            print(f"Error: Workflow with ID '{args.workflow_id}' not found in the Arazzo document")
            sys.exit(1)

        # Display workflow details
        print(f"Workflow: {target_workflow.get('workflowId')}")
        print(f"Summary: {target_workflow.get('description', 'No description available')}")
        # Display input parameters
        inputs_schema = target_workflow.get("inputs", {})
        if inputs_schema and "properties" in inputs_schema:
            properties = inputs_schema.get("properties", {})
            print("\nInputs:")
            for param_name, param_details in properties.items():
                param_type = param_details.get("type", "string")
                description = param_details.get("description", "")
                print(f"- {param_name} ({param_type}): {description}")
        # Display workflow steps
        steps = target_workflow.get("steps", [])
        if steps:
            print("\nSteps:")
            for i, step in enumerate(steps, 1):
                step_id = step.get("stepId", "Unknown")
                description = step.get("description", "")
                if description:
                    print(f"{i}. {step_id}: {description}")
                else:
                    print(f"{i}. {step_id}")
        # Display workflow outputs
        output_mappings = target_workflow.get("outputs", {})
        if output_mappings:
            print("\nOutputs:")
            for output_name in output_mappings.keys():
                print(f"- {output_name}")
        # --- End copied logic ---
        sys.exit(0)
    except Exception as e:
        logging.getLogger("oak-runner-cli").error(f"Failed to describe workflow: {e}", exc_info=True)
        sys.exit(1)


async def handle_generate_example(runner: OAKRunner | None, args: argparse.Namespace):
    """Handles the generate-example command using logic from old arazzo_runner."""
    # logger.info(f"Generating example command for workflow: {args.workflow_id}") # Suppressed
    try:
        if not args.arazzo_path:
            logger.error("Arazzo path is required for generate-example.")
            sys.exit(1)
        if not args.workflow_id:
            print("Error: --workflow-id is required for the generate-example operation")
            sys.exit(1)

        # Initialize runner to access Arazzo data
        runner = OAKRunner.from_arazzo_path(args.arazzo_path)
         # Check runner and runner.arazzo_doc
        if not runner or not runner.arazzo_doc:
             logger.error("Runner initialization or Arazzo doc loading failed.")
             sys.exit(1)

        # --- Logic copied from arazzo_runner ---
        workflows = runner.arazzo_doc.get("workflows", [])
        target_workflow = None
        for workflow in workflows:
            if workflow.get("workflowId") == args.workflow_id:
                target_workflow = workflow
                break
        if not target_workflow:
            print(f"Error: Workflow with ID '{args.workflow_id}' not found in the Arazzo document")
            sys.exit(1)

        # Generate input placeholders
        inputs_schema = target_workflow.get("inputs", {})
        input_json = {}
        if inputs_schema and "properties" in inputs_schema:
            properties = inputs_schema.get("properties", {})
            # required = inputs_schema.get("required", []) # Not used in original logic
            for param_name, param_details in properties.items():
                param_type = param_details.get("type", "string")
                # Generate appropriate placeholder based on parameter type
                if param_type == "string":
                    if "id" in param_name.lower():
                        placeholder = f"YOUR_{param_name.upper()}"
                    else:
                        placeholder = f"Your {param_name.replace('_', ' ')} here"
                elif param_type == "integer" or param_type == "number":
                    placeholder = 0
                elif param_type == "boolean":
                    placeholder = False
                elif param_type == "array":
                    placeholder = []
                elif param_type == "object":
                    placeholder = {}
                else:
                    placeholder = "PLACEHOLDER"
                input_json[param_name] = placeholder

        # Format the command example using the correct OAK Runner CLI format
        file_path = args.arazzo_path
        # Escape quotes twice for shell embedding (once for JSON, once for shell)
        inputs_json_string = json.dumps(input_json).replace('"', '\\"')
        example_cmd = (
            f"pdm run python -m oak_runner execute-workflow "
            f"{file_path} "
            f"--workflow-id {args.workflow_id} "
            f"--inputs \"{inputs_json_string}\""
        )
        print(example_cmd)
        # --- End copied logic ---
        sys.exit(0)

    except KeyError: # Added specific catch if workflow not found during input generation
         print(f"Error: Workflow ID '{args.workflow_id}' not found when generating example.")
         sys.exit(1)
    except Exception as e:
        logging.getLogger("oak-runner-cli").error(f"Failed to generate example: {e}", exc_info=True)
        sys.exit(1)


def run_main():
    """Provide a blocking main entry point for use in scripts"""
    asyncio.run(main())


if __name__ == "__main__":
    run_main()
</__main__.py>

here is /Users/rodrivera/repos/oak/tools/oak-runner/src/oak_runner/executor.py:
<executor.py>
# src/oak_runner/executor.py
"""
Executor module for OAK Runner

This module has been refactored into a package for better maintainability.
See the executor/ directory for the actual implementation.
"""

from .executor.step_executor import StepExecutor

# Re-export the StepExecutor class for backward compatibility
__all__ = ["StepExecutor"]
</executor.py>

